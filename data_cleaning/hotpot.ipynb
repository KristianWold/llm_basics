{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizer import TokenizerBPE\n",
    "from data_handling import normalize_to_ascii, clean_text\n",
    "from utils import saver, loader\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# disable gpu for testing purposes\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f199c71",
   "metadata": {},
   "source": [
    "## Hotpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3f6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../corpus/hotpot_train_v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9016b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0f3fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c043e640a841619c30042fa052e400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_list = []\n",
    "question_list = []\n",
    "answer_list = []\n",
    "\n",
    "\n",
    "for i, sample in tqdm(enumerate(data), total=length):\n",
    "    \n",
    "    supporting_facts = sample[\"supporting_facts\"]\n",
    "    supporting_facts = set([fact[0] for fact in supporting_facts])\n",
    "    context = sample[\"context\"]\n",
    "    context_accum = []\n",
    "    for fact in supporting_facts:\n",
    "        for paragraph in context:\n",
    "            if fact == paragraph[0]:\n",
    "                context_accum.extend(paragraph[1])\n",
    "\n",
    "    context = \" \".join(context_accum)\n",
    "    context_list.append(clean_text(normalize_to_ascii(context)))\n",
    "\n",
    "    q = clean_text(normalize_to_ascii(sample[\"question\"]))\n",
    "    a = clean_text(normalize_to_ascii(sample[\"answer\"]))\n",
    "    question_list.append(q)\n",
    "    answer_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd01509",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver('../corpus/hotpot_raw', [context_list, question_list, answer_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b52bbc",
   "metadata": {},
   "source": [
    "## No Redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8664657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7765132420d4c7a807d8f02abb9a9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError for fact Minoru Suzuki at index 2 in paragraph Minoru Suzuki\n",
      "IndexError for fact Jonathan Taylor Thomas at index 20 in paragraph Jonathan Taylor Thomas\n",
      "IndexError for fact Francis Poulenc at index 3 in paragraph Francis Poulenc\n",
      "IndexError for fact Pan's Labyrinth at index 2 in paragraph Pan's Labyrinth\n",
      "IndexError for fact Rupert Grint at index 4 in paragraph Rupert Grint\n",
      "IndexError for fact Walkerville, South Australia at index 52 in paragraph Walkerville, South Australia\n",
      "IndexError for fact Khady Sylla at index 2 in paragraph Khady Sylla\n",
      "IndexError for fact Robbie Rist at index 52 in paragraph Robbie Rist\n",
      "IndexError for fact Harry Styles at index 4 in paragraph Harry Styles\n",
      "IndexError for fact William T. Anderson at index 2 in paragraph William T. Anderson\n",
      "IndexError for fact South Park (season 12) at index 20 in paragraph South Park (season 12)\n",
      "IndexError for fact Hutterite at index 40 in paragraph Hutterite\n",
      "IndexError for fact 2013 Moore tornado at index 2 in paragraph 2013 Moore tornado\n",
      "IndexError for fact Savannah cat at index 2 in paragraph Savannah cat\n",
      "IndexError for fact Albany and Schenectady Railroad at index 30 in paragraph Albany and Schenectady Railroad\n",
      "IndexError for fact Buprenorphine at index 7 in paragraph Buprenorphine\n",
      "IndexError for fact Jan Hooks at index 8 in paragraph Jan Hooks\n",
      "IndexError for fact Drop Dead Fred at index 2 in paragraph Drop Dead Fred\n",
      "IndexError for fact Minister of Defence (India) at index 11 in paragraph Minister of Defence (India)\n",
      "IndexError for fact Rochester Hills, Michigan at index 2 in paragraph Rochester Hills, Michigan\n",
      "IndexError for fact Iowa State Cyclones football at index 4 in paragraph Iowa State Cyclones football\n",
      "IndexError for fact Jodie Whittaker at index 3 in paragraph Jodie Whittaker\n"
     ]
    }
   ],
   "source": [
    "context_list = []\n",
    "question_list = []\n",
    "answer_list = []\n",
    "\n",
    "\n",
    "for i, sample in tqdm(enumerate(data), total=length):\n",
    "    \n",
    "    supporting_facts = sample[\"supporting_facts\"]\n",
    "    context = sample[\"context\"]\n",
    "    context_accum = []\n",
    "    for fact, idx in supporting_facts:\n",
    "        for paragraph in context:\n",
    "            if fact == paragraph[0]:\n",
    "                try:\n",
    "                    context_accum.append(clean_text(normalize_to_ascii(paragraph[1][idx])))\n",
    "                except IndexError:\n",
    "                    print(f\"IndexError for fact {fact} at index {idx} in paragraph {paragraph[0]}\")\n",
    "                    continue\n",
    "        \n",
    "    context_list.append(\" \".join(context_accum))\n",
    "                                    \n",
    "    q = clean_text(normalize_to_ascii(sample[\"question\"]))\n",
    "    a = clean_text(normalize_to_ascii(sample[\"answer\"]))\n",
    "    question_list.append(q)\n",
    "    answer_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ba5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver('../corpus/hotpot_noRed_raw', [context_list, question_list, answer_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
