{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.tokenizer import TokenizerBPE\n",
    "from src.utils import saver, loader\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e726f5",
   "metadata": {},
   "source": [
    "## Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a1af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_train_list = loader(\"corpus/cnn_dailymail_highlight_train_cleaned.pkl\")\n",
    "article_train_list = loader(\"corpus/cnn_dailymail_article_train_cleaned.pkl\")\n",
    "\n",
    "highlight_test_list = loader(\"corpus/cnn_dailymail_highlight_test_cleaned.pkl\")\n",
    "article_test_list = loader(\"corpus/cnn_dailymail_article_test_cleaned.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9d614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the corpus: 1288375458\n"
     ]
    }
   ],
   "source": [
    "corpus = highlight_train_list + article_train_list + highlight_test_list + article_test_list\n",
    "print(f\"Total number of characters in the corpus: {len(\"\".join(corpus))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70993e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create character tokenizer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer = TokenizerBPE(corpus=corpus, \n\u001b[32m      2\u001b[39m                          num_merges=\u001b[32m24000\u001b[39m,  \u001b[38;5;66;03m# do 24k merges, resulting in ~24k tokens\u001b[39;00m\n\u001b[32m      3\u001b[39m                          ratio=\u001b[32m0.1\u001b[39m,         \u001b[38;5;66;03m# tokenize 10% of words in corpus\u001b[39;00m\n\u001b[32m      4\u001b[39m                          verbose=\u001b[38;5;28;01mTrue\u001b[39;00m       \u001b[38;5;66;03m# print merge details\u001b[39;00m\n\u001b[32m      5\u001b[39m                          )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/tokenizer.py:36\u001b[39m, in \u001b[36mTokenizerBPE.__init__\u001b[39m\u001b[34m(self, corpus, num_merges, ratio, verbose)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, corpus, num_merges, ratio=\u001b[38;5;28;01mNone\u001b[39;00m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreate character tokenizer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer_char = TokenizerChar(corpus)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m.token_to_idx = \u001b[38;5;28mself\u001b[39m.tokenizer_char.token_to_idx\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.idx_to_token = {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_to_idx.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/tokenizer.py:17\u001b[39m, in \u001b[36mTokenizerChar.__init__\u001b[39m\u001b[34m(self, corpus)\u001b[39m\n\u001b[32m     15\u001b[39m     words = \u001b[38;5;28mlist\u001b[39m(line)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         corpus_flatten.extend(word)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(corpus_flatten)))\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vocab)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerBPE(corpus=corpus, \n",
    "                         num_merges=24000,  # do 24k merges, resulting in ~24k tokens\n",
    "                         ratio=0.1,         # tokenize 10% of words in corpus\n",
    "                         verbose=True       # print merge details\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6633e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.add_special_tokens([\"<s>\", \"<h>\", \"<b>\"])  # add special tokens\n",
    "\n",
    "saver(\"cnn_tokenizer.pkl\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505c36f",
   "metadata": {},
   "source": [
    "## Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7092cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"cnn_tokenizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "909f1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(corpus_list):\n",
    "    corpus_list_new = []\n",
    "    for entry in tqdm(corpus_list, desc=\"Adding start and stop tokens\"):\n",
    "        highlight, article = entry\n",
    "        new_entry = f\"<s><h>{highlight}<b>{article}\"\n",
    "        corpus_list_new.append(new_entry)\n",
    "\n",
    "    return \"\".join(corpus_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c1ee5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91eef1832444bbaa3ba82306afdf6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding start and stop tokens:   0%|          | 0/287113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train = add_special_tokens(list(zip(highlight_train_list, article_train_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f5b48c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310482287\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_train[:length//4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7c065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a13d9c54ff048ada6c4545d06cd76be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m corpus_train_tokens = tokenizer.encode(corpus_train[:length//\u001b[32m6\u001b[39m], verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, pre_merge=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/tokenizer.py:88\u001b[39m, in \u001b[36mTokenizerBPE.encode\u001b[39m\u001b[34m(self, text, verbose, pre_merge)\u001b[39m\n\u001b[32m     85\u001b[39m     indices = \u001b[38;5;28mself\u001b[39m.pre_merge(indices)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (idx1, idx2), new_idx \u001b[38;5;129;01min\u001b[39;00m decorator(\u001b[38;5;28mself\u001b[39m.merge_list):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28mslice\u001b[39m = np.where(np.logical_and(indices[:-\u001b[32m1\u001b[39m] == idx1,  indices[\u001b[32m1\u001b[39m:] == idx2))\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mslice\u001b[39m[\u001b[32m0\u001b[39m]) > \u001b[32m0\u001b[39m:\n\u001b[32m     90\u001b[39m         indices[:-\u001b[32m1\u001b[39m][\u001b[38;5;28mslice\u001b[39m] = new_idx\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a34776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8573493f84474aaea32f8120f2d375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/287113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m article_train_tokens = [torch.tensor(tokenizer.encode(entry)) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m tqdm(article_train_list)]\n\u001b[32m      2\u001b[39m saver(\u001b[33m\"\u001b[39m\u001b[33mcorpus/cnn_dailymail_article_train_tokens.pkl\u001b[39m\u001b[33m\"\u001b[39m, article_train_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/tokenizer.py:88\u001b[39m, in \u001b[36mTokenizerBPE.encode\u001b[39m\u001b[34m(self, text, verbose, pre_merge)\u001b[39m\n\u001b[32m     85\u001b[39m     indices = \u001b[38;5;28mself\u001b[39m.pre_merge(indices)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (idx1, idx2), new_idx \u001b[38;5;129;01min\u001b[39;00m decorator(\u001b[38;5;28mself\u001b[39m.merge_list):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28mslice\u001b[39m = np.where(np.logical_and(indices[:-\u001b[32m1\u001b[39m] == idx1,  indices[\u001b[32m1\u001b[39m:] == idx2))\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mslice\u001b[39m[\u001b[32m0\u001b[39m]) > \u001b[32m0\u001b[39m:\n\u001b[32m     90\u001b[39m         indices[:-\u001b[32m1\u001b[39m][\u001b[38;5;28mslice\u001b[39m] = new_idx\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "article_train_tokens = [torch.tensor(tokenizer.encode(entry)) for entry in tqdm(article_train_list)]\n",
    "saver(\"corpus/cnn_dailymail_article_train_tokens.pkl\", article_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d54bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c995bb8dcae64179aafd9c79d53eb65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_test_tokens = [torch.tensor(tokenizer.encode(entry)) for entry in tqdm(highlight_test_list[:100])]\n",
    "saver(\"corpus/cnn_dailymail_highlight_test_tokens.pkl\", highlight_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_train_tokens = [torch.tensor(tokenizer.encode(entry)) for entry in tqdm(article_train_list)]\n",
    "saver(\"corpus/cnn_dailymail_article_train_tokens.pkl\", article_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
