{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.tokenizer import TokenizerBPE\n",
    "from src.utils import saver, loader\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e726f5",
   "metadata": {},
   "source": [
    "## Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a1af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_train_list = loader(\"corpus/cnn_dailymail_highlight_train_cleaned.pkl\")\n",
    "article_train_list = loader(\"corpus/cnn_dailymail_article_train_cleaned.pkl\")\n",
    "\n",
    "highlight_test_list = loader(\"corpus/cnn_dailymail_highlight_test_cleaned.pkl\")\n",
    "article_test_list = loader(\"corpus/cnn_dailymail_article_test_cleaned.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9d614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the corpus: 1288375458\n"
     ]
    }
   ],
   "source": [
    "corpus = highlight_train_list + article_train_list + highlight_test_list + article_test_list\n",
    "print(f\"Total number of characters in the corpus: {len(\"\".join(corpus))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70993e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create character tokenizer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer = TokenizerBPE(corpus=corpus, \n\u001b[32m      2\u001b[39m                          num_merges=\u001b[32m24000\u001b[39m,  \u001b[38;5;66;03m# do 24k merges, resulting in ~24k tokens\u001b[39;00m\n\u001b[32m      3\u001b[39m                          ratio=\u001b[32m0.1\u001b[39m,         \u001b[38;5;66;03m# tokenize 10% of words in corpus\u001b[39;00m\n\u001b[32m      4\u001b[39m                          verbose=\u001b[38;5;28;01mTrue\u001b[39;00m       \u001b[38;5;66;03m# print merge details\u001b[39;00m\n\u001b[32m      5\u001b[39m                          )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/tokenizer.py:36\u001b[39m, in \u001b[36mTokenizerBPE.__init__\u001b[39m\u001b[34m(self, corpus, num_merges, ratio, verbose)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, corpus, num_merges, ratio=\u001b[38;5;28;01mNone\u001b[39;00m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreate character tokenizer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer_char = TokenizerChar(corpus)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m.token_to_idx = \u001b[38;5;28mself\u001b[39m.tokenizer_char.token_to_idx\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.idx_to_token = {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_to_idx.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/tokenizer.py:17\u001b[39m, in \u001b[36mTokenizerChar.__init__\u001b[39m\u001b[34m(self, corpus)\u001b[39m\n\u001b[32m     15\u001b[39m     words = \u001b[38;5;28mlist\u001b[39m(line)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         corpus_flatten.extend(word)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(corpus_flatten)))\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.vocab)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerBPE(corpus=corpus, \n",
    "                         num_merges=24000,  # do 24k merges, resulting in ~24k tokens\n",
    "                         ratio=0.1,         # tokenize 10% of words in corpus\n",
    "                         verbose=True       # print merge details\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special tokens\n",
    "tokenizer.add_special_tokens([\"<s>\",  # start\n",
    "                              \"</s>\", # end\n",
    "                              \"<h>\",  # highlight\n",
    "                              \"<b>\"]) # body                          \n",
    "\n",
    "saver(\"cnn_tokenizer.pkl\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505c36f",
   "metadata": {},
   "source": [
    "## Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7092cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"cnn_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909f1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(corpus_list):\n",
    "    corpus_list_new = []\n",
    "    for entry in tqdm(corpus_list, desc=\"Adding start and stop tokens\"):\n",
    "        highlight, article = entry\n",
    "        new_entry = f\"<s><h>{highlight}<b>{article}</s>\"\n",
    "        corpus_list_new.append(new_entry)\n",
    "\n",
    "    return \"\".join(corpus_list_new)\n",
    "\n",
    "def add_special_tokens_HLlast(corpus_list):\n",
    "    corpus_list_new = []\n",
    "    for entry in tqdm(corpus_list, desc=\"Adding start and stop tokens\"):\n",
    "        highlight, article = entry\n",
    "        new_entry = f\"<s><b>{article}<h>{highlight}</s>\"\n",
    "        corpus_list_new.append(new_entry)\n",
    "\n",
    "    return \"\".join(corpus_list_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee697ae0",
   "metadata": {},
   "source": [
    "## Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1ee5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd0a57d5a7441a2be31a6922cb868a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding start and stop tokens:   0%|          | 0/287113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train = add_special_tokens(list(zip(highlight_train_list, article_train_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a7c065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ceeaef75944c19a6fc5aeb28285685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[:length//4], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_article_train_tokens1.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc265b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47fc91ddff04c94a908194970355d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//4:length//2], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_article_train_tokens2.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b101de9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30383f613010466d958eac2c9576fa67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//2:3*length//4], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_article_train_tokens3.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c5b862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0898ef4abdc444ef94ba55e7ffac0a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[3*length//4:], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_article_train_tokens4.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563cb3b5",
   "metadata": {},
   "source": [
    "### Highlight Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0408d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf2e7a716494c4caa0eb73372d92631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding start and stop tokens:   0%|          | 0/287113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train = add_special_tokens_HLlast(list(zip(highlight_train_list, article_train_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1b24ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9fa36590d140e7bb23c987142fc250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[:length//4], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_HLlast_train_tokens1.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf2b036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388ad4383ee944f69369a7bc16499b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//4:length//2], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_HLlast_train_tokens2.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c3628a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9564f832904b55bbb5f6d6ccb0dcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[length//2:3*length//4], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_HLlast_train_tokens3.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0431acad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f7ee602bc249c4ba3059333f82c3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_train_tokens = tokenizer.encode(corpus_train[3*length//4:], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_HLlast_train_tokens4.pkl\", corpus_train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c91cb",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "090ecf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc52ed812dda4d70b6d6767d8bd747e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding start and stop tokens:   0%|          | 0/11490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_test = add_special_tokens(list(zip(highlight_test_list, article_test_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ef8e9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d9406b834a46d89378739b8324f6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_test_tokens = tokenizer.encode(corpus_test[:1000000], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_article_test_tokens.pkl\", corpus_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54bca63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86667a1cdd684217bbf2cd04a8025bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding start and stop tokens:   0%|          | 0/11490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_test = add_special_tokens_HLlast(list(zip(highlight_test_list, article_test_list)))\n",
    "length = len(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "124082fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9453b549dda8464bae2dee3428865ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_test_tokens = tokenizer.encode(corpus_test[:1000000], verbose=True)\n",
    "saver(\"corpus/cnn_dailymail_HLlast_test_tokens.pkl\", corpus_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
