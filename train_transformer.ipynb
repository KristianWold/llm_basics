{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from src.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from src.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"cnn_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens4.pkl\"))\n",
    "corpus_train_a = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens4.pkl\"))\n",
    "corpus_train_b = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "corpus_train = torch.cat((corpus_train_a, corpus_train_b), dim=0)\n",
    "\n",
    "corpus_test_a = torch.tensor(loader(\"corpus/cnn_dailymail_article_test_tokens.pkl\"))\n",
    "corpus_test_b = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_test_tokens.pkl\"))\n",
    "corpus_test = torch.cat((corpus_test_a, corpus_test_b), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024, offset=None):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    # overlapping batches augmentation\n",
    "    if offset is not None:\n",
    "        corpus_offset = corpus_truncated[offset : offset - batch_length]\n",
    "        corpus_offset = corpus_offset.view(-1, batch_length)  # reshape into batches\n",
    "        corpus_batched = torch.cat((corpus_batched, corpus_offset), dim=0)  # concatenate the offset batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024, offset=None)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024, offset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=tokenizer.token_to_idx[\"<s>\"],\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "loss_train_list = []\n",
    "loss_eval_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7c59",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5961af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = load_checkpoint(\"checkpoint_transformer.pth\", model, optimizer, scheduler)\n",
    "loss_train_list = loader(\"loss_train.pkl\")\n",
    "loss_eval_list = loader(\"loss_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cbb577f2da4942a66ee46fdf045f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 3.0975, Loss_eval: 3.4277, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 3.2132, Loss_eval: 3.4315, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 3.2717, Loss_eval: 3.4318, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 3.2017, Loss_eval: 3.4303, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 3.0530, Loss_eval: 3.4410, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 3.0337, Loss_eval: 3.4405, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 2.9209, Loss_eval: 3.4260, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 3.2802, Loss_eval: 3.4172, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 3.2067, Loss_eval: 3.4438, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 3.3009, Loss_eval: 3.4319, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 3.2218, Loss_eval: 3.4242, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 3.2827, Loss_eval: 3.4163, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 3.2301, Loss_eval: 3.4125, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 3.1550, Loss_eval: 3.4158, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.1024, Loss_eval: 3.4134, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 3.5068, Loss_eval: 3.4086, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 3.1719, Loss_eval: 3.4433, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 3.2217, Loss_eval: 3.4161, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 3.2495, Loss_eval: 3.4407, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 3.2026, Loss_eval: 3.4367, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 3.2632, Loss_eval: 3.4238, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 3.2319, Loss_eval: 3.4150, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.2008, Loss_eval: 3.4296, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.1056, Loss_eval: 3.4224, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 3.3188, Loss_eval: 3.4285, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 3.2191, Loss_eval: 3.4368, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.1500, Loss_eval: 3.4167, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 3.0134, Loss_eval: 3.4200, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 3.4978, Loss_eval: 3.4084, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 3.4518, Loss_eval: 3.4336, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 3.2468, Loss_eval: 3.4238, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.1756, Loss_eval: 3.4093, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.2366, Loss_eval: 3.4089, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 3.0058, Loss_eval: 3.4250, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 3.0344, Loss_eval: 3.4417, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.1890, Loss_eval: 3.4419, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 3.1561, Loss_eval: 3.4346, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 3.2887, Loss_eval: 3.4280, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 3.4788, Loss_eval: 3.4153, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 3.2937, Loss_eval: 3.4249, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 3.0255, Loss_eval: 3.4233, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 2.9559, Loss_eval: 3.4126, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 3.1991, Loss_eval: 3.4293, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.1598, Loss_eval: 3.4400, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 3.3760, Loss_eval: 3.4191, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 2.9673, Loss_eval: 3.4197, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.2027, Loss_eval: 3.4250, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 3.2232, Loss_eval: 3.4313, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 3.2920, Loss_eval: 3.4317, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 3.3477, Loss_eval: 3.4355, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 3.0608, Loss_eval: 3.4171, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.1722, Loss_eval: 3.4199, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 3.3079, Loss_eval: 3.4308, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 2.9905, Loss_eval: 3.3941, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 3.4241, Loss_eval: 3.4071, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.1302, Loss_eval: 3.4200, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 3.2272, Loss_eval: 3.4234, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 3.1477, Loss_eval: 3.4185, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.2268, Loss_eval: 3.4148, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 3.3632, Loss_eval: 3.4309, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.0855, Loss_eval: 3.4377, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 3.4195, Loss_eval: 3.4368, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.2536, Loss_eval: 3.4057, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 3.3888, Loss_eval: 3.4074, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.0787, Loss_eval: 3.4341, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 2.9537, Loss_eval: 3.4174, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.2845, Loss_eval: 3.4107, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.0921, Loss_eval: 3.4331, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.2246, Loss_eval: 3.4255, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.2595, Loss_eval: 3.4390, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 3.3613, Loss_eval: 3.4115, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 2.9153, Loss_eval: 3.4098, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.1853, Loss_eval: 3.4088, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.1050, Loss_eval: 3.4373, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 3.3143, Loss_eval: 3.4043, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.0922, Loss_eval: 3.4294, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 3.5873, Loss_eval: 3.4191, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.1339, Loss_eval: 3.4243, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 3.4990, Loss_eval: 3.4081, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 3.3511, Loss_eval: 3.4133, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.1926, Loss_eval: 3.4100, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 3.0515, Loss_eval: 3.4135, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 3.0387, Loss_eval: 3.4058, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.3975, Loss_eval: 3.4163, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.2309, Loss_eval: 3.4142, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.1550, Loss_eval: 3.4083, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 3.2995, Loss_eval: 3.4276, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.1019, Loss_eval: 3.4166, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.2525, Loss_eval: 3.4194, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.1310, Loss_eval: 3.4206, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 3.0728, Loss_eval: 3.4431, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 3.1385, Loss_eval: 3.4222, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.2910, Loss_eval: 3.4256, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 3.3214, Loss_eval: 3.4298, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 2.8451, Loss_eval: 3.4053, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.1485, Loss_eval: 3.4040, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 2.6991, Loss_eval: 3.3845, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 3.4206, Loss_eval: 3.4247, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 3.1493, Loss_eval: 3.4115, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.1389, Loss_eval: 3.4195, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 3.3265, Loss_eval: 3.4015, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.1537, Loss_eval: 3.4003, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 3.2724, Loss_eval: 3.4258, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.1179, Loss_eval: 3.4360, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 3.4042, Loss_eval: 3.4231, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.3495, Loss_eval: 3.4111, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.1969, Loss_eval: 3.4166, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 3.0280, Loss_eval: 3.4143, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 3.3220, Loss_eval: 3.4152, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 3.3389, Loss_eval: 3.4201, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.2793, Loss_eval: 3.4244, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 2.8953, Loss_eval: 3.3860, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 3.6683, Loss_eval: 3.3845, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.1205, Loss_eval: 3.4192, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 3.2652, Loss_eval: 3.4068, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.1218, Loss_eval: 3.3923, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 3.1932, Loss_eval: 3.4078, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 3.3026, Loss_eval: 3.4111, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 2.9803, Loss_eval: 3.4252, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 3.1377, Loss_eval: 3.4035, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 3.1351, Loss_eval: 3.4316, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 2.8648, Loss_eval: 3.4150, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 3.0224, Loss_eval: 3.4188, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.1696, Loss_eval: 3.4245, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 3.0569, Loss_eval: 3.4119, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.2747, Loss_eval: 3.4148, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 3.3653, Loss_eval: 3.4264, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 2.9811, Loss_eval: 3.4148, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 3.0546, Loss_eval: 3.4144, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 2.9344, Loss_eval: 3.3966, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 2.8612, Loss_eval: 3.4294, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 3.0980, Loss_eval: 3.4119, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 2.8803, Loss_eval: 3.3938, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 3.4897, Loss_eval: 3.4115, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 2.8747, Loss_eval: 3.3953, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.1788, Loss_eval: 3.4246, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 3.0616, Loss_eval: 3.4288, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.1366, Loss_eval: 3.4016, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 3.0029, Loss_eval: 3.4166, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 2.9853, Loss_eval: 3.3988, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.2034, Loss_eval: 3.3965, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.2530, Loss_eval: 3.3918, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 2.8657, Loss_eval: 3.4031, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 3.0627, Loss_eval: 3.4114, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.0857, Loss_eval: 3.3975, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.3330, Loss_eval: 3.3760, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.1219, Loss_eval: 3.3860, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.2122, Loss_eval: 3.4128, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.2460, Loss_eval: 3.4003, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 2.9887, Loss_eval: 3.4273, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 2.9834, Loss_eval: 3.4072, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 2.8017, Loss_eval: 3.4159, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 3.0255, Loss_eval: 3.4066, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 3.2431, Loss_eval: 3.4019, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 2.9228, Loss_eval: 3.4235, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 3.2702, Loss_eval: 3.4140, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 3.0431, Loss_eval: 3.4061, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 2.9486, Loss_eval: 3.3903, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 3.0286, Loss_eval: 3.4188, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 3.4639, Loss_eval: 3.4199, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 3.2848, Loss_eval: 3.4175, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 2.8660, Loss_eval: 3.4179, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 3.5454, Loss_eval: 3.4120, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 3.2441, Loss_eval: 3.4160, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 2.9310, Loss_eval: 3.4125, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 3.3680, Loss_eval: 3.4099, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 3.0963, Loss_eval: 3.3969, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 2.9852, Loss_eval: 3.4199, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 3.0224, Loss_eval: 3.4066, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 3.3483, Loss_eval: 3.3885, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 3.3465, Loss_eval: 3.4086, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 3.1261, Loss_eval: 3.4146, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 3.3577, Loss_eval: 3.3913, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 3.0753, Loss_eval: 3.3813, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 2.7903, Loss_eval: 3.4231, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 2.9271, Loss_eval: 3.4142, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 3.1507, Loss_eval: 3.3942, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 3.0878, Loss_eval: 3.4166, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 3.1922, Loss_eval: 3.3829, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 3.0724, Loss_eval: 3.4189, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 3.1919, Loss_eval: 3.4291, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 3.0419, Loss_eval: 3.3889, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 3.1596, Loss_eval: 3.4069, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 3.1784, Loss_eval: 3.3915, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 3.5725, Loss_eval: 3.4269, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 3.2989, Loss_eval: 3.4059, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 3.4121, Loss_eval: 3.4059, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 3.0160, Loss_eval: 3.4159, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 3.2826, Loss_eval: 3.4081, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 3.1699, Loss_eval: 3.4295, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 3.0487, Loss_eval: 3.4357, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 2.9784, Loss_eval: 3.4022, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 3.2023, Loss_eval: 3.4155, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 3.0752, Loss_eval: 3.4140, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 3.2911, Loss_eval: 3.4020, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 2.7648, Loss_eval: 3.4183, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 3.0414, Loss_eval: 3.3839, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 2.8742, Loss_eval: 3.3933, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 3.2622, Loss_eval: 3.3996, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 2.9065, Loss_eval: 3.4054, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 3.1270, Loss_eval: 3.4059, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 3.2291, Loss_eval: 3.3783, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 3.0191, Loss_eval: 3.3997, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 3.1673, Loss_eval: 3.4259, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 3.3866, Loss_eval: 3.4203, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 3.3210, Loss_eval: 3.4139, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 3.2103, Loss_eval: 3.4199, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 3.0918, Loss_eval: 3.4116, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 3.3616, Loss_eval: 3.3855, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 3.0126, Loss_eval: 3.4034, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 2.9394, Loss_eval: 3.4002, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 3.1686, Loss_eval: 3.3956, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 2.8722, Loss_eval: 3.3793, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 3.0236, Loss_eval: 3.4085, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 3.3631, Loss_eval: 3.3883, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 2.7942, Loss_eval: 3.4077, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 2.8638, Loss_eval: 3.4057, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 2.7951, Loss_eval: 3.4115, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 3.0835, Loss_eval: 3.4165, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 3.2698, Loss_eval: 3.4031, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 3.1258, Loss_eval: 3.3998, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 3.4821, Loss_eval: 3.3983, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 2.9457, Loss_eval: 3.4039, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 3.0652, Loss_eval: 3.4030, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 3.1889, Loss_eval: 3.3880, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 3.2083, Loss_eval: 3.4148, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 3.1856, Loss_eval: 3.3906, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 3.2460, Loss_eval: 3.3879, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 3.8042, Loss_eval: 3.3874, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 3.1379, Loss_eval: 3.4265, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 3.2719, Loss_eval: 3.3925, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 3.3608, Loss_eval: 3.3947, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 3.0967, Loss_eval: 3.4141, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 3.2499, Loss_eval: 3.3987, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 3.2867, Loss_eval: 3.4282, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 3.0779, Loss_eval: 3.4253, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 3.4709, Loss_eval: 3.3994, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 3.3130, Loss_eval: 3.4037, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 3.2076, Loss_eval: 3.3861, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 2.9958, Loss_eval: 3.4081, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 3.3392, Loss_eval: 3.4064, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 3.1055, Loss_eval: 3.4197, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 3.2633, Loss_eval: 3.4009, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 3.2634, Loss_eval: 3.3898, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 3.2817, Loss_eval: 3.3972, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 3.0165, Loss_eval: 3.4053, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 3.1105, Loss_eval: 3.3979, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 3.5225, Loss_eval: 3.4140, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 2.9483, Loss_eval: 3.4128, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 3.1086, Loss_eval: 3.3924, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 3.0217, Loss_eval: 3.3894, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 3.3522, Loss_eval: 3.3898, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 3.0493, Loss_eval: 3.3947, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 3.0175, Loss_eval: 3.3853, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 2.8561, Loss_eval: 3.4096, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 2.9364, Loss_eval: 3.4003, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 3.1173, Loss_eval: 3.3910, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 3.1290, Loss_eval: 3.3788, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 3.2088, Loss_eval: 3.3860, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 3.4506, Loss_eval: 3.3673, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 2.9926, Loss_eval: 3.3755, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 3.0497, Loss_eval: 3.3842, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 3.0714, Loss_eval: 3.3951, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 3.3157, Loss_eval: 3.3951, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 3.2124, Loss_eval: 3.3985, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 2.7482, Loss_eval: 3.3864, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 3.5073, Loss_eval: 3.3939, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 3.1355, Loss_eval: 3.4032, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 3.3502, Loss_eval: 3.3812, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 3.2366, Loss_eval: 3.4014, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 3.1248, Loss_eval: 3.3851, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 3.0559, Loss_eval: 3.4059, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 3.4915, Loss_eval: 3.3715, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 3.2936, Loss_eval: 3.3756, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 3.1010, Loss_eval: 3.3816, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 2.9120, Loss_eval: 3.3762, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 3.4125, Loss_eval: 3.3873, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 3.0751, Loss_eval: 3.3972, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 3.4710, Loss_eval: 3.3993, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 3.2387, Loss_eval: 3.3945, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 3.1124, Loss_eval: 3.3820, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 3.1217, Loss_eval: 3.4038, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 3.2951, Loss_eval: 3.3871, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 3.3216, Loss_eval: 3.3882, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 3.0646, Loss_eval: 3.3941, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 3.0017, Loss_eval: 3.3811, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 3.1966, Loss_eval: 3.3665, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 3.2919, Loss_eval: 3.4032, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 2.9324, Loss_eval: 3.3962, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 3.1002, Loss_eval: 3.4062, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 3.2814, Loss_eval: 3.3888, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 3.4484, Loss_eval: 3.3620, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 3.1411, Loss_eval: 3.3922, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 3.0083, Loss_eval: 3.3899, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 3.1408, Loss_eval: 3.3884, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 3.3709, Loss_eval: 3.3886, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 3.1891, Loss_eval: 3.3773, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 3.4418, Loss_eval: 3.3978, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 2.9490, Loss_eval: 3.3760, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 3.1449, Loss_eval: 3.3873, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 2.9465, Loss_eval: 3.3891, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 3.5994, Loss_eval: 3.3831, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 3.3476, Loss_eval: 3.3858, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 3.0751, Loss_eval: 3.3906, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 3.0549, Loss_eval: 3.3975, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 3.4969, Loss_eval: 3.3748, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 3.0836, Loss_eval: 3.3880, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 3.1544, Loss_eval: 3.3913, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 3.1644, Loss_eval: 3.3829, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 3.4133, Loss_eval: 3.3923, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.1069, Loss_eval: 3.3759, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 3.3528, Loss_eval: 3.3923, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.1044, Loss_eval: 3.3874, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 3.4468, Loss_eval: 3.3994, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 3.3232, Loss_eval: 3.3915, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 2.9637, Loss_eval: 3.3800, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 2.9196, Loss_eval: 3.3685, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 3.0930, Loss_eval: 3.3792, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 3.2791, Loss_eval: 3.3741, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 3.1221, Loss_eval: 3.3980, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 2.9576, Loss_eval: 3.3910, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.1171, Loss_eval: 3.3658, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.0954, Loss_eval: 3.4020, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 3.1359, Loss_eval: 3.3638, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 3.2717, Loss_eval: 3.3905, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 3.1515, Loss_eval: 3.3916, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 3.2233, Loss_eval: 3.3858, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 2.9727, Loss_eval: 3.3679, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 3.0712, Loss_eval: 3.4143, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 3.1279, Loss_eval: 3.3993, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 2.9436, Loss_eval: 3.4081, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 3.0324, Loss_eval: 3.3708, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 3.0666, Loss_eval: 3.3871, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 3.0530, Loss_eval: 3.3820, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 3.4980, Loss_eval: 3.3827, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 3.3059, Loss_eval: 3.4011, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 3.2348, Loss_eval: 3.4042, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 3.1840, Loss_eval: 3.3809, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 3.2965, Loss_eval: 3.3767, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 2.9568, Loss_eval: 3.3782, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 2.9324, Loss_eval: 3.3733, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 3.3851, Loss_eval: 3.3986, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 2.9241, Loss_eval: 3.3788, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 3.0829, Loss_eval: 3.3638, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 3.4182, Loss_eval: 3.3669, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.1185, Loss_eval: 3.4014, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 3.1810, Loss_eval: 3.3794, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 2.8921, Loss_eval: 3.3997, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 3.2912, Loss_eval: 3.3805, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 3.0469, Loss_eval: 3.3743, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 80\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_eval = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.4f}, Loss_eval: {loss_eval:<.4f}, Learning Rate: {lr:4e}\")\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_eval_list.append(loss_eval)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler, \n",
    "                            filename=\"checkpoint_transformer.pth\")\n",
    "            saver(\"loss_train.pkl\", loss_train_list)\n",
    "            saver(\"loss_eval.pkl\", loss_eval_list)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler, \n",
    "                    filename=\"checkpoint_transformer.pth\")\n",
    "    saver(\"loss_train.pkl\", loss_train_list)\n",
    "    saver(\"loss_eval.pkl\", loss_eval_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "988c0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model, tokenizer, context_length, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def run(self, text, T, k, mode=None):\n",
    "        if mode == \"summary\":\n",
    "            text = \"<s><b>\" + text + \"<h>\"\n",
    "        elif mode == \"expand\":\n",
    "            text = \"<s><h>\" + text + \"<b>\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        tokens = torch.tensor(self.tokenizer.encode(text.lower()), dtype=torch.long).reshape(1, -1).to(self.device)\n",
    "\n",
    "        self.display = Display()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.context_length):\n",
    "                next = self.next_token(tokens, T, k,)\n",
    "\n",
    "                tokens = torch.cat([tokens, next.reshape(1,1)], dim=1)\n",
    "                text = tokenizer.decode(tokens[0].tolist())\n",
    "                self.display.update(text)\n",
    "\n",
    "                if next[0] == tokenizer.token_to_idx[\"</s>\"]:\n",
    "                    break\n",
    "                \n",
    "\n",
    "    def next_token(self, tokens, T, k):\n",
    "        logits = self.model(tokens)[0, -1:]\n",
    "        topk_vals, _    = torch.topk(logits, k=k)\n",
    "        kth_value       = topk_vals[:,-1]\n",
    "\n",
    "        logits = torch.where(logits >= kth_value, logits, -torch.inf)\n",
    "        dist = Categorical(logits=logits/T)\n",
    "        next = dist.sample()\n",
    "\n",
    "        return next\n",
    "\n",
    "\n",
    "class Display:\n",
    "    def __init__(self):\n",
    "        self.wrapper = textwrap.TextWrapper(width=80)\n",
    "        self.ta = widgets.Textarea(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(width='80ch', height='20em'),\n",
    "            disabled=True\n",
    "        )\n",
    "        display(self.ta)\n",
    "\n",
    "    def update(self, text):\n",
    "        self.ta.value = self.wrapper.fill(text.replace(\"\\n\", \" \"))  # this updates in-place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e36d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = Inference(model, tokenizer, context_length=1024, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a422ed4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594657902a2e4622a216134f71db4ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"<s><h>Israel attacks Palestine\"\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "inference.run(text, T, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
