{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from src.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from src.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"cnn_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens1.pkl\"))\n",
    "#corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens2.pkl\"))\n",
    "#corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens3.pkl\"))\n",
    "#corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens4.pkl\"))\n",
    "#corpus_train_a = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "#corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens1.pkl\"))\n",
    "#corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens2.pkl\"))\n",
    "#corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens3.pkl\"))\n",
    "#corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens4.pkl\"))\n",
    "#corpus_train_b = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "corpus_test_a = torch.tensor(loader(\"corpus/cnn_dailymail_article_test_tokens.pkl\"))\n",
    "corpus_test_b = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_test_tokens.pkl\"))\n",
    "corpus_test = torch.cat((corpus_test_a, corpus_test_b), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024, offset=None):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    # overlapping batches augmentation\n",
    "    if offset is not None:\n",
    "        corpus_offset = corpus_truncated[offset : offset - batch_length]\n",
    "        corpus_offset = corpus_offset.view(-1, batch_length)  # reshape into batches\n",
    "        corpus_batched = torch.cat((corpus_batched, corpus_offset), dim=0)  # concatenate the offset batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024, offset=None)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024, offset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*2\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 2\n",
    "tf_blocks = 2\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=tokenizer.token_to_idx[\"<s>\"],\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "loss_train_list = []\n",
    "loss_eval_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83fc4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model, tokenizer, context_length, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def run(self, text, T, k, mode=None):\n",
    "        if mode == \"summary\":\n",
    "            text = \"<s><b>\" + text + \"<h>\"\n",
    "        elif mode == \"expand\":\n",
    "            text = \"<s><h>\" + text + \"<b>\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        tokens = torch.tensor(self.tokenizer.encode(text.lower()), dtype=torch.long).reshape(1, -1).to(self.device)\n",
    "\n",
    "        self.display = Display()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.context_length):\n",
    "                next = self.next_token(tokens, T, k,)\n",
    "\n",
    "                tokens = torch.cat([tokens, next.reshape(1,1)], dim=1)\n",
    "                text = tokenizer.decode(tokens[0].tolist())\n",
    "                self.display.update(text)\n",
    "\n",
    "                if next[0] == tokenizer.token_to_idx[\"</s>\"]:\n",
    "                    break\n",
    "                \n",
    "\n",
    "    def next_token(self, tokens, T, k):\n",
    "        logits = self.model(tokens)[0, -1:]\n",
    "        topk_vals, _    = torch.topk(logits, k=k)\n",
    "        kth_value       = topk_vals[:,-1]\n",
    "\n",
    "        logits = torch.where(logits >= kth_value, logits, -torch.inf)\n",
    "        dist = Categorical(logits=logits/T)\n",
    "        next = dist.sample()\n",
    "\n",
    "        return next\n",
    "\n",
    "\n",
    "class Display:\n",
    "    def __init__(self):\n",
    "        self.wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "        self.ta = widgets.Textarea(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(width='80ch', height='20em'),\n",
    "            disabled=True\n",
    "        )\n",
    "        display(self.ta)\n",
    "\n",
    "    def update(self, text):\n",
    "        self.ta.value = self.wrapper.fill(text.replace(\"\\n\", \" \"))  # this updates in-place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c4c7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = Inference(model, tokenizer, context_length=1024, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "962b0b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb17c4fd1504f01850f14995a155921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1025) must match the size of tensor b (1024) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m T = \u001b[32m1\u001b[39m\n\u001b[32m      4\u001b[39m k = \u001b[32m50\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m inference.run(text, T, k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mInference.run\u001b[39m\u001b[34m(self, text, T, k, mode)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.context_length):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         \u001b[38;5;28mnext\u001b[39m = \u001b[38;5;28mself\u001b[39m.next_token(tokens, T, k,)\n\u001b[32m     31\u001b[39m         tokens = torch.cat([tokens, \u001b[38;5;28mnext\u001b[39m.reshape(\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     32\u001b[39m         text = tokenizer.decode(tokens[\u001b[32m0\u001b[39m].tolist())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mInference.next_token\u001b[39m\u001b[34m(self, tokens, T, k)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnext_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, T, k):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.model(tokens)[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m:]\n\u001b[32m     41\u001b[39m     topk_vals, _    = torch.topk(logits, k=k)\n\u001b[32m     42\u001b[39m     kth_value       = topk_vals[:,-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/env_pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/env_pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/transformer.py:59\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m     57\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embed(tokens)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer_list:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     x = block(x, mask)\n\u001b[32m     61\u001b[39m x = \u001b[38;5;28mself\u001b[39m.unembed(x)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/env_pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/env_pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/transformer.py:136\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x_embeds, mask)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_embeds, mask):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     x_embeds = \u001b[38;5;28mself\u001b[39m.attention(x_embeds, mask)\n\u001b[32m    138\u001b[39m     x_embeds = \u001b[38;5;28mself\u001b[39m.ffnn(x_embeds)\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/slarn_chatbot/src/transformer.py:163\u001b[39m, in \u001b[36mTransformerBlock.attention\u001b[39m\u001b[34m(self, x_embeds, mask)\u001b[39m\n\u001b[32m    159\u001b[39m attn = torch.matmul(x_q, x_k.transpose(-\u001b[32m1\u001b[39m, -\u001b[32m2\u001b[39m))  \u001b[38;5;66;03m# [batch, heads, seq, seq]\u001b[39;00m\n\u001b[32m    160\u001b[39m attn = attn / math.sqrt(\u001b[38;5;28mself\u001b[39m.head_dim)  \u001b[38;5;66;03m# scale attention scores\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m attn_masked = attn.masked_fill(mask, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m\"\u001b[39m))  \u001b[38;5;66;03m# [batch, heads, seq, seq]\u001b[39;00m\n\u001b[32m    164\u001b[39m attn_masked = F.softmax(attn_masked, dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# softmax over the last dimension\u001b[39;00m\n\u001b[32m    165\u001b[39m attn_masked = \u001b[38;5;28mself\u001b[39m.do_attn(attn_masked)  \u001b[38;5;66;03m# dropout \u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1025) must match the size of tensor b (1024) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "text = \"<s><h>A magical horse was spotted in England. It was found in a field. The magical horse is able to breath fire. Scientists don't know where it came from.<b>\"\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "inference.run(text, T, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c0f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
