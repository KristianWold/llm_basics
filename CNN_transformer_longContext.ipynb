{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "#from src.tokenizer import TokenizerBPE, fuse_tokenized_corpus, chunk_corpus\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e38fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = pkl.load(open('corpus/corpus_CNN_24k_whitespace_train_numpy', 'rb'))\n",
    "corpus_train = torch.tensor(corpus_train, dtype=torch.int64)\n",
    "corpus_train = TensorDataset(corpus_train)\n",
    "\n",
    "\n",
    "corpus_test = pkl.load(open('corpus/corpus_CNN_24k_whitespace_test_numpy', 'rb'))\n",
    "corpus_test = torch.tensor(corpus_test, dtype=torch.int64)\n",
    "corpus_test = TensorDataset(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train,\n",
    "    batch_size=8,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=False     # whether to drop the tail batch if smaller than batch_size\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test,\n",
    "    batch_size=8,\n",
    "    shuffle=True,      \n",
    "    drop_last=False\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64*10\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 10\n",
    "tf_blocks = 10\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=24072,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    pad_token_id=None,\n",
    "    start_token_id=24070\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ffd2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = pkl.load(open(\"model.model\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0a56ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def forward_and_loss(model, batch, criterion):\n",
    "    model.train()\n",
    "    #batch is a tensor of shape [batch, seq]\n",
    "    src, tgt = batch[:, :-1].to(device), batch[:, 1:].to(device)\n",
    "    logits = model(src)\n",
    "    return criterion(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1))\n",
    "\n",
    "# 2) Do the zero_grad/backward/step around it\n",
    "def train_step(model, batch, optimizer, criterion, i):\n",
    "    # zero the grads in Python (not inside the compiled graph)\n",
    "    #optimizer.zero_grad()\n",
    "\n",
    "    # call compiled forward+loss\n",
    "    loss = forward_and_loss(model, batch, criterion)\n",
    "\n",
    "    # backward & step in Python\n",
    "    loss.backward()\n",
    "\n",
    "    if i%8 == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d4661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c668b4a1a8634737959f0b60ae2962e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 0.0629\n",
      "Step 8, Loss: 0.0609\n",
      "Step 16, Loss: 0.0563\n",
      "Step 24, Loss: 0.0448\n",
      "Step 32, Loss: 0.0447\n",
      "Step 40, Loss: 0.0353\n",
      "Step 48, Loss: 0.0747\n",
      "Step 56, Loss: 0.0514\n",
      "Step 64, Loss: 0.0742\n",
      "Step 72, Loss: 0.0405\n",
      "Step 80, Loss: 0.0695\n",
      "Step 88, Loss: 0.0502\n",
      "Step 96, Loss: 0.0578\n",
      "Step 104, Loss: 0.0441\n",
      "Step 112, Loss: 0.0408\n",
      "Step 120, Loss: 0.0635\n",
      "Step 128, Loss: 0.0340\n",
      "Step 136, Loss: 0.0678\n",
      "Step 144, Loss: 0.0366\n",
      "Step 152, Loss: 0.0400\n",
      "Step 160, Loss: 0.0544\n",
      "Step 168, Loss: 0.0496\n",
      "Step 176, Loss: 0.0514\n",
      "Step 184, Loss: 0.0583\n",
      "Step 192, Loss: 0.0602\n",
      "Step 200, Loss: 0.0463\n",
      "Step 208, Loss: 0.0559\n",
      "Step 216, Loss: 0.0444\n",
      "Step 224, Loss: 0.0427\n",
      "Step 232, Loss: 0.0439\n",
      "Step 240, Loss: 0.0509\n",
      "Step 248, Loss: 0.0626\n",
      "Step 256, Loss: 0.0613\n",
      "Step 264, Loss: 0.0817\n",
      "Step 272, Loss: 0.0603\n",
      "Step 280, Loss: 0.0496\n",
      "Step 288, Loss: 0.0512\n",
      "Step 296, Loss: 0.0532\n",
      "Step 304, Loss: 0.0514\n",
      "Step 312, Loss: 0.0541\n",
      "Step 320, Loss: 0.0532\n",
      "Step 328, Loss: 0.0448\n",
      "Step 336, Loss: 0.0672\n",
      "Step 344, Loss: 0.0435\n",
      "Step 352, Loss: 0.0379\n",
      "Step 360, Loss: 0.0633\n",
      "Step 368, Loss: 0.0505\n",
      "Step 376, Loss: 0.0434\n",
      "Step 384, Loss: 0.0533\n",
      "Step 392, Loss: 0.0456\n",
      "Step 400, Loss: 0.0370\n",
      "Step 408, Loss: 0.0456\n",
      "Step 416, Loss: 0.0661\n",
      "Step 424, Loss: 0.0399\n",
      "Step 432, Loss: 0.0516\n",
      "Step 440, Loss: 0.0456\n",
      "Step 448, Loss: 0.0436\n",
      "Step 456, Loss: 0.0473\n",
      "Step 464, Loss: 0.0465\n",
      "Step 472, Loss: 0.0575\n",
      "Step 480, Loss: 0.0415\n",
      "Step 488, Loss: 0.0477\n",
      "Step 496, Loss: 0.0454\n",
      "Step 504, Loss: 0.0495\n",
      "Step 512, Loss: 0.0351\n",
      "Step 520, Loss: 0.0440\n",
      "Step 528, Loss: 0.0438\n",
      "Step 536, Loss: 0.0473\n",
      "Step 544, Loss: 0.0587\n",
      "Step 552, Loss: 0.0489\n",
      "Step 560, Loss: 0.0505\n",
      "Step 568, Loss: 0.0399\n",
      "Step 576, Loss: 0.0449\n",
      "Step 584, Loss: 0.0578\n",
      "Step 592, Loss: 0.0569\n",
      "Step 600, Loss: 0.0621\n",
      "Step 608, Loss: 0.0418\n",
      "Step 616, Loss: 0.0542\n",
      "Step 624, Loss: 0.0441\n",
      "Step 632, Loss: 0.0443\n",
      "Step 640, Loss: 0.0472\n",
      "Step 648, Loss: 0.0504\n",
      "Step 656, Loss: 0.0367\n",
      "Step 664, Loss: 0.0513\n",
      "Step 672, Loss: 0.0433\n",
      "Step 680, Loss: 0.0511\n",
      "Step 688, Loss: 0.0413\n",
      "Step 696, Loss: 0.0518\n",
      "Step 704, Loss: 0.0507\n",
      "Step 712, Loss: 0.0584\n",
      "Step 720, Loss: 0.0552\n",
      "Step 728, Loss: 0.0591\n",
      "Step 736, Loss: 0.0545\n",
      "Step 744, Loss: 0.0542\n",
      "Step 752, Loss: 0.0388\n",
      "Step 760, Loss: 0.0560\n",
      "Step 768, Loss: 0.0543\n",
      "Step 776, Loss: 0.0495\n",
      "Step 784, Loss: 0.0365\n",
      "Step 792, Loss: 0.0465\n",
      "Step 800, Loss: 0.0723\n",
      "Step 808, Loss: 0.0321\n",
      "Step 816, Loss: 0.0552\n",
      "Step 824, Loss: 0.0529\n",
      "Step 832, Loss: 0.0398\n",
      "Step 840, Loss: 0.0577\n",
      "Step 848, Loss: 0.0450\n",
      "Step 856, Loss: 0.0392\n",
      "Step 864, Loss: 0.0396\n",
      "Step 872, Loss: 0.0459\n",
      "Step 880, Loss: 0.0623\n",
      "Step 888, Loss: 0.0506\n",
      "Step 896, Loss: 0.0559\n",
      "Step 904, Loss: 0.0547\n",
      "Step 912, Loss: 0.0465\n",
      "Step 920, Loss: 0.0434\n",
      "Step 928, Loss: 0.0511\n",
      "Step 936, Loss: 0.0495\n",
      "Step 944, Loss: 0.0499\n",
      "Step 952, Loss: 0.0458\n",
      "Step 960, Loss: 0.0540\n",
      "Step 968, Loss: 0.0703\n",
      "Step 976, Loss: 0.0469\n",
      "Step 984, Loss: 0.0533\n",
      "Step 992, Loss: 0.0451\n",
      "Step 1000, Loss: 0.0435\n",
      "Step 1008, Loss: 0.0536\n",
      "Step 1016, Loss: 0.0512\n",
      "Step 1024, Loss: 0.0450\n",
      "Step 1032, Loss: 0.0438\n",
      "Step 1040, Loss: 0.0505\n",
      "Step 1048, Loss: 0.0399\n",
      "Step 1056, Loss: 0.0448\n",
      "Step 1064, Loss: 0.0487\n",
      "Step 1072, Loss: 0.0589\n",
      "Step 1080, Loss: 0.0638\n",
      "Step 1088, Loss: 0.0448\n",
      "Step 1096, Loss: 0.0435\n",
      "Step 1104, Loss: 0.0665\n",
      "Step 1112, Loss: 0.0393\n",
      "Step 1120, Loss: 0.0679\n",
      "Step 1128, Loss: 0.0534\n",
      "Step 1136, Loss: 0.0674\n",
      "Step 1144, Loss: 0.0403\n",
      "Step 1152, Loss: 0.0416\n",
      "Step 1160, Loss: 0.0525\n",
      "Step 1168, Loss: 0.0518\n",
      "Step 1176, Loss: 0.0459\n",
      "Step 1184, Loss: 0.0424\n",
      "Step 1192, Loss: 0.0485\n",
      "Step 1200, Loss: 0.0320\n",
      "Step 1208, Loss: 0.0511\n",
      "Step 1216, Loss: 0.0436\n",
      "Step 1224, Loss: 0.0565\n",
      "Step 1232, Loss: 0.0624\n",
      "Step 1240, Loss: 0.0454\n",
      "Step 1248, Loss: 0.0292\n",
      "Step 1256, Loss: 0.0451\n",
      "Step 1264, Loss: 0.0454\n",
      "Step 1272, Loss: 0.0571\n",
      "Step 1280, Loss: 0.0373\n",
      "Step 1288, Loss: 0.0424\n",
      "Step 1296, Loss: 0.0357\n",
      "Step 1304, Loss: 0.0654\n",
      "Step 1312, Loss: 0.0343\n",
      "Step 1320, Loss: 0.0522\n",
      "Step 1328, Loss: 0.0414\n",
      "Step 1336, Loss: 0.0496\n",
      "Step 1344, Loss: 0.0542\n",
      "Step 1352, Loss: 0.0530\n",
      "Step 1360, Loss: 0.0414\n",
      "Step 1368, Loss: 0.0558\n",
      "Step 1376, Loss: 0.0623\n",
      "Step 1384, Loss: 0.0409\n",
      "Step 1392, Loss: 0.0559\n",
      "Step 1400, Loss: 0.0521\n",
      "Step 1408, Loss: 0.0534\n",
      "Step 1416, Loss: 0.0532\n",
      "Step 1424, Loss: 0.0402\n",
      "Step 1432, Loss: 0.0465\n",
      "Step 1440, Loss: 0.0528\n",
      "Step 1448, Loss: 0.0571\n",
      "Step 1456, Loss: 0.0436\n",
      "Step 1464, Loss: 0.0537\n",
      "Step 1472, Loss: 0.0515\n",
      "Step 1480, Loss: 0.0643\n",
      "Step 1488, Loss: 0.0466\n",
      "Step 1496, Loss: 0.0440\n",
      "Step 1504, Loss: 0.0439\n",
      "Step 1512, Loss: 0.0311\n",
      "Step 1520, Loss: 0.0405\n",
      "Step 1528, Loss: 0.0335\n",
      "Step 1536, Loss: 0.0368\n",
      "Step 1544, Loss: 0.0404\n",
      "Step 1552, Loss: 0.0415\n",
      "Step 1560, Loss: 0.0384\n",
      "Step 1568, Loss: 0.0475\n",
      "Step 1576, Loss: 0.0441\n",
      "Step 1584, Loss: 0.0586\n",
      "Step 1592, Loss: 0.0406\n",
      "Step 1600, Loss: 0.0496\n",
      "Step 1608, Loss: 0.0414\n",
      "Step 1616, Loss: 0.0510\n",
      "Step 1624, Loss: 0.0597\n",
      "Step 1632, Loss: 0.0440\n",
      "Step 1640, Loss: 0.0416\n",
      "Step 1648, Loss: 0.0322\n",
      "Step 1656, Loss: 0.0384\n",
      "Step 1664, Loss: 0.0466\n",
      "Step 1672, Loss: 0.0464\n",
      "Step 1680, Loss: 0.0527\n",
      "Step 1688, Loss: 0.0500\n",
      "Step 1696, Loss: 0.0425\n",
      "Step 1704, Loss: 0.0330\n",
      "Step 1712, Loss: 0.0558\n",
      "Step 1720, Loss: 0.0579\n",
      "Step 1728, Loss: 0.0586\n",
      "Step 1736, Loss: 0.0521\n",
      "Step 1744, Loss: 0.0450\n",
      "Step 1752, Loss: 0.0637\n",
      "Step 1760, Loss: 0.0361\n",
      "Step 1768, Loss: 0.0376\n",
      "Step 1776, Loss: 0.0408\n",
      "Step 1784, Loss: 0.0447\n",
      "Step 1792, Loss: 0.0665\n",
      "Step 1800, Loss: 0.0344\n",
      "Step 1808, Loss: 0.0432\n",
      "Step 1816, Loss: 0.0462\n",
      "Step 1824, Loss: 0.0398\n",
      "Step 1832, Loss: 0.0429\n",
      "Step 1840, Loss: 0.0572\n",
      "Step 1848, Loss: 0.0501\n",
      "Step 1856, Loss: 0.0442\n",
      "Step 1864, Loss: 0.0648\n",
      "Step 1872, Loss: 0.0497\n",
      "Step 1880, Loss: 0.0568\n",
      "Step 1888, Loss: 0.0664\n",
      "Step 1896, Loss: 0.0489\n",
      "Step 1904, Loss: 0.0470\n",
      "Step 1912, Loss: 0.0483\n",
      "Step 1920, Loss: 0.0371\n",
      "Step 1928, Loss: 0.0477\n",
      "Step 1936, Loss: 0.0453\n",
      "Step 1944, Loss: 0.0537\n",
      "Step 1952, Loss: 0.0351\n",
      "Step 1960, Loss: 0.0432\n",
      "Step 1968, Loss: 0.0523\n",
      "Step 1976, Loss: 0.0487\n",
      "Step 1984, Loss: 0.0517\n",
      "Step 1992, Loss: 0.0448\n",
      "Step 2000, Loss: 0.0518\n",
      "Step 2008, Loss: 0.0387\n",
      "Step 2016, Loss: 0.0514\n",
      "Step 2024, Loss: 0.0323\n",
      "Step 2032, Loss: 0.0433\n",
      "Step 2040, Loss: 0.0640\n",
      "Step 2048, Loss: 0.0437\n",
      "Step 2056, Loss: 0.0336\n",
      "Step 2064, Loss: 0.0631\n",
      "Step 2072, Loss: 0.0282\n",
      "Step 2080, Loss: 0.0431\n",
      "Step 2088, Loss: 0.0470\n",
      "Step 2096, Loss: 0.0509\n",
      "Step 2104, Loss: 0.0477\n",
      "Step 2112, Loss: 0.0503\n",
      "Step 2120, Loss: 0.0607\n",
      "Step 2128, Loss: 0.0422\n",
      "Step 2136, Loss: 0.0404\n",
      "Step 2144, Loss: 0.0564\n",
      "Step 2152, Loss: 0.0425\n",
      "Step 2160, Loss: 0.0420\n",
      "Step 2168, Loss: 0.0407\n",
      "Step 2176, Loss: 0.0474\n",
      "Step 2184, Loss: 0.0493\n",
      "Step 2192, Loss: 0.0470\n",
      "Step 2200, Loss: 0.0526\n",
      "Step 2208, Loss: 0.0376\n",
      "Step 2216, Loss: 0.0602\n",
      "Step 2224, Loss: 0.0529\n",
      "Step 2232, Loss: 0.0417\n",
      "Step 2240, Loss: 0.0479\n",
      "Step 2248, Loss: 0.0346\n",
      "Step 2256, Loss: 0.0400\n",
      "Step 2264, Loss: 0.0429\n",
      "Step 2272, Loss: 0.0415\n",
      "Step 2280, Loss: 0.0334\n",
      "Step 2288, Loss: 0.0689\n",
      "Step 2296, Loss: 0.0384\n",
      "Step 2304, Loss: 0.0280\n",
      "Step 2312, Loss: 0.0433\n",
      "Step 2320, Loss: 0.0503\n",
      "Step 2328, Loss: 0.0529\n",
      "Step 2336, Loss: 0.0557\n",
      "Step 2344, Loss: 0.0425\n",
      "Step 2352, Loss: 0.0392\n",
      "Step 2360, Loss: 0.0388\n",
      "Step 2368, Loss: 0.0361\n",
      "Step 2376, Loss: 0.0434\n",
      "Step 2384, Loss: 0.0444\n",
      "Step 2392, Loss: 0.0319\n",
      "Step 2400, Loss: 0.0512\n",
      "Step 2408, Loss: 0.0517\n",
      "Step 2416, Loss: 0.0361\n",
      "Step 2424, Loss: 0.0468\n",
      "Step 2432, Loss: 0.0551\n",
      "Step 2440, Loss: 0.0475\n",
      "Step 2448, Loss: 0.0587\n",
      "Step 2456, Loss: 0.0351\n",
      "Step 2464, Loss: 0.0466\n",
      "Step 2472, Loss: 0.0439\n",
      "Step 2480, Loss: 0.0407\n",
      "Step 2488, Loss: 0.0439\n",
      "Step 2496, Loss: 0.0461\n",
      "Step 2504, Loss: 0.0392\n",
      "Step 2512, Loss: 0.0441\n",
      "Step 2520, Loss: 0.0446\n",
      "Step 2528, Loss: 0.0435\n",
      "Step 2536, Loss: 0.0587\n",
      "Step 2544, Loss: 0.0569\n",
      "Step 2552, Loss: 0.0460\n",
      "Step 2560, Loss: 0.0529\n",
      "Step 2568, Loss: 0.0460\n",
      "Step 2576, Loss: 0.0536\n",
      "Step 2584, Loss: 0.0330\n",
      "Step 2592, Loss: 0.0435\n",
      "Step 2600, Loss: 0.0470\n",
      "Step 2608, Loss: 0.0339\n",
      "Step 2616, Loss: 0.0446\n",
      "Step 2624, Loss: 0.0458\n",
      "Step 2632, Loss: 0.0380\n",
      "Step 2640, Loss: 0.0545\n",
      "Step 2648, Loss: 0.0421\n",
      "Step 2656, Loss: 0.0414\n",
      "Step 2664, Loss: 0.0444\n",
      "Step 2672, Loss: 0.0388\n",
      "Step 2680, Loss: 0.0500\n",
      "Step 2688, Loss: 0.0380\n",
      "Step 2696, Loss: 0.0360\n",
      "Step 2704, Loss: 0.0631\n",
      "Step 2712, Loss: 0.0444\n",
      "Step 2720, Loss: 0.0346\n",
      "Step 2728, Loss: 0.0383\n",
      "Step 2736, Loss: 0.0438\n",
      "Step 2744, Loss: 0.0496\n",
      "Step 2752, Loss: 0.0499\n",
      "Step 2760, Loss: 0.0565\n",
      "Step 2768, Loss: 0.0431\n",
      "Step 2776, Loss: 0.0464\n",
      "Step 2784, Loss: 0.0363\n",
      "Step 2792, Loss: 0.0475\n",
      "Step 2800, Loss: 0.0346\n",
      "Step 2808, Loss: 0.0486\n",
      "Step 2816, Loss: 0.0562\n",
      "Step 2824, Loss: 0.0496\n",
      "Step 2832, Loss: 0.0499\n",
      "Step 2840, Loss: 0.0514\n",
      "Step 2848, Loss: 0.0433\n",
      "Step 2856, Loss: 0.0430\n",
      "Step 2864, Loss: 0.0365\n",
      "Step 2872, Loss: 0.0439\n",
      "Step 2880, Loss: 0.0582\n",
      "Step 2888, Loss: 0.0432\n",
      "Step 2896, Loss: 0.0488\n",
      "Step 2904, Loss: 0.0365\n",
      "Step 2912, Loss: 0.0473\n",
      "Step 2920, Loss: 0.0456\n",
      "Step 2928, Loss: 0.0392\n",
      "Step 2936, Loss: 0.0528\n",
      "Step 2944, Loss: 0.0522\n",
      "Step 2952, Loss: 0.0458\n",
      "Step 2960, Loss: 0.0530\n",
      "Step 2968, Loss: 0.0426\n",
      "Step 2976, Loss: 0.0525\n",
      "Step 2984, Loss: 0.0570\n",
      "Step 2992, Loss: 0.0376\n",
      "Step 3000, Loss: 0.0616\n",
      "Step 3008, Loss: 0.0479\n",
      "Step 3016, Loss: 0.0435\n",
      "Step 3024, Loss: 0.0450\n",
      "Step 3032, Loss: 0.0432\n",
      "Step 3040, Loss: 0.0525\n",
      "Step 3048, Loss: 0.0359\n",
      "Step 3056, Loss: 0.0438\n",
      "Step 3064, Loss: 0.0257\n",
      "Step 3072, Loss: 0.0511\n",
      "Step 3080, Loss: 0.0506\n",
      "Step 3088, Loss: 0.0492\n",
      "Step 3096, Loss: 0.0440\n",
      "Step 3104, Loss: 0.0500\n",
      "Step 3112, Loss: 0.0438\n",
      "Step 3120, Loss: 0.0349\n",
      "Step 3128, Loss: 0.0326\n",
      "Step 3136, Loss: 0.0347\n",
      "Step 3144, Loss: 0.0391\n",
      "Step 3152, Loss: 0.0321\n",
      "Step 3160, Loss: 0.0625\n",
      "Step 3168, Loss: 0.0455\n",
      "Step 3176, Loss: 0.0554\n",
      "Step 3184, Loss: 0.0423\n",
      "Step 3192, Loss: 0.0465\n",
      "Step 3200, Loss: 0.0385\n",
      "Step 3208, Loss: 0.0462\n",
      "Step 3216, Loss: 0.0534\n",
      "Step 3224, Loss: 0.0484\n",
      "Step 3232, Loss: 0.0359\n",
      "Step 3240, Loss: 0.0463\n",
      "Step 3248, Loss: 0.0527\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer.zero_grad()\n",
    "for e in range(2):\n",
    "    for i, (batch,) in tqdm(enumerate(loader_train), total=len(loader_train), desc=\"Training\"):\n",
    "        batch = batch.to(device)\n",
    "        loss = train_step(transformer, batch, optimizer, criterion, i)\n",
    "        if i%8 == 0:\n",
    "            print(f\"Step {i}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9008b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pkl.load(open(\"table_CNN_24k_whitespace\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "962b0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor([[24070]], dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f599efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7f4a4dbf2744ada2f4db008903d33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "# create a read-only text area\n",
    "ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    layout=widgets.Layout(width='80ch', height='20em'),\n",
    "    disabled=True\n",
    ")\n",
    "display(ta)\n",
    "\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "\n",
    "torch.random.seed(42)\n",
    "\n",
    "for i in range(512):\n",
    "    logits = transformer(tokens)[0, -1:]\n",
    "    topk_vals, _    = torch.topk(logits, k=k)\n",
    "    #print(topk_vals)\n",
    "    kth_value       = topk_vals[:,-1]\n",
    "\n",
    "    logits = torch.where(logits >= kth_value, logits, -torch.inf)\n",
    "    dist = Categorical(logits=logits/T)\n",
    "    idx = dist.sample()\n",
    "    tokens = torch.cat([tokens, idx.reshape(1,1)], dim=1)\n",
    "    #print(tokens.shape)\n",
    "    text = \"\"\n",
    "    for i in tokens[0]:\n",
    "        s = table[int(i)]\n",
    "        text = text + s\n",
    "\n",
    "    ta.value = wrapper.fill(text)  # this updates in-place\n",
    "\n",
    "    if idx[0] == 24071:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91a8bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(transformer, open(\"model.model\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
