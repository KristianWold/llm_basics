{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "#from src.tokenizer import TokenizerBPE, fuse_tokenized_corpus, chunk_corpus\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e38fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = pkl.load(open('corpus/corpus_CNN_24k_whitespace_train_numpy', 'rb'))\n",
    "corpus_train = torch.tensor(corpus_train, dtype=torch.int64)\n",
    "corpus_train = TensorDataset(corpus_train)\n",
    "\n",
    "\n",
    "corpus_test = pkl.load(open('corpus/corpus_CNN_24k_whitespace_test_numpy', 'rb'))\n",
    "corpus_test = torch.tensor(corpus_test, dtype=torch.int64)\n",
    "corpus_test = TensorDataset(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=False     # whether to drop the tail batch if smaller than batch_size\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test,\n",
    "    batch_size=8,\n",
    "    shuffle=True,      \n",
    "    drop_last=False\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64*10\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 10\n",
    "tf_blocks = 10\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=24072,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    pad_token_id=None,\n",
    "    start_token_id=24070\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a56ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_loss(model, batch, criterion):\n",
    "    model.train()\n",
    "    #batch is a tensor of shape [batch, seq]\n",
    "    src, tgt = batch[:, :-1].to(device), batch[:, 1:].to(device)\n",
    "    logits = model(src)\n",
    "    return criterion(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1))\n",
    "\n",
    "# 2) Do the zero_grad/backward/step around it\n",
    "def train_step(model, batch, optimizer, criterion):\n",
    "    # zero the grads in Python (not inside the compiled graph)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # call compiled forward+loss\n",
    "    loss = forward_and_loss(model, batch, criterion)\n",
    "\n",
    "    # backward & step in Python\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29d4661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc4013b501a46699d167a98408761a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/51794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 637.0493\n",
      "Step 1, Loss: 602.8871\n",
      "Step 2, Loss: 571.5277\n",
      "Step 3, Loss: 542.0153\n",
      "Step 4, Loss: 520.7668\n",
      "Step 5, Loss: 508.2817\n",
      "Step 6, Loss: 490.9936\n",
      "Step 7, Loss: 496.5493\n",
      "Step 8, Loss: 483.6722\n",
      "Step 9, Loss: 470.3539\n",
      "Step 10, Loss: 450.7548\n",
      "Step 11, Loss: 450.7498\n",
      "Step 12, Loss: 459.8244\n",
      "Step 13, Loss: 479.2814\n",
      "Step 14, Loss: 477.1370\n",
      "Step 15, Loss: 472.9950\n",
      "Step 16, Loss: 463.4001\n",
      "Step 17, Loss: 453.2903\n",
      "Step 18, Loss: 445.7467\n",
      "Step 19, Loss: 408.5549\n",
      "Step 20, Loss: 414.5872\n",
      "Step 21, Loss: 412.7249\n",
      "Step 22, Loss: 403.9278\n",
      "Step 23, Loss: 395.9747\n",
      "Step 24, Loss: 416.9826\n",
      "Step 25, Loss: 400.8383\n",
      "Step 26, Loss: 392.5642\n",
      "Step 27, Loss: 384.4294\n",
      "Step 28, Loss: 374.0632\n",
      "Step 29, Loss: 380.4855\n",
      "Step 30, Loss: 374.5624\n",
      "Step 31, Loss: 365.6567\n",
      "Step 32, Loss: 365.1854\n",
      "Step 33, Loss: 359.2446\n",
      "Step 34, Loss: 356.8309\n",
      "Step 35, Loss: 351.9932\n",
      "Step 36, Loss: 349.5576\n",
      "Step 37, Loss: 341.1428\n",
      "Step 38, Loss: 338.7044\n",
      "Step 39, Loss: 339.5231\n",
      "Step 40, Loss: 352.5169\n",
      "Step 41, Loss: 338.5050\n",
      "Step 42, Loss: 321.2575\n",
      "Step 43, Loss: 312.6315\n",
      "Step 44, Loss: 319.7083\n",
      "Step 45, Loss: 320.3056\n",
      "Step 46, Loss: 318.2722\n",
      "Step 47, Loss: 306.5938\n",
      "Step 48, Loss: 310.6296\n",
      "Step 49, Loss: 315.5068\n",
      "Step 50, Loss: 313.6149\n",
      "Step 51, Loss: 300.0656\n",
      "Step 52, Loss: 300.3838\n",
      "Step 53, Loss: 304.0886\n",
      "Step 54, Loss: 292.3032\n",
      "Step 55, Loss: 295.5017\n",
      "Step 56, Loss: 282.6968\n",
      "Step 57, Loss: 287.5998\n",
      "Step 58, Loss: 286.4982\n",
      "Step 59, Loss: 288.3436\n",
      "Step 60, Loss: 277.5789\n",
      "Step 61, Loss: 283.8255\n",
      "Step 62, Loss: 272.8747\n",
      "Step 63, Loss: 266.3561\n",
      "Step 64, Loss: 275.7816\n",
      "Step 65, Loss: 277.4872\n",
      "Step 66, Loss: 260.2603\n",
      "Step 67, Loss: 260.5382\n",
      "Step 68, Loss: 255.1505\n",
      "Step 69, Loss: 268.7612\n",
      "Step 70, Loss: 255.1575\n",
      "Step 71, Loss: 249.4004\n",
      "Step 72, Loss: 256.8577\n",
      "Step 73, Loss: 259.6551\n",
      "Step 74, Loss: 239.8998\n",
      "Step 75, Loss: 257.7468\n",
      "Step 76, Loss: 241.5465\n",
      "Step 77, Loss: 247.0653\n",
      "Step 78, Loss: 243.1355\n",
      "Step 79, Loss: 235.9845\n",
      "Step 80, Loss: 236.5187\n",
      "Step 81, Loss: 240.7414\n",
      "Step 82, Loss: 241.1626\n",
      "Step 83, Loss: 235.2142\n",
      "Step 84, Loss: 231.2433\n",
      "Step 85, Loss: 234.8628\n",
      "Step 86, Loss: 225.1126\n",
      "Step 87, Loss: 229.3782\n",
      "Step 88, Loss: 234.6831\n",
      "Step 89, Loss: 224.5317\n",
      "Step 90, Loss: 219.9029\n",
      "Step 91, Loss: 217.9485\n",
      "Step 92, Loss: 217.6957\n",
      "Step 93, Loss: 233.4755\n",
      "Step 94, Loss: 220.5002\n",
      "Step 95, Loss: 226.1802\n",
      "Step 96, Loss: 222.4039\n",
      "Step 97, Loss: 222.0668\n",
      "Step 98, Loss: 211.9157\n",
      "Step 99, Loss: 212.2198\n",
      "Step 100, Loss: 210.0768\n",
      "Step 101, Loss: 213.6343\n",
      "Step 102, Loss: 215.9735\n",
      "Step 103, Loss: 213.9238\n",
      "Step 104, Loss: 206.8811\n",
      "Step 105, Loss: 213.0177\n",
      "Step 106, Loss: 208.2601\n",
      "Step 107, Loss: 205.8216\n",
      "Step 108, Loss: 213.0112\n",
      "Step 109, Loss: 203.3418\n",
      "Step 110, Loss: 202.7432\n",
      "Step 111, Loss: 197.5682\n",
      "Step 112, Loss: 205.3441\n",
      "Step 113, Loss: 197.1010\n",
      "Step 114, Loss: 189.6478\n",
      "Step 115, Loss: 195.0526\n",
      "Step 116, Loss: 194.0813\n",
      "Step 117, Loss: 195.6812\n",
      "Step 118, Loss: 192.9190\n",
      "Step 119, Loss: 196.5931\n",
      "Step 120, Loss: 193.2028\n",
      "Step 121, Loss: 193.2697\n",
      "Step 122, Loss: 193.1457\n",
      "Step 123, Loss: 197.0075\n",
      "Step 124, Loss: 184.4640\n",
      "Step 125, Loss: 186.6653\n",
      "Step 126, Loss: 187.5139\n",
      "Step 127, Loss: 184.6687\n",
      "Step 128, Loss: 186.6724\n",
      "Step 129, Loss: 186.7695\n",
      "Step 130, Loss: 200.8991\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch,) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader_train), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader_train), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_step(transformer, batch, optimizer, criterion)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, batch, optimizer, criterion)\u001b[0m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# call compiled forward+loss\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m forward_and_loss(model, batch, criterion)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# backward & step in Python\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m, in \u001b[0;36mforward_and_loss\u001b[1;34m(model, batch, criterion)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#batch is a tensor of shape [batch, seq]\u001b[39;00m\n\u001b[0;32m      4\u001b[0m src, tgt \u001b[38;5;241m=\u001b[39m batch[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(src)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m criterion(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\env_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\env_pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\krist\\Documents\\slarn_chatbot\\src\\transformer.py:186\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    183\u001b[0m x, tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(tokens)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_blocks:\n\u001b[1;32m--> 186\u001b[0m     x \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward([x, tokens])\n\u001b[0;32m    188\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munembed(x)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\krist\\Documents\\slarn_chatbot\\src\\transformer.py:95\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     94\u001b[0m     x_embeds, tokens \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 95\u001b[0m     x_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x_embeds, tokens)\n\u001b[0;32m     96\u001b[0m     x_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffnn(x_embeds)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_embeds\n",
      "File \u001b[1;32mc:\\Users\\krist\\Documents\\slarn_chatbot\\src\\transformer.py:112\u001b[0m, in \u001b[0;36mTransformerBlock.attention\u001b[1;34m(self, x_embeds, tokens)\u001b[0m\n\u001b[0;32m    109\u001b[0m x_q \u001b[38;5;241m=\u001b[39m x_kqv[:, :, \u001b[38;5;241m1\u001b[39m, :, :] \u001b[38;5;66;03m# [batch, heads, seq, head_dim]\u001b[39;00m\n\u001b[0;32m    110\u001b[0m x_v \u001b[38;5;241m=\u001b[39m x_kqv[:, :, \u001b[38;5;241m2\u001b[39m, :, :] \u001b[38;5;66;03m# [batch, heads, seq, head_dim]\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m mask_causal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, seq, seq), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(x_embeds\u001b[38;5;241m.\u001b[39mget_device())\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     mask \u001b[38;5;241m=\u001b[39m block_diag_mask(tokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_token_id)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i, (batch,) in tqdm(enumerate(loader_train), total=len(loader_train), desc=\"Training\"):\n",
    "    batch = batch.to(device)\n",
    "    loss = train_step(transformer, batch, optimizer, criterion)\n",
    "    \n",
    "    print(f\"Step {i}, Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
