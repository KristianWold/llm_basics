{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from src.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from src.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"cnn_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens4.pkl\"))\n",
    "corpus_train_a = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_train_tokens4.pkl\"))\n",
    "corpus_train_b = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "corpus_train = torch.cat((corpus_train_a, corpus_train_b), dim=0)\n",
    "\n",
    "corpus_test_a = torch.tensor(loader(\"corpus/cnn_dailymail_article_test_tokens.pkl\"))\n",
    "corpus_test_b = torch.tensor(loader(\"corpus/cnn_dailymail_HLlast_test_tokens.pkl\"))\n",
    "corpus_test = torch.cat((corpus_test_a, corpus_test_b), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024, offset=None):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    # overlapping batches augmentation\n",
    "    if offset is not None:\n",
    "        corpus_offset = corpus_truncated[offset : offset - batch_length]\n",
    "        corpus_offset = corpus_offset.view(-1, batch_length)  # reshape into batches\n",
    "        corpus_batched = torch.cat((corpus_batched, corpus_offset), dim=0)  # concatenate the offset batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024, offset=None)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024, offset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=tokenizer.token_to_idx[\"<s>\"],\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )\n",
    "\n",
    "loss_train_list = []\n",
    "loss_eval_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf7c59",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5961af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = load_checkpoint(\"checkpoint_transformer.pth\", model, optimizer, scheduler)\n",
    "loss_train_list = loader(\"loss_train.pkl\")\n",
    "loss_eval_list = loader(\"loss_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ff108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c4d9eed9e1469da49dc5698235ffdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 3.2926, Loss_eval: 3.5649, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 3.4125, Loss_eval: 3.5647, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 3.4629, Loss_eval: 3.5654, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 3.3948, Loss_eval: 3.5595, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 3.2385, Loss_eval: 3.5679, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 3.2179, Loss_eval: 3.5674, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 3.1153, Loss_eval: 3.5531, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 3.4583, Loss_eval: 3.5421, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 3.4093, Loss_eval: 3.5727, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 3.4973, Loss_eval: 3.5581, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 3.4095, Loss_eval: 3.5493, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 3.4714, Loss_eval: 3.5421, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 3.4075, Loss_eval: 3.5370, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 3.3277, Loss_eval: 3.5397, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.2831, Loss_eval: 3.5367, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 3.7060, Loss_eval: 3.5314, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 3.3770, Loss_eval: 3.5672, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 3.4102, Loss_eval: 3.5398, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 3.4358, Loss_eval: 3.5613, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 3.3991, Loss_eval: 3.5575, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 3.4602, Loss_eval: 3.5440, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 3.4107, Loss_eval: 3.5329, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.3788, Loss_eval: 3.5496, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.2892, Loss_eval: 3.5429, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 3.4811, Loss_eval: 3.5485, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 3.3950, Loss_eval: 3.5559, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.3429, Loss_eval: 3.5368, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 3.2115, Loss_eval: 3.5419, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 3.6756, Loss_eval: 3.5285, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 3.6284, Loss_eval: 3.5539, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 3.4609, Loss_eval: 3.5422, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.3812, Loss_eval: 3.5286, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.4145, Loss_eval: 3.5305, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 3.1779, Loss_eval: 3.5436, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 3.2178, Loss_eval: 3.5609, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.3535, Loss_eval: 3.5627, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 3.3271, Loss_eval: 3.5540, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 3.4578, Loss_eval: 3.5478, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 3.6621, Loss_eval: 3.5343, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 3.5332, Loss_eval: 3.5427, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 3.2219, Loss_eval: 3.5402, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 3.1418, Loss_eval: 3.5293, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 3.3717, Loss_eval: 3.5476, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.3447, Loss_eval: 3.5575, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 3.5613, Loss_eval: 3.5366, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 3.1631, Loss_eval: 3.5385, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.3812, Loss_eval: 3.5418, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 3.4278, Loss_eval: 3.5479, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 3.4595, Loss_eval: 3.5485, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 3.5293, Loss_eval: 3.5515, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 3.2599, Loss_eval: 3.5351, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.3755, Loss_eval: 3.5365, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 3.4826, Loss_eval: 3.5465, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 3.1511, Loss_eval: 3.5093, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 3.6219, Loss_eval: 3.5212, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.3064, Loss_eval: 3.5368, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 3.4095, Loss_eval: 3.5407, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 3.3250, Loss_eval: 3.5324, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.4330, Loss_eval: 3.5304, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 3.5421, Loss_eval: 3.5459, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.2829, Loss_eval: 3.5518, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 3.6018, Loss_eval: 3.5504, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.4099, Loss_eval: 3.5221, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 3.5666, Loss_eval: 3.5230, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.2438, Loss_eval: 3.5468, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 3.1457, Loss_eval: 3.5339, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.4691, Loss_eval: 3.5245, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.2597, Loss_eval: 3.5469, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.3954, Loss_eval: 3.5397, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.4347, Loss_eval: 3.5547, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 3.5717, Loss_eval: 3.5250, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 3.0995, Loss_eval: 3.5241, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.3675, Loss_eval: 3.5206, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.2845, Loss_eval: 3.5513, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 3.4933, Loss_eval: 3.5178, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.2693, Loss_eval: 3.5422, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 3.7773, Loss_eval: 3.5337, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.2978, Loss_eval: 3.5376, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 3.6884, Loss_eval: 3.5217, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 3.5466, Loss_eval: 3.5260, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.3505, Loss_eval: 3.5218, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 3.2061, Loss_eval: 3.5277, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 3.2125, Loss_eval: 3.5182, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.5602, Loss_eval: 3.5312, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.4123, Loss_eval: 3.5256, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.3197, Loss_eval: 3.5211, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 3.4874, Loss_eval: 3.5401, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.2724, Loss_eval: 3.5293, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.4310, Loss_eval: 3.5299, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.2979, Loss_eval: 3.5324, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 3.2581, Loss_eval: 3.5540, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 3.3391, Loss_eval: 3.5321, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.4540, Loss_eval: 3.5369, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 3.5149, Loss_eval: 3.5413, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 2.9969, Loss_eval: 3.5158, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.3253, Loss_eval: 3.5156, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 2.8759, Loss_eval: 3.4955, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 3.6165, Loss_eval: 3.5351, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 3.3061, Loss_eval: 3.5215, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.3098, Loss_eval: 3.5296, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 3.5098, Loss_eval: 3.5122, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.3390, Loss_eval: 3.5097, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 3.4471, Loss_eval: 3.5358, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.2754, Loss_eval: 3.5444, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 3.5738, Loss_eval: 3.5329, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.5159, Loss_eval: 3.5217, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.3782, Loss_eval: 3.5258, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 3.2145, Loss_eval: 3.5238, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 3.4759, Loss_eval: 3.5252, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 3.5441, Loss_eval: 3.5302, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.4542, Loss_eval: 3.5333, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 3.0523, Loss_eval: 3.4948, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 3.8445, Loss_eval: 3.4939, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.2709, Loss_eval: 3.5270, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 3.4587, Loss_eval: 3.5147, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.3084, Loss_eval: 3.5005, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 3.3645, Loss_eval: 3.5157, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 3.4968, Loss_eval: 3.5211, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 3.1502, Loss_eval: 3.5331, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 3.3271, Loss_eval: 3.5112, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 3.2962, Loss_eval: 3.5381, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 3.0465, Loss_eval: 3.5212, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 3.1881, Loss_eval: 3.5246, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.3324, Loss_eval: 3.5323, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 3.2258, Loss_eval: 3.5176, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.4468, Loss_eval: 3.5217, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 3.5490, Loss_eval: 3.5313, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 3.1887, Loss_eval: 3.5211, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 3.2157, Loss_eval: 3.5205, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 3.0922, Loss_eval: 3.5017, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 3.0381, Loss_eval: 3.5355, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 3.2507, Loss_eval: 3.5171, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 3.0520, Loss_eval: 3.4974, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 3.6534, Loss_eval: 3.5175, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 3.0356, Loss_eval: 3.5001, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.3597, Loss_eval: 3.5289, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 3.2353, Loss_eval: 3.5327, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.2955, Loss_eval: 3.5061, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 3.1621, Loss_eval: 3.5213, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 3.1651, Loss_eval: 3.5056, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.4026, Loss_eval: 3.5011, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.4233, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 3.0217, Loss_eval: 3.5072, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 3.2465, Loss_eval: 3.5150, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.2671, Loss_eval: 3.5020, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.5111, Loss_eval: 3.4809, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.2990, Loss_eval: 3.4890, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.3870, Loss_eval: 3.5177, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.4021, Loss_eval: 3.5050, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 3.1403, Loss_eval: 3.5320, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 3.1608, Loss_eval: 3.5115, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 2.9558, Loss_eval: 3.5189, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 3.1834, Loss_eval: 3.5098, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 80\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss_train = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step).item()\n",
    "        if (step+1) % 500 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_eval = np.mean([forward_and_loss(model, next(iter_test).to(device), criterion).item() for _ in range(accum_steps)])\n",
    "                print(f\"Step {step+1}, Loss: {loss_train:<.4f}, Loss_eval: {loss_eval:<.4f}, Learning Rate: {lr:4e}\")\n",
    "\n",
    "            loss_train_list.append(loss_train)\n",
    "            loss_eval_list.append(loss_eval)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler, \n",
    "                            filename=\"checkpoint_transformer.pth\")\n",
    "            saver(\"loss_train.pkl\", loss_train_list)\n",
    "            saver(\"loss_eval.pkl\", loss_eval_list)\n",
    "            \n",
    "    save_checkpoint(model, \n",
    "                    optimizer, \n",
    "                    scheduler, \n",
    "                    filename=\"checkpoint_transformer.pth\")\n",
    "    saver(\"loss_train.pkl\", loss_train_list)\n",
    "    saver(\"loss_eval.pkl\", loss_eval_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c0f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
