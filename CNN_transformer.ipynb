{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n"
     ]
    }
   ],
   "source": [
    "import torch as torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer\n",
    "from src.optimization import train_step, forward_and_loss, group_decay_parameters, save_checkpoint, load_checkpoint\n",
    "from src.utils import saver, loader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06f6e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6c5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = loader(\"cnn_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a233301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train1 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens1.pkl\"))\n",
    "corpus_train2 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens2.pkl\"))\n",
    "corpus_train3 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens3.pkl\"))\n",
    "corpus_train4 = torch.tensor(loader(\"corpus/cnn_dailymail_article_train_tokens4.pkl\"))\n",
    "corpus_train = torch.cat((corpus_train1, corpus_train2, corpus_train3, corpus_train4), dim=0)\n",
    "\n",
    "corpus_test = torch.tensor(loader(\"corpus/cnn_dailymail_article_test_tokens.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(corpus, batch_length=1024, offset=None):\n",
    "    \"\"\"\n",
    "    Splits the corpus into batches of size batch_size.\n",
    "    \"\"\"\n",
    "    length = len(corpus)\n",
    "    batches = length // batch_length\n",
    "    corpus_truncated = corpus[:batches * batch_length]  # trim to a multiple of batch_length\n",
    "    corpus_batched = corpus_truncated.view(-1, batch_length)  # reshape into batches\n",
    "\n",
    "    # overlapping batches augmentation\n",
    "    if offset is not None:\n",
    "        corpus_offset = corpus_truncated[offset : offset - batch_length]\n",
    "        corpus_offset = corpus_offset.view(-1, batch_length)  # reshape into batches\n",
    "        corpus_batched = torch.cat((corpus_batched, corpus_offset), dim=0)  # concatenate the offset batches\n",
    "\n",
    "    return corpus_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fac7bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_batched = batch_data(corpus_train, batch_length=1024, offset=512)\n",
    "corpus_test_batched = batch_data(corpus_test, batch_length=1024, offset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c15e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader(\n",
    "    corpus_train_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,       # shuffle every epoch\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    corpus_test_batched,\n",
    "    batch_size=3,\n",
    "    shuffle=True,      # no need to shuffle test data\n",
    "    drop_last=True      # drop the last incomplete batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fec23",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f2ab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "embed_dim = 64*18\n",
    "ff_dim = 4*embed_dim\n",
    "heads = 18\n",
    "tf_blocks = 18\n",
    "\n",
    "model = Transformer(\n",
    "    embed_dim=embed_dim,\n",
    "    ff_dim=ff_dim,\n",
    "    heads=heads,\n",
    "    tf_blocks=tf_blocks,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=1024,\n",
    "    dropout=0.1,\n",
    "    start_token_id=tokenizer.token_to_idx[\"<s>\"],\n",
    "    use_weight_tying=True\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = group_decay_parameters(\n",
    "    model,\n",
    "    weight_decay=0.1,\n",
    "    no_decay=[\"bias\", \"LayerNorm.weight\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5ead6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=5e-5)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "\n",
    "loss_train = []\n",
    "\n",
    "num_epochs      = 1\n",
    "steps_per_epoch = len(loader_train)\n",
    "warmup_steps    = 1000\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82229018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler = load_checkpoint(\"checkpoint_transformer.pth\", model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c9d73b",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c29d4661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e935526140f042f58267d37144faefe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/175315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 3.4477, Loss_eval: 3.5698, Learning Rate: 5.000000e-05\n",
      "Step 200, Loss: 3.0590, Loss_eval: 3.5081, Learning Rate: 5.000000e-05\n",
      "Step 300, Loss: 3.4448, Loss_eval: 3.5213, Learning Rate: 5.000000e-05\n",
      "Step 400, Loss: 3.2959, Loss_eval: 3.5556, Learning Rate: 5.000000e-05\n",
      "Step 500, Loss: 3.4306, Loss_eval: 3.5298, Learning Rate: 5.000000e-05\n",
      "Step 600, Loss: 3.0973, Loss_eval: 3.5499, Learning Rate: 5.000000e-05\n",
      "Step 700, Loss: 3.1289, Loss_eval: 3.5220, Learning Rate: 5.000000e-05\n",
      "Step 800, Loss: 3.3314, Loss_eval: 3.5511, Learning Rate: 5.000000e-05\n",
      "Step 900, Loss: 3.3305, Loss_eval: 3.5418, Learning Rate: 5.000000e-05\n",
      "Step 1000, Loss: 3.1556, Loss_eval: 3.5685, Learning Rate: 5.000000e-05\n",
      "Step 1100, Loss: 3.2533, Loss_eval: 3.5340, Learning Rate: 5.000000e-05\n",
      "Step 1200, Loss: 3.3586, Loss_eval: 3.5337, Learning Rate: 5.000000e-05\n",
      "Step 1300, Loss: 3.5454, Loss_eval: 3.5311, Learning Rate: 5.000000e-05\n",
      "Step 1400, Loss: 3.2435, Loss_eval: 3.5580, Learning Rate: 5.000000e-05\n",
      "Step 1500, Loss: 3.1625, Loss_eval: 3.5321, Learning Rate: 5.000000e-05\n",
      "Step 1600, Loss: 3.3015, Loss_eval: 3.5595, Learning Rate: 5.000000e-05\n",
      "Step 1700, Loss: 3.2826, Loss_eval: 3.5424, Learning Rate: 5.000000e-05\n",
      "Step 1800, Loss: 3.1865, Loss_eval: 3.4971, Learning Rate: 5.000000e-05\n",
      "Step 1900, Loss: 3.1757, Loss_eval: 3.5358, Learning Rate: 5.000000e-05\n",
      "Step 2000, Loss: 3.3985, Loss_eval: 3.5524, Learning Rate: 5.000000e-05\n",
      "Step 2100, Loss: 3.2908, Loss_eval: 3.5165, Learning Rate: 5.000000e-05\n",
      "Step 2200, Loss: 3.3562, Loss_eval: 3.5588, Learning Rate: 5.000000e-05\n",
      "Step 2300, Loss: 3.4255, Loss_eval: 3.5273, Learning Rate: 5.000000e-05\n",
      "Step 2400, Loss: 3.1028, Loss_eval: 3.5390, Learning Rate: 5.000000e-05\n",
      "Step 2500, Loss: 3.4650, Loss_eval: 3.5333, Learning Rate: 5.000000e-05\n",
      "Step 2600, Loss: 3.4694, Loss_eval: 3.5481, Learning Rate: 5.000000e-05\n",
      "Step 2700, Loss: 3.2322, Loss_eval: 3.5513, Learning Rate: 5.000000e-05\n",
      "Step 2800, Loss: 3.2737, Loss_eval: 3.5464, Learning Rate: 5.000000e-05\n",
      "Step 2900, Loss: 3.3568, Loss_eval: 3.5340, Learning Rate: 5.000000e-05\n",
      "Step 3000, Loss: 3.2902, Loss_eval: 3.5437, Learning Rate: 5.000000e-05\n",
      "Step 3100, Loss: 3.1250, Loss_eval: 3.5266, Learning Rate: 5.000000e-05\n",
      "Step 3200, Loss: 3.3644, Loss_eval: 3.5397, Learning Rate: 5.000000e-05\n",
      "Step 3300, Loss: 3.3389, Loss_eval: 3.5336, Learning Rate: 5.000000e-05\n",
      "Step 3400, Loss: 2.9976, Loss_eval: 3.5193, Learning Rate: 5.000000e-05\n",
      "Step 3500, Loss: 3.0374, Loss_eval: 3.5602, Learning Rate: 5.000000e-05\n",
      "Step 3600, Loss: 3.2729, Loss_eval: 3.5453, Learning Rate: 5.000000e-05\n",
      "Step 3700, Loss: 3.1777, Loss_eval: 3.5373, Learning Rate: 5.000000e-05\n",
      "Step 3800, Loss: 3.4849, Loss_eval: 3.5136, Learning Rate: 5.000000e-05\n",
      "Step 3900, Loss: 3.0234, Loss_eval: 3.5179, Learning Rate: 5.000000e-05\n",
      "Step 4000, Loss: 3.1469, Loss_eval: 3.5392, Learning Rate: 5.000000e-05\n",
      "Step 4100, Loss: 3.5150, Loss_eval: 3.5719, Learning Rate: 5.000000e-05\n",
      "Step 4200, Loss: 3.1313, Loss_eval: 3.5384, Learning Rate: 5.000000e-05\n",
      "Step 4300, Loss: 3.1653, Loss_eval: 3.5300, Learning Rate: 5.000000e-05\n",
      "Step 4400, Loss: 3.4465, Loss_eval: 3.5528, Learning Rate: 5.000000e-05\n",
      "Step 4500, Loss: 3.2166, Loss_eval: 3.5531, Learning Rate: 5.000000e-05\n",
      "Step 4600, Loss: 3.4463, Loss_eval: 3.5259, Learning Rate: 5.000000e-05\n",
      "Step 4700, Loss: 2.8390, Loss_eval: 3.5434, Learning Rate: 5.000000e-05\n",
      "Step 4800, Loss: 3.1123, Loss_eval: 3.5253, Learning Rate: 5.000000e-05\n",
      "Step 4900, Loss: 3.5235, Loss_eval: 3.5352, Learning Rate: 5.000000e-05\n",
      "Step 5000, Loss: 3.1476, Loss_eval: 3.5009, Learning Rate: 5.000000e-05\n",
      "Step 5100, Loss: 2.8980, Loss_eval: 3.5148, Learning Rate: 5.000000e-05\n",
      "Step 5200, Loss: 3.1229, Loss_eval: 3.5436, Learning Rate: 5.000000e-05\n",
      "Step 5300, Loss: 3.4334, Loss_eval: 3.5719, Learning Rate: 5.000000e-05\n",
      "Step 5400, Loss: 3.4465, Loss_eval: 3.5187, Learning Rate: 5.000000e-05\n",
      "Step 5500, Loss: 3.5754, Loss_eval: 3.5463, Learning Rate: 5.000000e-05\n",
      "Step 5600, Loss: 3.3426, Loss_eval: 3.5044, Learning Rate: 5.000000e-05\n",
      "Step 5700, Loss: 3.0374, Loss_eval: 3.4924, Learning Rate: 5.000000e-05\n",
      "Step 5800, Loss: 3.1059, Loss_eval: 3.5536, Learning Rate: 5.000000e-05\n",
      "Step 5900, Loss: 3.3035, Loss_eval: 3.5110, Learning Rate: 5.000000e-05\n",
      "Step 6000, Loss: 3.0495, Loss_eval: 3.5266, Learning Rate: 5.000000e-05\n",
      "Step 6100, Loss: 3.2338, Loss_eval: 3.5214, Learning Rate: 5.000000e-05\n",
      "Step 6200, Loss: 3.1920, Loss_eval: 3.5206, Learning Rate: 5.000000e-05\n",
      "Step 6300, Loss: 3.2803, Loss_eval: 3.5405, Learning Rate: 5.000000e-05\n",
      "Step 6400, Loss: 3.3224, Loss_eval: 3.5428, Learning Rate: 5.000000e-05\n",
      "Step 6500, Loss: 2.8191, Loss_eval: 3.5467, Learning Rate: 5.000000e-05\n",
      "Step 6600, Loss: 3.4639, Loss_eval: 3.4922, Learning Rate: 5.000000e-05\n",
      "Step 6700, Loss: 3.2514, Loss_eval: 3.5148, Learning Rate: 5.000000e-05\n",
      "Step 6800, Loss: 3.3266, Loss_eval: 3.5300, Learning Rate: 5.000000e-05\n",
      "Step 6900, Loss: 2.8618, Loss_eval: 3.5332, Learning Rate: 5.000000e-05\n",
      "Step 7000, Loss: 3.1180, Loss_eval: 3.5328, Learning Rate: 5.000000e-05\n",
      "Step 7100, Loss: 3.3631, Loss_eval: 3.4774, Learning Rate: 5.000000e-05\n",
      "Step 7200, Loss: 3.2536, Loss_eval: 3.5298, Learning Rate: 5.000000e-05\n",
      "Step 7300, Loss: 3.0270, Loss_eval: 3.5435, Learning Rate: 5.000000e-05\n",
      "Step 7400, Loss: 3.0850, Loss_eval: 3.5072, Learning Rate: 5.000000e-05\n",
      "Step 7500, Loss: 3.3505, Loss_eval: 3.5426, Learning Rate: 5.000000e-05\n",
      "Step 7600, Loss: 3.0782, Loss_eval: 3.5370, Learning Rate: 5.000000e-05\n",
      "Step 7700, Loss: 3.1537, Loss_eval: 3.5463, Learning Rate: 5.000000e-05\n",
      "Step 7800, Loss: 3.1145, Loss_eval: 3.5398, Learning Rate: 5.000000e-05\n",
      "Step 7900, Loss: 3.3719, Loss_eval: 3.5000, Learning Rate: 5.000000e-05\n",
      "Step 8000, Loss: 3.2210, Loss_eval: 3.5447, Learning Rate: 5.000000e-05\n",
      "Step 8100, Loss: 3.0245, Loss_eval: 3.5349, Learning Rate: 5.000000e-05\n",
      "Step 8200, Loss: 3.3069, Loss_eval: 3.5215, Learning Rate: 5.000000e-05\n",
      "Step 8300, Loss: 3.3357, Loss_eval: 3.5183, Learning Rate: 5.000000e-05\n",
      "Step 8400, Loss: 3.5103, Loss_eval: 3.5410, Learning Rate: 5.000000e-05\n",
      "Step 8500, Loss: 3.3703, Loss_eval: 3.5479, Learning Rate: 5.000000e-05\n",
      "Step 8600, Loss: 3.1735, Loss_eval: 3.5587, Learning Rate: 5.000000e-05\n",
      "Step 8700, Loss: 2.9935, Loss_eval: 3.5161, Learning Rate: 5.000000e-05\n",
      "Step 8800, Loss: 2.9353, Loss_eval: 3.5329, Learning Rate: 5.000000e-05\n",
      "Step 8900, Loss: 3.0804, Loss_eval: 3.5287, Learning Rate: 5.000000e-05\n",
      "Step 9000, Loss: 2.9529, Loss_eval: 3.4875, Learning Rate: 5.000000e-05\n",
      "Step 9100, Loss: 3.3587, Loss_eval: 3.5715, Learning Rate: 5.000000e-05\n",
      "Step 9200, Loss: 3.0786, Loss_eval: 3.5319, Learning Rate: 5.000000e-05\n",
      "Step 9300, Loss: 3.0554, Loss_eval: 3.5476, Learning Rate: 5.000000e-05\n",
      "Step 9400, Loss: 3.3021, Loss_eval: 3.5437, Learning Rate: 5.000000e-05\n",
      "Step 9500, Loss: 3.1669, Loss_eval: 3.5156, Learning Rate: 5.000000e-05\n",
      "Step 9600, Loss: 3.2862, Loss_eval: 3.5138, Learning Rate: 5.000000e-05\n",
      "Step 9700, Loss: 3.0825, Loss_eval: 3.5477, Learning Rate: 5.000000e-05\n",
      "Step 9800, Loss: 3.0014, Loss_eval: 3.5257, Learning Rate: 5.000000e-05\n",
      "Step 9900, Loss: 3.1607, Loss_eval: 3.5462, Learning Rate: 5.000000e-05\n",
      "Step 10000, Loss: 3.3764, Loss_eval: 3.5417, Learning Rate: 5.000000e-05\n",
      "Step 10100, Loss: 3.5095, Loss_eval: 3.5387, Learning Rate: 5.000000e-05\n",
      "Step 10200, Loss: 3.5248, Loss_eval: 3.5244, Learning Rate: 5.000000e-05\n",
      "Step 10300, Loss: 3.6005, Loss_eval: 3.5094, Learning Rate: 5.000000e-05\n",
      "Step 10400, Loss: 3.4788, Loss_eval: 3.5414, Learning Rate: 5.000000e-05\n",
      "Step 10500, Loss: 3.2253, Loss_eval: 3.5414, Learning Rate: 5.000000e-05\n",
      "Step 10600, Loss: 3.1233, Loss_eval: 3.5254, Learning Rate: 5.000000e-05\n",
      "Step 10700, Loss: 3.4100, Loss_eval: 3.5157, Learning Rate: 5.000000e-05\n",
      "Step 10800, Loss: 3.3158, Loss_eval: 3.5143, Learning Rate: 5.000000e-05\n",
      "Step 10900, Loss: 3.3196, Loss_eval: 3.5257, Learning Rate: 5.000000e-05\n",
      "Step 11000, Loss: 3.3692, Loss_eval: 3.5301, Learning Rate: 5.000000e-05\n",
      "Step 11100, Loss: 3.2673, Loss_eval: 3.5211, Learning Rate: 5.000000e-05\n",
      "Step 11200, Loss: 3.2300, Loss_eval: 3.5202, Learning Rate: 5.000000e-05\n",
      "Step 11300, Loss: 3.5844, Loss_eval: 3.5474, Learning Rate: 5.000000e-05\n",
      "Step 11400, Loss: 3.2541, Loss_eval: 3.5072, Learning Rate: 5.000000e-05\n",
      "Step 11500, Loss: 3.0342, Loss_eval: 3.5447, Learning Rate: 5.000000e-05\n",
      "Step 11600, Loss: 3.5023, Loss_eval: 3.5411, Learning Rate: 5.000000e-05\n",
      "Step 11700, Loss: 3.2346, Loss_eval: 3.5337, Learning Rate: 5.000000e-05\n",
      "Step 11800, Loss: 3.1635, Loss_eval: 3.5636, Learning Rate: 5.000000e-05\n",
      "Step 11900, Loss: 3.2423, Loss_eval: 3.5487, Learning Rate: 5.000000e-05\n",
      "Step 12000, Loss: 3.0641, Loss_eval: 3.5282, Learning Rate: 5.000000e-05\n",
      "Step 12100, Loss: 3.2881, Loss_eval: 3.5436, Learning Rate: 5.000000e-05\n",
      "Step 12200, Loss: 3.4826, Loss_eval: 3.5104, Learning Rate: 5.000000e-05\n",
      "Step 12300, Loss: 3.1915, Loss_eval: 3.5558, Learning Rate: 5.000000e-05\n",
      "Step 12400, Loss: 3.3339, Loss_eval: 3.5503, Learning Rate: 5.000000e-05\n",
      "Step 12500, Loss: 3.3104, Loss_eval: 3.5143, Learning Rate: 5.000000e-05\n",
      "Step 12600, Loss: 3.2196, Loss_eval: 3.5211, Learning Rate: 5.000000e-05\n",
      "Step 12700, Loss: 3.4566, Loss_eval: 3.5394, Learning Rate: 5.000000e-05\n",
      "Step 12800, Loss: 2.9606, Loss_eval: 3.5451, Learning Rate: 5.000000e-05\n",
      "Step 12900, Loss: 3.0845, Loss_eval: 3.5204, Learning Rate: 5.000000e-05\n",
      "Step 13000, Loss: 3.1256, Loss_eval: 3.5469, Learning Rate: 5.000000e-05\n",
      "Step 13100, Loss: 3.2447, Loss_eval: 3.5376, Learning Rate: 5.000000e-05\n",
      "Step 13200, Loss: 3.0574, Loss_eval: 3.5037, Learning Rate: 5.000000e-05\n",
      "Step 13300, Loss: 3.0545, Loss_eval: 3.5115, Learning Rate: 5.000000e-05\n",
      "Step 13400, Loss: 3.1500, Loss_eval: 3.5634, Learning Rate: 5.000000e-05\n",
      "Step 13500, Loss: 3.1304, Loss_eval: 3.5501, Learning Rate: 5.000000e-05\n",
      "Step 13600, Loss: 3.0864, Loss_eval: 3.5136, Learning Rate: 5.000000e-05\n",
      "Step 13700, Loss: 3.1068, Loss_eval: 3.5366, Learning Rate: 5.000000e-05\n",
      "Step 13800, Loss: 3.1442, Loss_eval: 3.5127, Learning Rate: 5.000000e-05\n",
      "Step 13900, Loss: 3.3129, Loss_eval: 3.4899, Learning Rate: 5.000000e-05\n",
      "Step 14000, Loss: 3.3976, Loss_eval: 3.5394, Learning Rate: 5.000000e-05\n",
      "Step 14100, Loss: 3.0335, Loss_eval: 3.5105, Learning Rate: 5.000000e-05\n",
      "Step 14200, Loss: 3.3068, Loss_eval: 3.5489, Learning Rate: 5.000000e-05\n",
      "Step 14300, Loss: 3.4918, Loss_eval: 3.5238, Learning Rate: 5.000000e-05\n",
      "Step 14400, Loss: 3.4442, Loss_eval: 3.5467, Learning Rate: 5.000000e-05\n",
      "Step 14500, Loss: 3.2664, Loss_eval: 3.5353, Learning Rate: 5.000000e-05\n",
      "Step 14600, Loss: 3.2633, Loss_eval: 3.5108, Learning Rate: 5.000000e-05\n",
      "Step 14700, Loss: 3.2770, Loss_eval: 3.5346, Learning Rate: 5.000000e-05\n",
      "Step 14800, Loss: 3.4756, Loss_eval: 3.5445, Learning Rate: 5.000000e-05\n",
      "Step 14900, Loss: 3.2646, Loss_eval: 3.5228, Learning Rate: 5.000000e-05\n",
      "Step 15000, Loss: 2.9358, Loss_eval: 3.5179, Learning Rate: 5.000000e-05\n",
      "Step 15100, Loss: 3.3247, Loss_eval: 3.5481, Learning Rate: 5.000000e-05\n",
      "Step 15200, Loss: 3.4162, Loss_eval: 3.5071, Learning Rate: 5.000000e-05\n",
      "Step 15300, Loss: 3.0522, Loss_eval: 3.5382, Learning Rate: 5.000000e-05\n",
      "Step 15400, Loss: 3.4654, Loss_eval: 3.5396, Learning Rate: 5.000000e-05\n",
      "Step 15500, Loss: 3.4431, Loss_eval: 3.5409, Learning Rate: 5.000000e-05\n",
      "Step 15600, Loss: 3.5300, Loss_eval: 3.5376, Learning Rate: 5.000000e-05\n",
      "Step 15700, Loss: 2.9190, Loss_eval: 3.5247, Learning Rate: 5.000000e-05\n",
      "Step 15800, Loss: 3.0583, Loss_eval: 3.5054, Learning Rate: 5.000000e-05\n",
      "Step 15900, Loss: 3.4503, Loss_eval: 3.5106, Learning Rate: 5.000000e-05\n",
      "Step 16000, Loss: 3.2376, Loss_eval: 3.5169, Learning Rate: 5.000000e-05\n",
      "Step 16100, Loss: 3.3538, Loss_eval: 3.5370, Learning Rate: 5.000000e-05\n",
      "Step 16200, Loss: 3.0669, Loss_eval: 3.5232, Learning Rate: 5.000000e-05\n",
      "Step 16300, Loss: 3.3855, Loss_eval: 3.5468, Learning Rate: 5.000000e-05\n",
      "Step 16400, Loss: 3.5534, Loss_eval: 3.4963, Learning Rate: 5.000000e-05\n",
      "Step 16500, Loss: 3.6564, Loss_eval: 3.5510, Learning Rate: 5.000000e-05\n",
      "Step 16600, Loss: 3.2098, Loss_eval: 3.5600, Learning Rate: 5.000000e-05\n",
      "Step 16700, Loss: 2.9043, Loss_eval: 3.5057, Learning Rate: 5.000000e-05\n",
      "Step 16800, Loss: 3.1065, Loss_eval: 3.5095, Learning Rate: 5.000000e-05\n",
      "Step 16900, Loss: 3.5111, Loss_eval: 3.5348, Learning Rate: 5.000000e-05\n",
      "Step 17000, Loss: 3.2519, Loss_eval: 3.5195, Learning Rate: 5.000000e-05\n",
      "Step 17100, Loss: 3.2410, Loss_eval: 3.5111, Learning Rate: 5.000000e-05\n",
      "Step 17200, Loss: 3.2222, Loss_eval: 3.5236, Learning Rate: 5.000000e-05\n",
      "Step 17300, Loss: 3.2279, Loss_eval: 3.5254, Learning Rate: 5.000000e-05\n",
      "Step 17400, Loss: 2.9426, Loss_eval: 3.5375, Learning Rate: 5.000000e-05\n",
      "Step 17500, Loss: 3.5515, Loss_eval: 3.5340, Learning Rate: 5.000000e-05\n",
      "Step 17600, Loss: 3.3062, Loss_eval: 3.5080, Learning Rate: 5.000000e-05\n",
      "Step 17700, Loss: 3.1596, Loss_eval: 3.5128, Learning Rate: 5.000000e-05\n",
      "Step 17800, Loss: 2.9505, Loss_eval: 3.5129, Learning Rate: 5.000000e-05\n",
      "Step 17900, Loss: 3.0017, Loss_eval: 3.5347, Learning Rate: 5.000000e-05\n",
      "Step 18000, Loss: 3.4002, Loss_eval: 3.5530, Learning Rate: 5.000000e-05\n",
      "Step 18100, Loss: 3.5525, Loss_eval: 3.4852, Learning Rate: 5.000000e-05\n",
      "Step 18200, Loss: 3.1656, Loss_eval: 3.5401, Learning Rate: 5.000000e-05\n",
      "Step 18300, Loss: 3.3815, Loss_eval: 3.5029, Learning Rate: 5.000000e-05\n",
      "Step 18400, Loss: 3.4975, Loss_eval: 3.5591, Learning Rate: 5.000000e-05\n",
      "Step 18500, Loss: 2.9388, Loss_eval: 3.5323, Learning Rate: 5.000000e-05\n",
      "Step 18600, Loss: 3.3575, Loss_eval: 3.4969, Learning Rate: 5.000000e-05\n",
      "Step 18700, Loss: 3.1870, Loss_eval: 3.5023, Learning Rate: 5.000000e-05\n",
      "Step 18800, Loss: 3.0704, Loss_eval: 3.5436, Learning Rate: 5.000000e-05\n",
      "Step 18900, Loss: 3.0953, Loss_eval: 3.5116, Learning Rate: 5.000000e-05\n",
      "Step 19000, Loss: 3.2013, Loss_eval: 3.5533, Learning Rate: 5.000000e-05\n",
      "Step 19100, Loss: 3.1577, Loss_eval: 3.5106, Learning Rate: 5.000000e-05\n",
      "Step 19200, Loss: 3.2796, Loss_eval: 3.5049, Learning Rate: 5.000000e-05\n",
      "Step 19300, Loss: 3.0931, Loss_eval: 3.5345, Learning Rate: 5.000000e-05\n",
      "Step 19400, Loss: 3.6581, Loss_eval: 3.5413, Learning Rate: 5.000000e-05\n",
      "Step 19500, Loss: 3.1328, Loss_eval: 3.5397, Learning Rate: 5.000000e-05\n",
      "Step 19600, Loss: 3.3220, Loss_eval: 3.5385, Learning Rate: 5.000000e-05\n",
      "Step 19700, Loss: 3.2906, Loss_eval: 3.5055, Learning Rate: 5.000000e-05\n",
      "Step 19800, Loss: 3.4189, Loss_eval: 3.5313, Learning Rate: 5.000000e-05\n",
      "Step 19900, Loss: 3.1793, Loss_eval: 3.5205, Learning Rate: 5.000000e-05\n",
      "Step 20000, Loss: 3.4591, Loss_eval: 3.5258, Learning Rate: 5.000000e-05\n",
      "Step 20100, Loss: 3.4758, Loss_eval: 3.5043, Learning Rate: 5.000000e-05\n",
      "Step 20200, Loss: 3.3698, Loss_eval: 3.5249, Learning Rate: 5.000000e-05\n",
      "Step 20300, Loss: 3.1411, Loss_eval: 3.5189, Learning Rate: 5.000000e-05\n",
      "Step 20400, Loss: 3.4441, Loss_eval: 3.5088, Learning Rate: 5.000000e-05\n",
      "Step 20500, Loss: 2.8780, Loss_eval: 3.5072, Learning Rate: 5.000000e-05\n",
      "Step 20600, Loss: 3.1409, Loss_eval: 3.4973, Learning Rate: 5.000000e-05\n",
      "Step 20700, Loss: 3.3522, Loss_eval: 3.5193, Learning Rate: 5.000000e-05\n",
      "Step 20800, Loss: 3.1770, Loss_eval: 3.5330, Learning Rate: 5.000000e-05\n",
      "Step 20900, Loss: 3.3015, Loss_eval: 3.5348, Learning Rate: 5.000000e-05\n",
      "Step 21000, Loss: 3.3040, Loss_eval: 3.5481, Learning Rate: 5.000000e-05\n",
      "Step 21100, Loss: 3.5069, Loss_eval: 3.5339, Learning Rate: 5.000000e-05\n",
      "Step 21200, Loss: 3.2415, Loss_eval: 3.4900, Learning Rate: 5.000000e-05\n",
      "Step 21300, Loss: 3.3052, Loss_eval: 3.5339, Learning Rate: 5.000000e-05\n",
      "Step 21400, Loss: 3.7294, Loss_eval: 3.5045, Learning Rate: 5.000000e-05\n",
      "Step 21500, Loss: 2.9938, Loss_eval: 3.5220, Learning Rate: 5.000000e-05\n",
      "Step 21600, Loss: 3.5401, Loss_eval: 3.5142, Learning Rate: 5.000000e-05\n",
      "Step 21700, Loss: 2.8704, Loss_eval: 3.5298, Learning Rate: 5.000000e-05\n",
      "Step 21800, Loss: 3.2976, Loss_eval: 3.5423, Learning Rate: 5.000000e-05\n",
      "Step 21900, Loss: 3.2948, Loss_eval: 3.5171, Learning Rate: 5.000000e-05\n",
      "Step 22000, Loss: 3.0569, Loss_eval: 3.5080, Learning Rate: 5.000000e-05\n",
      "Step 22100, Loss: 3.3396, Loss_eval: 3.5285, Learning Rate: 5.000000e-05\n",
      "Step 22200, Loss: 3.2558, Loss_eval: 3.5408, Learning Rate: 5.000000e-05\n",
      "Step 22300, Loss: 2.9391, Loss_eval: 3.5351, Learning Rate: 5.000000e-05\n",
      "Step 22400, Loss: 2.9846, Loss_eval: 3.5384, Learning Rate: 5.000000e-05\n",
      "Step 22500, Loss: 3.2631, Loss_eval: 3.5163, Learning Rate: 5.000000e-05\n",
      "Step 22600, Loss: 3.3028, Loss_eval: 3.5567, Learning Rate: 5.000000e-05\n",
      "Step 22700, Loss: 3.0783, Loss_eval: 3.5043, Learning Rate: 5.000000e-05\n",
      "Step 22800, Loss: 3.4049, Loss_eval: 3.5428, Learning Rate: 5.000000e-05\n",
      "Step 22900, Loss: 3.3659, Loss_eval: 3.4946, Learning Rate: 5.000000e-05\n",
      "Step 23000, Loss: 3.0341, Loss_eval: 3.5089, Learning Rate: 5.000000e-05\n",
      "Step 23100, Loss: 3.3608, Loss_eval: 3.5404, Learning Rate: 5.000000e-05\n",
      "Step 23200, Loss: 3.2030, Loss_eval: 3.4897, Learning Rate: 5.000000e-05\n",
      "Step 23300, Loss: 3.2192, Loss_eval: 3.5310, Learning Rate: 5.000000e-05\n",
      "Step 23400, Loss: 3.3976, Loss_eval: 3.5309, Learning Rate: 5.000000e-05\n",
      "Step 23500, Loss: 3.1696, Loss_eval: 3.5133, Learning Rate: 5.000000e-05\n",
      "Step 23600, Loss: 3.1682, Loss_eval: 3.5160, Learning Rate: 5.000000e-05\n",
      "Step 23700, Loss: 3.0904, Loss_eval: 3.5164, Learning Rate: 5.000000e-05\n",
      "Step 23800, Loss: 3.1440, Loss_eval: 3.5272, Learning Rate: 5.000000e-05\n",
      "Step 23900, Loss: 3.6715, Loss_eval: 3.5192, Learning Rate: 5.000000e-05\n",
      "Step 24000, Loss: 3.4862, Loss_eval: 3.5322, Learning Rate: 5.000000e-05\n",
      "Step 24100, Loss: 3.0485, Loss_eval: 3.5037, Learning Rate: 5.000000e-05\n",
      "Step 24200, Loss: 3.1847, Loss_eval: 3.5140, Learning Rate: 5.000000e-05\n",
      "Step 24300, Loss: 3.1447, Loss_eval: 3.5198, Learning Rate: 5.000000e-05\n",
      "Step 24400, Loss: 3.2267, Loss_eval: 3.5065, Learning Rate: 5.000000e-05\n",
      "Step 24500, Loss: 3.2650, Loss_eval: 3.5498, Learning Rate: 5.000000e-05\n",
      "Step 24600, Loss: 3.0209, Loss_eval: 3.5415, Learning Rate: 5.000000e-05\n",
      "Step 24700, Loss: 3.0935, Loss_eval: 3.5114, Learning Rate: 5.000000e-05\n",
      "Step 24800, Loss: 3.2150, Loss_eval: 3.5197, Learning Rate: 5.000000e-05\n",
      "Step 24900, Loss: 3.1565, Loss_eval: 3.5362, Learning Rate: 5.000000e-05\n",
      "Step 25000, Loss: 3.1735, Loss_eval: 3.5038, Learning Rate: 5.000000e-05\n",
      "Step 25100, Loss: 2.9897, Loss_eval: 3.5262, Learning Rate: 5.000000e-05\n",
      "Step 25200, Loss: 3.5050, Loss_eval: 3.5047, Learning Rate: 5.000000e-05\n",
      "Step 25300, Loss: 3.3364, Loss_eval: 3.5251, Learning Rate: 5.000000e-05\n",
      "Step 25400, Loss: 3.1325, Loss_eval: 3.5073, Learning Rate: 5.000000e-05\n",
      "Step 25500, Loss: 3.2483, Loss_eval: 3.5193, Learning Rate: 5.000000e-05\n",
      "Step 25600, Loss: 3.2866, Loss_eval: 3.5154, Learning Rate: 5.000000e-05\n",
      "Step 25700, Loss: 3.1388, Loss_eval: 3.5255, Learning Rate: 5.000000e-05\n",
      "Step 25800, Loss: 3.3691, Loss_eval: 3.5472, Learning Rate: 5.000000e-05\n",
      "Step 25900, Loss: 3.3792, Loss_eval: 3.5255, Learning Rate: 5.000000e-05\n",
      "Step 26000, Loss: 3.1117, Loss_eval: 3.5143, Learning Rate: 5.000000e-05\n",
      "Step 26100, Loss: 3.4167, Loss_eval: 3.5084, Learning Rate: 5.000000e-05\n",
      "Step 26200, Loss: 3.3363, Loss_eval: 3.5011, Learning Rate: 5.000000e-05\n",
      "Step 26300, Loss: 3.1262, Loss_eval: 3.5344, Learning Rate: 5.000000e-05\n",
      "Step 26400, Loss: 3.3056, Loss_eval: 3.5044, Learning Rate: 5.000000e-05\n",
      "Step 26500, Loss: 3.0462, Loss_eval: 3.5093, Learning Rate: 5.000000e-05\n",
      "Step 26600, Loss: 2.9947, Loss_eval: 3.4943, Learning Rate: 5.000000e-05\n",
      "Step 26700, Loss: 3.2084, Loss_eval: 3.5471, Learning Rate: 5.000000e-05\n",
      "Step 26800, Loss: 3.2297, Loss_eval: 3.5482, Learning Rate: 5.000000e-05\n",
      "Step 26900, Loss: 3.3369, Loss_eval: 3.5433, Learning Rate: 5.000000e-05\n",
      "Step 27000, Loss: 3.4526, Loss_eval: 3.4889, Learning Rate: 5.000000e-05\n",
      "Step 27100, Loss: 3.3895, Loss_eval: 3.5264, Learning Rate: 5.000000e-05\n",
      "Step 27200, Loss: 3.2169, Loss_eval: 3.5307, Learning Rate: 5.000000e-05\n",
      "Step 27300, Loss: 3.4588, Loss_eval: 3.5554, Learning Rate: 5.000000e-05\n",
      "Step 27400, Loss: 2.9089, Loss_eval: 3.5130, Learning Rate: 5.000000e-05\n",
      "Step 27500, Loss: 3.3560, Loss_eval: 3.5045, Learning Rate: 5.000000e-05\n",
      "Step 27600, Loss: 3.4302, Loss_eval: 3.5062, Learning Rate: 5.000000e-05\n",
      "Step 27700, Loss: 3.2369, Loss_eval: 3.5234, Learning Rate: 5.000000e-05\n",
      "Step 27800, Loss: 3.4786, Loss_eval: 3.5151, Learning Rate: 5.000000e-05\n",
      "Step 27900, Loss: 3.1550, Loss_eval: 3.5172, Learning Rate: 5.000000e-05\n",
      "Step 28000, Loss: 3.0279, Loss_eval: 3.5448, Learning Rate: 5.000000e-05\n",
      "Step 28100, Loss: 2.8757, Loss_eval: 3.5366, Learning Rate: 5.000000e-05\n",
      "Step 28200, Loss: 3.0997, Loss_eval: 3.5404, Learning Rate: 5.000000e-05\n",
      "Step 28300, Loss: 3.3618, Loss_eval: 3.5041, Learning Rate: 5.000000e-05\n",
      "Step 28400, Loss: 3.0195, Loss_eval: 3.5153, Learning Rate: 5.000000e-05\n",
      "Step 28500, Loss: 3.2735, Loss_eval: 3.5190, Learning Rate: 5.000000e-05\n",
      "Step 28600, Loss: 3.2371, Loss_eval: 3.5168, Learning Rate: 5.000000e-05\n",
      "Step 28700, Loss: 3.1704, Loss_eval: 3.5178, Learning Rate: 5.000000e-05\n",
      "Step 28800, Loss: 3.4296, Loss_eval: 3.5127, Learning Rate: 5.000000e-05\n",
      "Step 28900, Loss: 3.3191, Loss_eval: 3.5517, Learning Rate: 5.000000e-05\n",
      "Step 29000, Loss: 3.3219, Loss_eval: 3.5187, Learning Rate: 5.000000e-05\n",
      "Step 29100, Loss: 3.5008, Loss_eval: 3.5124, Learning Rate: 5.000000e-05\n",
      "Step 29200, Loss: 3.0817, Loss_eval: 3.5102, Learning Rate: 5.000000e-05\n",
      "Step 29300, Loss: 3.3112, Loss_eval: 3.5376, Learning Rate: 5.000000e-05\n",
      "Step 29400, Loss: 3.1126, Loss_eval: 3.5099, Learning Rate: 5.000000e-05\n",
      "Step 29500, Loss: 3.0695, Loss_eval: 3.5402, Learning Rate: 5.000000e-05\n",
      "Step 29600, Loss: 3.3362, Loss_eval: 3.5461, Learning Rate: 5.000000e-05\n",
      "Step 29700, Loss: 3.0693, Loss_eval: 3.5360, Learning Rate: 5.000000e-05\n",
      "Step 29800, Loss: 3.3610, Loss_eval: 3.5332, Learning Rate: 5.000000e-05\n",
      "Step 29900, Loss: 3.1185, Loss_eval: 3.4891, Learning Rate: 5.000000e-05\n",
      "Step 30000, Loss: 3.3933, Loss_eval: 3.5471, Learning Rate: 5.000000e-05\n",
      "Step 30100, Loss: 3.4636, Loss_eval: 3.5170, Learning Rate: 5.000000e-05\n",
      "Step 30200, Loss: 3.0947, Loss_eval: 3.5404, Learning Rate: 5.000000e-05\n",
      "Step 30300, Loss: 3.3872, Loss_eval: 3.5016, Learning Rate: 5.000000e-05\n",
      "Step 30400, Loss: 3.8498, Loss_eval: 3.4988, Learning Rate: 5.000000e-05\n",
      "Step 30500, Loss: 3.0497, Loss_eval: 3.5031, Learning Rate: 5.000000e-05\n",
      "Step 30600, Loss: 2.8642, Loss_eval: 3.5054, Learning Rate: 5.000000e-05\n",
      "Step 30700, Loss: 3.6784, Loss_eval: 3.5073, Learning Rate: 5.000000e-05\n",
      "Step 30800, Loss: 3.1904, Loss_eval: 3.5199, Learning Rate: 5.000000e-05\n",
      "Step 30900, Loss: 3.2560, Loss_eval: 3.5195, Learning Rate: 5.000000e-05\n",
      "Step 31000, Loss: 3.1324, Loss_eval: 3.5325, Learning Rate: 5.000000e-05\n",
      "Step 31100, Loss: 3.1757, Loss_eval: 3.5238, Learning Rate: 5.000000e-05\n",
      "Step 31200, Loss: 3.1767, Loss_eval: 3.5321, Learning Rate: 5.000000e-05\n",
      "Step 31300, Loss: 3.3137, Loss_eval: 3.5131, Learning Rate: 5.000000e-05\n",
      "Step 31400, Loss: 3.3969, Loss_eval: 3.5471, Learning Rate: 5.000000e-05\n",
      "Step 31500, Loss: 3.3069, Loss_eval: 3.5371, Learning Rate: 5.000000e-05\n",
      "Step 31600, Loss: 3.0271, Loss_eval: 3.5004, Learning Rate: 5.000000e-05\n",
      "Step 31700, Loss: 3.5588, Loss_eval: 3.5305, Learning Rate: 5.000000e-05\n",
      "Step 31800, Loss: 3.0891, Loss_eval: 3.4866, Learning Rate: 5.000000e-05\n",
      "Step 31900, Loss: 3.1321, Loss_eval: 3.5580, Learning Rate: 5.000000e-05\n",
      "Step 32000, Loss: 3.3370, Loss_eval: 3.5477, Learning Rate: 5.000000e-05\n",
      "Step 32100, Loss: 3.2088, Loss_eval: 3.5007, Learning Rate: 5.000000e-05\n",
      "Step 32200, Loss: 2.9252, Loss_eval: 3.5392, Learning Rate: 5.000000e-05\n",
      "Step 32300, Loss: 3.3022, Loss_eval: 3.5310, Learning Rate: 5.000000e-05\n",
      "Step 32400, Loss: 3.1021, Loss_eval: 3.5123, Learning Rate: 5.000000e-05\n",
      "Step 32500, Loss: 3.1266, Loss_eval: 3.5446, Learning Rate: 5.000000e-05\n",
      "Step 32600, Loss: 3.3350, Loss_eval: 3.5110, Learning Rate: 5.000000e-05\n",
      "Step 32700, Loss: 3.1650, Loss_eval: 3.5298, Learning Rate: 5.000000e-05\n",
      "Step 32800, Loss: 3.1706, Loss_eval: 3.5335, Learning Rate: 5.000000e-05\n",
      "Step 32900, Loss: 3.3804, Loss_eval: 3.5241, Learning Rate: 5.000000e-05\n",
      "Step 33000, Loss: 2.8313, Loss_eval: 3.5235, Learning Rate: 5.000000e-05\n",
      "Step 33100, Loss: 3.1696, Loss_eval: 3.5077, Learning Rate: 5.000000e-05\n",
      "Step 33200, Loss: 3.1534, Loss_eval: 3.5099, Learning Rate: 5.000000e-05\n",
      "Step 33300, Loss: 3.1223, Loss_eval: 3.5330, Learning Rate: 5.000000e-05\n",
      "Step 33400, Loss: 3.2684, Loss_eval: 3.5370, Learning Rate: 5.000000e-05\n",
      "Step 33500, Loss: 3.0452, Loss_eval: 3.5503, Learning Rate: 5.000000e-05\n",
      "Step 33600, Loss: 3.2066, Loss_eval: 3.5060, Learning Rate: 5.000000e-05\n",
      "Step 33700, Loss: 3.5796, Loss_eval: 3.5144, Learning Rate: 5.000000e-05\n",
      "Step 33800, Loss: 3.1540, Loss_eval: 3.5020, Learning Rate: 5.000000e-05\n",
      "Step 33900, Loss: 3.2397, Loss_eval: 3.5287, Learning Rate: 5.000000e-05\n",
      "Step 34000, Loss: 3.2297, Loss_eval: 3.5110, Learning Rate: 5.000000e-05\n",
      "Step 34100, Loss: 3.6868, Loss_eval: 3.5096, Learning Rate: 5.000000e-05\n",
      "Step 34200, Loss: 3.0720, Loss_eval: 3.5250, Learning Rate: 5.000000e-05\n",
      "Step 34300, Loss: 3.0206, Loss_eval: 3.5213, Learning Rate: 5.000000e-05\n",
      "Step 34400, Loss: 2.9762, Loss_eval: 3.5487, Learning Rate: 5.000000e-05\n",
      "Step 34500, Loss: 3.1935, Loss_eval: 3.5268, Learning Rate: 5.000000e-05\n",
      "Step 34600, Loss: 3.0465, Loss_eval: 3.5178, Learning Rate: 5.000000e-05\n",
      "Step 34700, Loss: 3.4405, Loss_eval: 3.5580, Learning Rate: 5.000000e-05\n",
      "Step 34800, Loss: 3.6447, Loss_eval: 3.5334, Learning Rate: 5.000000e-05\n",
      "Step 34900, Loss: 3.0566, Loss_eval: 3.5110, Learning Rate: 5.000000e-05\n",
      "Step 35000, Loss: 3.0710, Loss_eval: 3.5235, Learning Rate: 5.000000e-05\n",
      "Step 35100, Loss: 3.3545, Loss_eval: 3.4954, Learning Rate: 5.000000e-05\n",
      "Step 35200, Loss: 3.1711, Loss_eval: 3.5368, Learning Rate: 5.000000e-05\n",
      "Step 35300, Loss: 3.2378, Loss_eval: 3.5020, Learning Rate: 5.000000e-05\n",
      "Step 35400, Loss: 3.2806, Loss_eval: 3.5072, Learning Rate: 5.000000e-05\n",
      "Step 35500, Loss: 3.2002, Loss_eval: 3.5219, Learning Rate: 5.000000e-05\n",
      "Step 35600, Loss: 2.7550, Loss_eval: 3.4996, Learning Rate: 5.000000e-05\n",
      "Step 35700, Loss: 3.1724, Loss_eval: 3.5191, Learning Rate: 5.000000e-05\n",
      "Step 35800, Loss: 3.1179, Loss_eval: 3.5221, Learning Rate: 5.000000e-05\n",
      "Step 35900, Loss: 3.1854, Loss_eval: 3.4992, Learning Rate: 5.000000e-05\n",
      "Step 36000, Loss: 3.4140, Loss_eval: 3.5534, Learning Rate: 5.000000e-05\n",
      "Step 36100, Loss: 3.1980, Loss_eval: 3.5354, Learning Rate: 5.000000e-05\n",
      "Step 36200, Loss: 3.2582, Loss_eval: 3.5289, Learning Rate: 5.000000e-05\n",
      "Step 36300, Loss: 3.1003, Loss_eval: 3.5041, Learning Rate: 5.000000e-05\n",
      "Step 36400, Loss: 3.0588, Loss_eval: 3.4735, Learning Rate: 5.000000e-05\n",
      "Step 36500, Loss: 3.4519, Loss_eval: 3.5042, Learning Rate: 5.000000e-05\n",
      "Step 36600, Loss: 3.2131, Loss_eval: 3.5071, Learning Rate: 5.000000e-05\n",
      "Step 36700, Loss: 3.4950, Loss_eval: 3.5167, Learning Rate: 5.000000e-05\n",
      "Step 36800, Loss: 2.9773, Loss_eval: 3.5015, Learning Rate: 5.000000e-05\n",
      "Step 36900, Loss: 3.2448, Loss_eval: 3.5071, Learning Rate: 5.000000e-05\n",
      "Step 37000, Loss: 3.1261, Loss_eval: 3.5241, Learning Rate: 5.000000e-05\n",
      "Step 37100, Loss: 3.1778, Loss_eval: 3.5057, Learning Rate: 5.000000e-05\n",
      "Step 37200, Loss: 3.0342, Loss_eval: 3.5030, Learning Rate: 5.000000e-05\n",
      "Step 37300, Loss: 3.1351, Loss_eval: 3.5399, Learning Rate: 5.000000e-05\n",
      "Step 37400, Loss: 3.0904, Loss_eval: 3.5264, Learning Rate: 5.000000e-05\n",
      "Step 37500, Loss: 3.2965, Loss_eval: 3.5147, Learning Rate: 5.000000e-05\n",
      "Step 37600, Loss: 3.3615, Loss_eval: 3.5013, Learning Rate: 5.000000e-05\n",
      "Step 37700, Loss: 3.3467, Loss_eval: 3.5121, Learning Rate: 5.000000e-05\n",
      "Step 37800, Loss: 3.1890, Loss_eval: 3.5405, Learning Rate: 5.000000e-05\n",
      "Step 37900, Loss: 3.3843, Loss_eval: 3.5298, Learning Rate: 5.000000e-05\n",
      "Step 38000, Loss: 3.2675, Loss_eval: 3.5203, Learning Rate: 5.000000e-05\n",
      "Step 38100, Loss: 2.9232, Loss_eval: 3.5301, Learning Rate: 5.000000e-05\n",
      "Step 38200, Loss: 3.0352, Loss_eval: 3.5024, Learning Rate: 5.000000e-05\n",
      "Step 38300, Loss: 3.3854, Loss_eval: 3.5337, Learning Rate: 5.000000e-05\n",
      "Step 38400, Loss: 3.2660, Loss_eval: 3.5131, Learning Rate: 5.000000e-05\n",
      "Step 38500, Loss: 3.2296, Loss_eval: 3.5101, Learning Rate: 5.000000e-05\n",
      "Step 38600, Loss: 3.1768, Loss_eval: 3.5126, Learning Rate: 5.000000e-05\n",
      "Step 38700, Loss: 3.0220, Loss_eval: 3.5129, Learning Rate: 5.000000e-05\n",
      "Step 38800, Loss: 3.1036, Loss_eval: 3.4963, Learning Rate: 5.000000e-05\n",
      "Step 38900, Loss: 3.2683, Loss_eval: 3.5240, Learning Rate: 5.000000e-05\n",
      "Step 39000, Loss: 3.6540, Loss_eval: 3.5252, Learning Rate: 5.000000e-05\n",
      "Step 39100, Loss: 3.2820, Loss_eval: 3.4823, Learning Rate: 5.000000e-05\n",
      "Step 39200, Loss: 3.1791, Loss_eval: 3.4910, Learning Rate: 5.000000e-05\n",
      "Step 39300, Loss: 3.3156, Loss_eval: 3.5036, Learning Rate: 5.000000e-05\n",
      "Step 39400, Loss: 3.0148, Loss_eval: 3.5054, Learning Rate: 5.000000e-05\n",
      "Step 39500, Loss: 3.3074, Loss_eval: 3.4990, Learning Rate: 5.000000e-05\n",
      "Step 39600, Loss: 3.2706, Loss_eval: 3.5191, Learning Rate: 5.000000e-05\n",
      "Step 39700, Loss: 3.1181, Loss_eval: 3.5223, Learning Rate: 5.000000e-05\n",
      "Step 39800, Loss: 3.2751, Loss_eval: 3.5111, Learning Rate: 5.000000e-05\n",
      "Step 39900, Loss: 3.3428, Loss_eval: 3.5042, Learning Rate: 5.000000e-05\n",
      "Step 40000, Loss: 3.1053, Loss_eval: 3.5274, Learning Rate: 5.000000e-05\n",
      "Step 40100, Loss: 3.3146, Loss_eval: 3.5051, Learning Rate: 5.000000e-05\n",
      "Step 40200, Loss: 3.5818, Loss_eval: 3.4932, Learning Rate: 5.000000e-05\n",
      "Step 40300, Loss: 3.0252, Loss_eval: 3.5075, Learning Rate: 5.000000e-05\n",
      "Step 40400, Loss: 2.9322, Loss_eval: 3.4822, Learning Rate: 5.000000e-05\n",
      "Step 40500, Loss: 3.2266, Loss_eval: 3.5199, Learning Rate: 5.000000e-05\n",
      "Step 40600, Loss: 3.4215, Loss_eval: 3.5114, Learning Rate: 5.000000e-05\n",
      "Step 40700, Loss: 3.3313, Loss_eval: 3.4656, Learning Rate: 5.000000e-05\n",
      "Step 40800, Loss: 3.1729, Loss_eval: 3.5366, Learning Rate: 5.000000e-05\n",
      "Step 40900, Loss: 3.1866, Loss_eval: 3.5109, Learning Rate: 5.000000e-05\n",
      "Step 41000, Loss: 3.2501, Loss_eval: 3.5171, Learning Rate: 5.000000e-05\n",
      "Step 41100, Loss: 3.4308, Loss_eval: 3.4981, Learning Rate: 5.000000e-05\n",
      "Step 41200, Loss: 3.4049, Loss_eval: 3.5040, Learning Rate: 5.000000e-05\n",
      "Step 41300, Loss: 3.5611, Loss_eval: 3.5102, Learning Rate: 5.000000e-05\n",
      "Step 41400, Loss: 3.5343, Loss_eval: 3.5196, Learning Rate: 5.000000e-05\n",
      "Step 41500, Loss: 3.0564, Loss_eval: 3.4943, Learning Rate: 5.000000e-05\n",
      "Step 41600, Loss: 3.3159, Loss_eval: 3.5297, Learning Rate: 5.000000e-05\n",
      "Step 41700, Loss: 3.3818, Loss_eval: 3.5015, Learning Rate: 5.000000e-05\n",
      "Step 41800, Loss: 3.3409, Loss_eval: 3.5156, Learning Rate: 5.000000e-05\n",
      "Step 41900, Loss: 3.3880, Loss_eval: 3.4983, Learning Rate: 5.000000e-05\n",
      "Step 42000, Loss: 3.4744, Loss_eval: 3.5266, Learning Rate: 5.000000e-05\n",
      "Step 42100, Loss: 3.1329, Loss_eval: 3.5091, Learning Rate: 5.000000e-05\n",
      "Step 42200, Loss: 3.3758, Loss_eval: 3.5286, Learning Rate: 5.000000e-05\n",
      "Step 42300, Loss: 3.0075, Loss_eval: 3.4946, Learning Rate: 5.000000e-05\n",
      "Step 42400, Loss: 3.0399, Loss_eval: 3.5304, Learning Rate: 5.000000e-05\n",
      "Step 42500, Loss: 3.2348, Loss_eval: 3.5079, Learning Rate: 5.000000e-05\n",
      "Step 42600, Loss: 3.1374, Loss_eval: 3.5357, Learning Rate: 5.000000e-05\n",
      "Step 42700, Loss: 3.1070, Loss_eval: 3.5116, Learning Rate: 5.000000e-05\n",
      "Step 42800, Loss: 3.2607, Loss_eval: 3.5056, Learning Rate: 5.000000e-05\n",
      "Step 42900, Loss: 3.0525, Loss_eval: 3.5357, Learning Rate: 5.000000e-05\n",
      "Step 43000, Loss: 3.3029, Loss_eval: 3.5026, Learning Rate: 5.000000e-05\n",
      "Step 43100, Loss: 2.9358, Loss_eval: 3.5176, Learning Rate: 5.000000e-05\n",
      "Step 43200, Loss: 3.1624, Loss_eval: 3.5148, Learning Rate: 5.000000e-05\n",
      "Step 43300, Loss: 3.3037, Loss_eval: 3.5126, Learning Rate: 5.000000e-05\n",
      "Step 43400, Loss: 3.5931, Loss_eval: 3.5210, Learning Rate: 5.000000e-05\n",
      "Step 43500, Loss: 2.9737, Loss_eval: 3.5186, Learning Rate: 5.000000e-05\n",
      "Step 43600, Loss: 3.5404, Loss_eval: 3.5387, Learning Rate: 5.000000e-05\n",
      "Step 43700, Loss: 3.0854, Loss_eval: 3.5079, Learning Rate: 5.000000e-05\n",
      "Step 43800, Loss: 3.3221, Loss_eval: 3.4919, Learning Rate: 5.000000e-05\n",
      "Step 43900, Loss: 3.3685, Loss_eval: 3.4958, Learning Rate: 5.000000e-05\n",
      "Step 44000, Loss: 3.1463, Loss_eval: 3.5252, Learning Rate: 5.000000e-05\n",
      "Step 44100, Loss: 3.4343, Loss_eval: 3.5375, Learning Rate: 5.000000e-05\n",
      "Step 44200, Loss: 3.2172, Loss_eval: 3.5126, Learning Rate: 5.000000e-05\n",
      "Step 44300, Loss: 3.1363, Loss_eval: 3.4941, Learning Rate: 5.000000e-05\n",
      "Step 44400, Loss: 3.3311, Loss_eval: 3.5291, Learning Rate: 5.000000e-05\n",
      "Step 44500, Loss: 3.3114, Loss_eval: 3.4988, Learning Rate: 5.000000e-05\n",
      "Step 44600, Loss: 3.4328, Loss_eval: 3.5143, Learning Rate: 5.000000e-05\n",
      "Step 44700, Loss: 2.9083, Loss_eval: 3.5214, Learning Rate: 5.000000e-05\n",
      "Step 44800, Loss: 3.0486, Loss_eval: 3.4899, Learning Rate: 5.000000e-05\n",
      "Step 44900, Loss: 3.1615, Loss_eval: 3.4898, Learning Rate: 5.000000e-05\n",
      "Step 45000, Loss: 3.1898, Loss_eval: 3.5477, Learning Rate: 5.000000e-05\n",
      "Step 45100, Loss: 3.4128, Loss_eval: 3.5124, Learning Rate: 5.000000e-05\n",
      "Step 45200, Loss: 3.3933, Loss_eval: 3.5164, Learning Rate: 5.000000e-05\n",
      "Step 45300, Loss: 3.6219, Loss_eval: 3.5303, Learning Rate: 5.000000e-05\n",
      "Step 45400, Loss: 3.4875, Loss_eval: 3.5203, Learning Rate: 5.000000e-05\n",
      "Step 45500, Loss: 2.9823, Loss_eval: 3.4873, Learning Rate: 5.000000e-05\n",
      "Step 45600, Loss: 3.4353, Loss_eval: 3.5001, Learning Rate: 5.000000e-05\n",
      "Step 45700, Loss: 3.4095, Loss_eval: 3.5171, Learning Rate: 5.000000e-05\n",
      "Step 45800, Loss: 2.9758, Loss_eval: 3.4668, Learning Rate: 5.000000e-05\n",
      "Step 45900, Loss: 3.1756, Loss_eval: 3.5184, Learning Rate: 5.000000e-05\n",
      "Step 46000, Loss: 2.9643, Loss_eval: 3.4904, Learning Rate: 5.000000e-05\n",
      "Step 46100, Loss: 3.4785, Loss_eval: 3.4653, Learning Rate: 5.000000e-05\n",
      "Step 46200, Loss: 3.1863, Loss_eval: 3.5243, Learning Rate: 5.000000e-05\n",
      "Step 46300, Loss: 3.5001, Loss_eval: 3.5228, Learning Rate: 5.000000e-05\n",
      "Step 46400, Loss: 3.4799, Loss_eval: 3.5698, Learning Rate: 5.000000e-05\n",
      "Step 46500, Loss: 3.3473, Loss_eval: 3.5186, Learning Rate: 5.000000e-05\n",
      "Step 46600, Loss: 3.1282, Loss_eval: 3.4871, Learning Rate: 5.000000e-05\n",
      "Step 46700, Loss: 2.8780, Loss_eval: 3.5042, Learning Rate: 5.000000e-05\n",
      "Step 46800, Loss: 3.5338, Loss_eval: 3.4942, Learning Rate: 5.000000e-05\n",
      "Step 46900, Loss: 3.3157, Loss_eval: 3.5136, Learning Rate: 5.000000e-05\n",
      "Step 47000, Loss: 3.2242, Loss_eval: 3.5093, Learning Rate: 5.000000e-05\n",
      "Step 47100, Loss: 2.9890, Loss_eval: 3.5447, Learning Rate: 5.000000e-05\n",
      "Step 47200, Loss: 2.8337, Loss_eval: 3.5002, Learning Rate: 5.000000e-05\n",
      "Step 47300, Loss: 3.1304, Loss_eval: 3.5076, Learning Rate: 5.000000e-05\n",
      "Step 47400, Loss: 3.0790, Loss_eval: 3.4909, Learning Rate: 5.000000e-05\n",
      "Step 47500, Loss: 3.0116, Loss_eval: 3.5234, Learning Rate: 5.000000e-05\n",
      "Step 47600, Loss: 3.2280, Loss_eval: 3.5310, Learning Rate: 5.000000e-05\n",
      "Step 47700, Loss: 3.6054, Loss_eval: 3.5392, Learning Rate: 5.000000e-05\n",
      "Step 47800, Loss: 3.4871, Loss_eval: 3.4935, Learning Rate: 5.000000e-05\n",
      "Step 47900, Loss: 3.0818, Loss_eval: 3.5073, Learning Rate: 5.000000e-05\n",
      "Step 48000, Loss: 3.2645, Loss_eval: 3.5029, Learning Rate: 5.000000e-05\n",
      "Step 48100, Loss: 3.2237, Loss_eval: 3.5442, Learning Rate: 5.000000e-05\n",
      "Step 48200, Loss: 3.1693, Loss_eval: 3.5007, Learning Rate: 5.000000e-05\n",
      "Step 48300, Loss: 3.0351, Loss_eval: 3.5046, Learning Rate: 5.000000e-05\n",
      "Step 48400, Loss: 3.3399, Loss_eval: 3.5168, Learning Rate: 5.000000e-05\n",
      "Step 48500, Loss: 3.0057, Loss_eval: 3.5087, Learning Rate: 5.000000e-05\n",
      "Step 48600, Loss: 3.3085, Loss_eval: 3.4985, Learning Rate: 5.000000e-05\n",
      "Step 48700, Loss: 3.1766, Loss_eval: 3.4944, Learning Rate: 5.000000e-05\n",
      "Step 48800, Loss: 3.2532, Loss_eval: 3.4892, Learning Rate: 5.000000e-05\n",
      "Step 48900, Loss: 3.1723, Loss_eval: 3.5282, Learning Rate: 5.000000e-05\n",
      "Step 49000, Loss: 3.1804, Loss_eval: 3.5131, Learning Rate: 5.000000e-05\n",
      "Step 49100, Loss: 3.1612, Loss_eval: 3.4596, Learning Rate: 5.000000e-05\n",
      "Step 49200, Loss: 3.1836, Loss_eval: 3.5357, Learning Rate: 5.000000e-05\n",
      "Step 49300, Loss: 3.1189, Loss_eval: 3.5015, Learning Rate: 5.000000e-05\n",
      "Step 49400, Loss: 3.4867, Loss_eval: 3.5040, Learning Rate: 5.000000e-05\n",
      "Step 49500, Loss: 2.9009, Loss_eval: 3.5023, Learning Rate: 5.000000e-05\n",
      "Step 49600, Loss: 3.2384, Loss_eval: 3.5007, Learning Rate: 5.000000e-05\n",
      "Step 49700, Loss: 3.2600, Loss_eval: 3.5112, Learning Rate: 5.000000e-05\n",
      "Step 49800, Loss: 3.1454, Loss_eval: 3.4993, Learning Rate: 5.000000e-05\n",
      "Step 49900, Loss: 3.2372, Loss_eval: 3.5127, Learning Rate: 5.000000e-05\n",
      "Step 50000, Loss: 3.1622, Loss_eval: 3.5131, Learning Rate: 5.000000e-05\n",
      "Step 50100, Loss: 3.2109, Loss_eval: 3.5181, Learning Rate: 5.000000e-05\n",
      "Step 50200, Loss: 3.3083, Loss_eval: 3.4964, Learning Rate: 5.000000e-05\n",
      "Step 50300, Loss: 3.2758, Loss_eval: 3.4786, Learning Rate: 5.000000e-05\n",
      "Step 50400, Loss: 3.2131, Loss_eval: 3.4811, Learning Rate: 5.000000e-05\n",
      "Step 50500, Loss: 2.9366, Loss_eval: 3.5176, Learning Rate: 5.000000e-05\n",
      "Step 50600, Loss: 3.0507, Loss_eval: 3.5215, Learning Rate: 5.000000e-05\n",
      "Step 50700, Loss: 3.0497, Loss_eval: 3.5286, Learning Rate: 5.000000e-05\n",
      "Step 50800, Loss: 3.0685, Loss_eval: 3.4626, Learning Rate: 5.000000e-05\n",
      "Step 50900, Loss: 3.0056, Loss_eval: 3.5257, Learning Rate: 5.000000e-05\n",
      "Step 51000, Loss: 3.5473, Loss_eval: 3.5102, Learning Rate: 5.000000e-05\n",
      "Step 51100, Loss: 3.0657, Loss_eval: 3.5315, Learning Rate: 5.000000e-05\n",
      "Step 51200, Loss: 3.1981, Loss_eval: 3.5442, Learning Rate: 5.000000e-05\n",
      "Step 51300, Loss: 3.0408, Loss_eval: 3.4367, Learning Rate: 5.000000e-05\n",
      "Step 51400, Loss: 3.0654, Loss_eval: 3.4998, Learning Rate: 5.000000e-05\n",
      "Step 51500, Loss: 2.9556, Loss_eval: 3.4922, Learning Rate: 5.000000e-05\n",
      "Step 51600, Loss: 2.9789, Loss_eval: 3.5040, Learning Rate: 5.000000e-05\n",
      "Step 51700, Loss: 3.0792, Loss_eval: 3.5060, Learning Rate: 5.000000e-05\n",
      "Step 51800, Loss: 3.4033, Loss_eval: 3.5091, Learning Rate: 5.000000e-05\n",
      "Step 51900, Loss: 3.1971, Loss_eval: 3.5149, Learning Rate: 5.000000e-05\n",
      "Step 52000, Loss: 3.2005, Loss_eval: 3.4963, Learning Rate: 5.000000e-05\n",
      "Step 52100, Loss: 3.5281, Loss_eval: 3.5046, Learning Rate: 5.000000e-05\n",
      "Step 52200, Loss: 3.3854, Loss_eval: 3.5481, Learning Rate: 5.000000e-05\n",
      "Step 52300, Loss: 3.2728, Loss_eval: 3.5311, Learning Rate: 5.000000e-05\n",
      "Step 52400, Loss: 3.5369, Loss_eval: 3.4888, Learning Rate: 5.000000e-05\n",
      "Step 52500, Loss: 3.4812, Loss_eval: 3.5028, Learning Rate: 5.000000e-05\n",
      "Step 52600, Loss: 3.6570, Loss_eval: 3.4899, Learning Rate: 5.000000e-05\n",
      "Step 52700, Loss: 3.0439, Loss_eval: 3.5261, Learning Rate: 5.000000e-05\n",
      "Step 52800, Loss: 3.2996, Loss_eval: 3.4934, Learning Rate: 5.000000e-05\n",
      "Step 52900, Loss: 3.1739, Loss_eval: 3.5204, Learning Rate: 5.000000e-05\n",
      "Step 53000, Loss: 3.1950, Loss_eval: 3.5067, Learning Rate: 5.000000e-05\n",
      "Step 53100, Loss: 3.2616, Loss_eval: 3.4781, Learning Rate: 5.000000e-05\n",
      "Step 53200, Loss: 3.3236, Loss_eval: 3.5038, Learning Rate: 5.000000e-05\n",
      "Step 53300, Loss: 3.3209, Loss_eval: 3.4910, Learning Rate: 5.000000e-05\n",
      "Step 53400, Loss: 3.2857, Loss_eval: 3.5535, Learning Rate: 5.000000e-05\n",
      "Step 53500, Loss: 3.0237, Loss_eval: 3.5069, Learning Rate: 5.000000e-05\n",
      "Step 53600, Loss: 3.2689, Loss_eval: 3.4911, Learning Rate: 5.000000e-05\n",
      "Step 53700, Loss: 3.2795, Loss_eval: 3.5447, Learning Rate: 5.000000e-05\n",
      "Step 53800, Loss: 3.2727, Loss_eval: 3.4998, Learning Rate: 5.000000e-05\n",
      "Step 53900, Loss: 3.2464, Loss_eval: 3.5019, Learning Rate: 5.000000e-05\n",
      "Step 54000, Loss: 3.2144, Loss_eval: 3.5218, Learning Rate: 5.000000e-05\n",
      "Step 54100, Loss: 3.0467, Loss_eval: 3.5073, Learning Rate: 5.000000e-05\n",
      "Step 54200, Loss: 3.1006, Loss_eval: 3.5124, Learning Rate: 5.000000e-05\n",
      "Step 54300, Loss: 3.0617, Loss_eval: 3.4939, Learning Rate: 5.000000e-05\n",
      "Step 54400, Loss: 3.3363, Loss_eval: 3.4992, Learning Rate: 5.000000e-05\n",
      "Step 54500, Loss: 2.8719, Loss_eval: 3.4660, Learning Rate: 5.000000e-05\n",
      "Step 54600, Loss: 3.3632, Loss_eval: 3.4954, Learning Rate: 5.000000e-05\n",
      "Step 54700, Loss: 3.4215, Loss_eval: 3.4860, Learning Rate: 5.000000e-05\n",
      "Step 54800, Loss: 3.1401, Loss_eval: 3.4985, Learning Rate: 5.000000e-05\n",
      "Step 54900, Loss: 3.2551, Loss_eval: 3.4892, Learning Rate: 5.000000e-05\n",
      "Step 55000, Loss: 3.2144, Loss_eval: 3.5216, Learning Rate: 5.000000e-05\n",
      "Step 55100, Loss: 3.2046, Loss_eval: 3.4899, Learning Rate: 5.000000e-05\n",
      "Step 55200, Loss: 3.2367, Loss_eval: 3.5179, Learning Rate: 5.000000e-05\n",
      "Step 55300, Loss: 3.0576, Loss_eval: 3.5003, Learning Rate: 5.000000e-05\n",
      "Step 55400, Loss: 3.3398, Loss_eval: 3.5232, Learning Rate: 5.000000e-05\n",
      "Step 55500, Loss: 3.2292, Loss_eval: 3.5165, Learning Rate: 5.000000e-05\n",
      "Step 55600, Loss: 3.0884, Loss_eval: 3.5237, Learning Rate: 5.000000e-05\n",
      "Step 55700, Loss: 3.4578, Loss_eval: 3.4921, Learning Rate: 5.000000e-05\n",
      "Step 55800, Loss: 3.4157, Loss_eval: 3.5085, Learning Rate: 5.000000e-05\n",
      "Step 55900, Loss: 3.0470, Loss_eval: 3.5265, Learning Rate: 5.000000e-05\n",
      "Step 56000, Loss: 3.2684, Loss_eval: 3.5221, Learning Rate: 5.000000e-05\n",
      "Step 56100, Loss: 3.4238, Loss_eval: 3.5031, Learning Rate: 5.000000e-05\n",
      "Step 56200, Loss: 2.9713, Loss_eval: 3.4773, Learning Rate: 5.000000e-05\n",
      "Step 56300, Loss: 3.2938, Loss_eval: 3.4979, Learning Rate: 5.000000e-05\n",
      "Step 56400, Loss: 3.0722, Loss_eval: 3.5448, Learning Rate: 5.000000e-05\n",
      "Step 56500, Loss: 2.8338, Loss_eval: 3.5346, Learning Rate: 5.000000e-05\n",
      "Step 56600, Loss: 2.8323, Loss_eval: 3.5392, Learning Rate: 5.000000e-05\n",
      "Step 56700, Loss: 2.6116, Loss_eval: 3.4991, Learning Rate: 5.000000e-05\n",
      "Step 56800, Loss: 3.2593, Loss_eval: 3.5140, Learning Rate: 5.000000e-05\n",
      "Step 56900, Loss: 3.0225, Loss_eval: 3.5379, Learning Rate: 5.000000e-05\n",
      "Step 57000, Loss: 3.0850, Loss_eval: 3.4897, Learning Rate: 5.000000e-05\n",
      "Step 57100, Loss: 3.3068, Loss_eval: 3.4828, Learning Rate: 5.000000e-05\n",
      "Step 57200, Loss: 3.2041, Loss_eval: 3.4862, Learning Rate: 5.000000e-05\n",
      "Step 57300, Loss: 3.3513, Loss_eval: 3.5047, Learning Rate: 5.000000e-05\n",
      "Step 57400, Loss: 3.1553, Loss_eval: 3.4878, Learning Rate: 5.000000e-05\n",
      "Step 57500, Loss: 2.9913, Loss_eval: 3.5262, Learning Rate: 5.000000e-05\n",
      "Step 57600, Loss: 3.3135, Loss_eval: 3.4793, Learning Rate: 5.000000e-05\n",
      "Step 57700, Loss: 3.1322, Loss_eval: 3.5084, Learning Rate: 5.000000e-05\n",
      "Step 57800, Loss: 3.1590, Loss_eval: 3.4827, Learning Rate: 5.000000e-05\n",
      "Step 57900, Loss: 3.4439, Loss_eval: 3.4813, Learning Rate: 5.000000e-05\n",
      "Step 58000, Loss: 3.2289, Loss_eval: 3.4940, Learning Rate: 5.000000e-05\n",
      "Step 58100, Loss: 3.2869, Loss_eval: 3.5258, Learning Rate: 5.000000e-05\n",
      "Step 58200, Loss: 3.0825, Loss_eval: 3.5284, Learning Rate: 5.000000e-05\n",
      "Step 58300, Loss: 3.1382, Loss_eval: 3.5245, Learning Rate: 5.000000e-05\n",
      "Step 58400, Loss: 2.9492, Loss_eval: 3.4945, Learning Rate: 5.000000e-05\n",
      "Step 58500, Loss: 2.9902, Loss_eval: 3.5014, Learning Rate: 5.000000e-05\n",
      "Step 58600, Loss: 3.3787, Loss_eval: 3.5221, Learning Rate: 5.000000e-05\n",
      "Step 58700, Loss: 3.3939, Loss_eval: 3.5326, Learning Rate: 5.000000e-05\n",
      "Step 58800, Loss: 3.2426, Loss_eval: 3.4737, Learning Rate: 5.000000e-05\n",
      "Step 58900, Loss: 3.4027, Loss_eval: 3.5176, Learning Rate: 5.000000e-05\n",
      "Step 59000, Loss: 3.0620, Loss_eval: 3.5157, Learning Rate: 5.000000e-05\n",
      "Step 59100, Loss: 3.2313, Loss_eval: 3.5117, Learning Rate: 5.000000e-05\n",
      "Step 59200, Loss: 3.2995, Loss_eval: 3.4797, Learning Rate: 5.000000e-05\n",
      "Step 59300, Loss: 3.3020, Loss_eval: 3.4869, Learning Rate: 5.000000e-05\n",
      "Step 59400, Loss: 3.4569, Loss_eval: 3.4688, Learning Rate: 5.000000e-05\n",
      "Step 59500, Loss: 3.1327, Loss_eval: 3.5034, Learning Rate: 5.000000e-05\n",
      "Step 59600, Loss: 2.9728, Loss_eval: 3.5056, Learning Rate: 5.000000e-05\n",
      "Step 59700, Loss: 3.1043, Loss_eval: 3.4898, Learning Rate: 5.000000e-05\n",
      "Step 59800, Loss: 3.1299, Loss_eval: 3.4628, Learning Rate: 5.000000e-05\n",
      "Step 59900, Loss: 3.1461, Loss_eval: 3.4915, Learning Rate: 5.000000e-05\n",
      "Step 60000, Loss: 2.9394, Loss_eval: 3.5124, Learning Rate: 5.000000e-05\n",
      "Step 60100, Loss: 3.6301, Loss_eval: 3.4917, Learning Rate: 5.000000e-05\n",
      "Step 60200, Loss: 3.3569, Loss_eval: 3.5188, Learning Rate: 5.000000e-05\n",
      "Step 60300, Loss: 3.1574, Loss_eval: 3.5139, Learning Rate: 5.000000e-05\n",
      "Step 60400, Loss: 3.1586, Loss_eval: 3.5237, Learning Rate: 5.000000e-05\n",
      "Step 60500, Loss: 3.3488, Loss_eval: 3.5000, Learning Rate: 5.000000e-05\n",
      "Step 60600, Loss: 3.2085, Loss_eval: 3.4956, Learning Rate: 5.000000e-05\n",
      "Step 60700, Loss: 3.3585, Loss_eval: 3.5184, Learning Rate: 5.000000e-05\n",
      "Step 60800, Loss: 3.3940, Loss_eval: 3.5309, Learning Rate: 5.000000e-05\n",
      "Step 60900, Loss: 3.1022, Loss_eval: 3.4967, Learning Rate: 5.000000e-05\n",
      "Step 61000, Loss: 3.3038, Loss_eval: 3.5001, Learning Rate: 5.000000e-05\n",
      "Step 61100, Loss: 3.0821, Loss_eval: 3.5085, Learning Rate: 5.000000e-05\n",
      "Step 61200, Loss: 3.4096, Loss_eval: 3.5004, Learning Rate: 5.000000e-05\n",
      "Step 61300, Loss: 3.1630, Loss_eval: 3.4980, Learning Rate: 5.000000e-05\n",
      "Step 61400, Loss: 2.9109, Loss_eval: 3.4972, Learning Rate: 5.000000e-05\n",
      "Step 61500, Loss: 3.1187, Loss_eval: 3.4719, Learning Rate: 5.000000e-05\n",
      "Step 61600, Loss: 3.4003, Loss_eval: 3.5060, Learning Rate: 5.000000e-05\n",
      "Step 61700, Loss: 3.2369, Loss_eval: 3.5054, Learning Rate: 5.000000e-05\n",
      "Step 61800, Loss: 2.9464, Loss_eval: 3.5085, Learning Rate: 5.000000e-05\n",
      "Step 61900, Loss: 3.2727, Loss_eval: 3.4995, Learning Rate: 5.000000e-05\n",
      "Step 62000, Loss: 3.1966, Loss_eval: 3.4946, Learning Rate: 5.000000e-05\n",
      "Step 62100, Loss: 3.1941, Loss_eval: 3.4850, Learning Rate: 5.000000e-05\n",
      "Step 62200, Loss: 3.1406, Loss_eval: 3.5017, Learning Rate: 5.000000e-05\n",
      "Step 62300, Loss: 2.9931, Loss_eval: 3.4558, Learning Rate: 5.000000e-05\n",
      "Step 62400, Loss: 3.2595, Loss_eval: 3.4977, Learning Rate: 5.000000e-05\n",
      "Step 62500, Loss: 3.2850, Loss_eval: 3.5359, Learning Rate: 5.000000e-05\n",
      "Step 62600, Loss: 3.4884, Loss_eval: 3.4917, Learning Rate: 5.000000e-05\n",
      "Step 62700, Loss: 3.5160, Loss_eval: 3.4808, Learning Rate: 5.000000e-05\n",
      "Step 62800, Loss: 3.0057, Loss_eval: 3.4893, Learning Rate: 5.000000e-05\n",
      "Step 62900, Loss: 3.1064, Loss_eval: 3.5121, Learning Rate: 5.000000e-05\n",
      "Step 63000, Loss: 3.2298, Loss_eval: 3.5196, Learning Rate: 5.000000e-05\n",
      "Step 63100, Loss: 3.5481, Loss_eval: 3.4729, Learning Rate: 5.000000e-05\n",
      "Step 63200, Loss: 3.1155, Loss_eval: 3.4939, Learning Rate: 5.000000e-05\n",
      "Step 63300, Loss: 3.0849, Loss_eval: 3.4891, Learning Rate: 5.000000e-05\n",
      "Step 63400, Loss: 2.9126, Loss_eval: 3.5157, Learning Rate: 5.000000e-05\n",
      "Step 63500, Loss: 3.5397, Loss_eval: 3.4873, Learning Rate: 5.000000e-05\n",
      "Step 63600, Loss: 3.2853, Loss_eval: 3.4858, Learning Rate: 5.000000e-05\n",
      "Step 63700, Loss: 2.8905, Loss_eval: 3.5014, Learning Rate: 5.000000e-05\n",
      "Step 63800, Loss: 3.4411, Loss_eval: 3.5042, Learning Rate: 5.000000e-05\n",
      "Step 63900, Loss: 3.2816, Loss_eval: 3.5009, Learning Rate: 5.000000e-05\n",
      "Step 64000, Loss: 3.1936, Loss_eval: 3.5155, Learning Rate: 5.000000e-05\n",
      "Step 64100, Loss: 2.9118, Loss_eval: 3.4982, Learning Rate: 5.000000e-05\n",
      "Step 64200, Loss: 3.1209, Loss_eval: 3.5174, Learning Rate: 5.000000e-05\n",
      "Step 64300, Loss: 3.2985, Loss_eval: 3.5459, Learning Rate: 5.000000e-05\n",
      "Step 64400, Loss: 3.3391, Loss_eval: 3.5031, Learning Rate: 5.000000e-05\n",
      "Step 64500, Loss: 3.1902, Loss_eval: 3.5020, Learning Rate: 5.000000e-05\n",
      "Step 64600, Loss: 3.2795, Loss_eval: 3.5281, Learning Rate: 5.000000e-05\n",
      "Step 64700, Loss: 3.1753, Loss_eval: 3.5373, Learning Rate: 5.000000e-05\n",
      "Step 64800, Loss: 3.1188, Loss_eval: 3.5074, Learning Rate: 5.000000e-05\n",
      "Step 64900, Loss: 2.9441, Loss_eval: 3.4728, Learning Rate: 5.000000e-05\n",
      "Step 65000, Loss: 3.0544, Loss_eval: 3.5030, Learning Rate: 5.000000e-05\n",
      "Step 65100, Loss: 3.1757, Loss_eval: 3.5021, Learning Rate: 5.000000e-05\n",
      "Step 65200, Loss: 3.3271, Loss_eval: 3.5074, Learning Rate: 5.000000e-05\n",
      "Step 65300, Loss: 3.3822, Loss_eval: 3.4660, Learning Rate: 5.000000e-05\n",
      "Step 65400, Loss: 3.2127, Loss_eval: 3.4924, Learning Rate: 5.000000e-05\n",
      "Step 65500, Loss: 3.1208, Loss_eval: 3.4859, Learning Rate: 5.000000e-05\n",
      "Step 65600, Loss: 3.1401, Loss_eval: 3.4970, Learning Rate: 5.000000e-05\n",
      "Step 65700, Loss: 3.3293, Loss_eval: 3.4984, Learning Rate: 5.000000e-05\n",
      "Step 65800, Loss: 3.5981, Loss_eval: 3.4799, Learning Rate: 5.000000e-05\n",
      "Step 65900, Loss: 3.2207, Loss_eval: 3.5253, Learning Rate: 5.000000e-05\n",
      "Step 66000, Loss: 3.3850, Loss_eval: 3.4971, Learning Rate: 5.000000e-05\n",
      "Step 66100, Loss: 3.2081, Loss_eval: 3.5336, Learning Rate: 5.000000e-05\n",
      "Step 66200, Loss: 3.1842, Loss_eval: 3.4887, Learning Rate: 5.000000e-05\n",
      "Step 66300, Loss: 3.4052, Loss_eval: 3.5105, Learning Rate: 5.000000e-05\n",
      "Step 66400, Loss: 3.1043, Loss_eval: 3.5270, Learning Rate: 5.000000e-05\n",
      "Step 66500, Loss: 3.1442, Loss_eval: 3.4957, Learning Rate: 5.000000e-05\n",
      "Step 66600, Loss: 3.3255, Loss_eval: 3.4802, Learning Rate: 5.000000e-05\n",
      "Step 66700, Loss: 3.3907, Loss_eval: 3.4745, Learning Rate: 5.000000e-05\n",
      "Step 66800, Loss: 2.9575, Loss_eval: 3.5168, Learning Rate: 5.000000e-05\n",
      "Step 66900, Loss: 3.2640, Loss_eval: 3.4788, Learning Rate: 5.000000e-05\n",
      "Step 67000, Loss: 3.0875, Loss_eval: 3.4681, Learning Rate: 5.000000e-05\n",
      "Step 67100, Loss: 3.1839, Loss_eval: 3.4994, Learning Rate: 5.000000e-05\n",
      "Step 67200, Loss: 3.1133, Loss_eval: 3.4935, Learning Rate: 5.000000e-05\n",
      "Step 67300, Loss: 2.9332, Loss_eval: 3.4958, Learning Rate: 5.000000e-05\n",
      "Step 67400, Loss: 3.1999, Loss_eval: 3.4827, Learning Rate: 5.000000e-05\n",
      "Step 67500, Loss: 3.1957, Loss_eval: 3.5161, Learning Rate: 5.000000e-05\n",
      "Step 67600, Loss: 3.1108, Loss_eval: 3.4921, Learning Rate: 5.000000e-05\n",
      "Step 67700, Loss: 3.1601, Loss_eval: 3.4946, Learning Rate: 5.000000e-05\n",
      "Step 67800, Loss: 3.3207, Loss_eval: 3.4832, Learning Rate: 5.000000e-05\n",
      "Step 67900, Loss: 3.1115, Loss_eval: 3.4949, Learning Rate: 5.000000e-05\n",
      "Step 68000, Loss: 3.0978, Loss_eval: 3.5158, Learning Rate: 5.000000e-05\n",
      "Step 68100, Loss: 3.2051, Loss_eval: 3.4988, Learning Rate: 5.000000e-05\n",
      "Step 68200, Loss: 3.3731, Loss_eval: 3.4975, Learning Rate: 5.000000e-05\n",
      "Step 68300, Loss: 3.0313, Loss_eval: 3.4806, Learning Rate: 5.000000e-05\n",
      "Step 68400, Loss: 3.1733, Loss_eval: 3.5039, Learning Rate: 5.000000e-05\n",
      "Step 68500, Loss: 3.0915, Loss_eval: 3.5026, Learning Rate: 5.000000e-05\n",
      "Step 68600, Loss: 3.3763, Loss_eval: 3.4798, Learning Rate: 5.000000e-05\n",
      "Step 68700, Loss: 3.2268, Loss_eval: 3.5497, Learning Rate: 5.000000e-05\n",
      "Step 68800, Loss: 3.1406, Loss_eval: 3.5205, Learning Rate: 5.000000e-05\n",
      "Step 68900, Loss: 3.2027, Loss_eval: 3.5070, Learning Rate: 5.000000e-05\n",
      "Step 69000, Loss: 3.6047, Loss_eval: 3.4949, Learning Rate: 5.000000e-05\n",
      "Step 69100, Loss: 2.9281, Loss_eval: 3.4957, Learning Rate: 5.000000e-05\n",
      "Step 69200, Loss: 2.7327, Loss_eval: 3.4825, Learning Rate: 5.000000e-05\n",
      "Step 69300, Loss: 2.9630, Loss_eval: 3.5170, Learning Rate: 5.000000e-05\n",
      "Step 69400, Loss: 2.9528, Loss_eval: 3.5170, Learning Rate: 5.000000e-05\n",
      "Step 69500, Loss: 3.1939, Loss_eval: 3.4860, Learning Rate: 5.000000e-05\n",
      "Step 69600, Loss: 3.3567, Loss_eval: 3.5268, Learning Rate: 5.000000e-05\n",
      "Step 69700, Loss: 3.3273, Loss_eval: 3.5124, Learning Rate: 5.000000e-05\n",
      "Step 69800, Loss: 3.2007, Loss_eval: 3.4638, Learning Rate: 5.000000e-05\n",
      "Step 69900, Loss: 3.3475, Loss_eval: 3.4756, Learning Rate: 5.000000e-05\n",
      "Step 70000, Loss: 3.0260, Loss_eval: 3.4990, Learning Rate: 5.000000e-05\n",
      "Step 70100, Loss: 3.1650, Loss_eval: 3.5208, Learning Rate: 5.000000e-05\n",
      "Step 70200, Loss: 3.3183, Loss_eval: 3.5027, Learning Rate: 5.000000e-05\n",
      "Step 70300, Loss: 3.1163, Loss_eval: 3.5102, Learning Rate: 5.000000e-05\n",
      "Step 70400, Loss: 3.0380, Loss_eval: 3.4858, Learning Rate: 5.000000e-05\n",
      "Step 70500, Loss: 3.1751, Loss_eval: 3.4997, Learning Rate: 5.000000e-05\n",
      "Step 70600, Loss: 3.5520, Loss_eval: 3.4923, Learning Rate: 5.000000e-05\n",
      "Step 70700, Loss: 3.0858, Loss_eval: 3.4597, Learning Rate: 5.000000e-05\n",
      "Step 70800, Loss: 3.1105, Loss_eval: 3.5323, Learning Rate: 5.000000e-05\n",
      "Step 70900, Loss: 3.1076, Loss_eval: 3.4983, Learning Rate: 5.000000e-05\n",
      "Step 71000, Loss: 3.1265, Loss_eval: 3.4650, Learning Rate: 5.000000e-05\n",
      "Step 71100, Loss: 3.4403, Loss_eval: 3.4807, Learning Rate: 5.000000e-05\n",
      "Step 71200, Loss: 3.1572, Loss_eval: 3.4921, Learning Rate: 5.000000e-05\n",
      "Step 71300, Loss: 3.4588, Loss_eval: 3.4943, Learning Rate: 5.000000e-05\n",
      "Step 71400, Loss: 3.3276, Loss_eval: 3.4910, Learning Rate: 5.000000e-05\n",
      "Step 71500, Loss: 3.2291, Loss_eval: 3.5077, Learning Rate: 5.000000e-05\n",
      "Step 71600, Loss: 3.3853, Loss_eval: 3.4968, Learning Rate: 5.000000e-05\n",
      "Step 71700, Loss: 3.2178, Loss_eval: 3.5018, Learning Rate: 5.000000e-05\n",
      "Step 71800, Loss: 3.5098, Loss_eval: 3.5072, Learning Rate: 5.000000e-05\n",
      "Step 71900, Loss: 3.1582, Loss_eval: 3.5140, Learning Rate: 5.000000e-05\n",
      "Step 72000, Loss: 2.9336, Loss_eval: 3.4843, Learning Rate: 5.000000e-05\n",
      "Step 72100, Loss: 3.2534, Loss_eval: 3.4833, Learning Rate: 5.000000e-05\n",
      "Step 72200, Loss: 3.2588, Loss_eval: 3.4833, Learning Rate: 5.000000e-05\n",
      "Step 72300, Loss: 3.3132, Loss_eval: 3.4924, Learning Rate: 5.000000e-05\n",
      "Step 72400, Loss: 3.4482, Loss_eval: 3.5019, Learning Rate: 5.000000e-05\n",
      "Step 72500, Loss: 3.1574, Loss_eval: 3.4928, Learning Rate: 5.000000e-05\n",
      "Step 72600, Loss: 3.5247, Loss_eval: 3.4802, Learning Rate: 5.000000e-05\n",
      "Step 72700, Loss: 3.2901, Loss_eval: 3.4946, Learning Rate: 5.000000e-05\n",
      "Step 72800, Loss: 3.1662, Loss_eval: 3.4870, Learning Rate: 5.000000e-05\n",
      "Step 72900, Loss: 3.4060, Loss_eval: 3.4968, Learning Rate: 5.000000e-05\n",
      "Step 73000, Loss: 3.3271, Loss_eval: 3.4939, Learning Rate: 5.000000e-05\n",
      "Step 73100, Loss: 3.0489, Loss_eval: 3.5226, Learning Rate: 5.000000e-05\n",
      "Step 73200, Loss: 3.2999, Loss_eval: 3.5420, Learning Rate: 5.000000e-05\n",
      "Step 73300, Loss: 3.3202, Loss_eval: 3.4692, Learning Rate: 5.000000e-05\n",
      "Step 73400, Loss: 3.3051, Loss_eval: 3.4847, Learning Rate: 5.000000e-05\n",
      "Step 73500, Loss: 3.0059, Loss_eval: 3.4786, Learning Rate: 5.000000e-05\n",
      "Step 73600, Loss: 3.3576, Loss_eval: 3.5071, Learning Rate: 5.000000e-05\n",
      "Step 73700, Loss: 3.3031, Loss_eval: 3.4938, Learning Rate: 5.000000e-05\n",
      "Step 73800, Loss: 3.3659, Loss_eval: 3.4847, Learning Rate: 5.000000e-05\n",
      "Step 73900, Loss: 3.0020, Loss_eval: 3.4841, Learning Rate: 5.000000e-05\n",
      "Step 74000, Loss: 3.3377, Loss_eval: 3.4908, Learning Rate: 5.000000e-05\n",
      "Step 74100, Loss: 3.1606, Loss_eval: 3.4946, Learning Rate: 5.000000e-05\n",
      "Step 74200, Loss: 3.3447, Loss_eval: 3.4901, Learning Rate: 5.000000e-05\n",
      "Step 74300, Loss: 3.2745, Loss_eval: 3.5144, Learning Rate: 5.000000e-05\n",
      "Step 74400, Loss: 3.0935, Loss_eval: 3.4800, Learning Rate: 5.000000e-05\n",
      "Step 74500, Loss: 3.3541, Loss_eval: 3.5131, Learning Rate: 5.000000e-05\n",
      "Step 74600, Loss: 3.3238, Loss_eval: 3.4979, Learning Rate: 5.000000e-05\n",
      "Step 74700, Loss: 3.0787, Loss_eval: 3.4926, Learning Rate: 5.000000e-05\n",
      "Step 74800, Loss: 2.9762, Loss_eval: 3.4812, Learning Rate: 5.000000e-05\n",
      "Step 74900, Loss: 3.0486, Loss_eval: 3.4947, Learning Rate: 5.000000e-05\n",
      "Step 75000, Loss: 3.0903, Loss_eval: 3.4983, Learning Rate: 5.000000e-05\n",
      "Step 75100, Loss: 3.2473, Loss_eval: 3.4545, Learning Rate: 5.000000e-05\n",
      "Step 75200, Loss: 3.1893, Loss_eval: 3.5055, Learning Rate: 5.000000e-05\n",
      "Step 75300, Loss: 3.5368, Loss_eval: 3.4741, Learning Rate: 5.000000e-05\n",
      "Step 75400, Loss: 3.0232, Loss_eval: 3.5019, Learning Rate: 5.000000e-05\n",
      "Step 75500, Loss: 3.1888, Loss_eval: 3.5095, Learning Rate: 5.000000e-05\n",
      "Step 75600, Loss: 3.1497, Loss_eval: 3.5044, Learning Rate: 5.000000e-05\n",
      "Step 75700, Loss: 3.2989, Loss_eval: 3.5062, Learning Rate: 5.000000e-05\n",
      "Step 75800, Loss: 3.1731, Loss_eval: 3.4855, Learning Rate: 5.000000e-05\n",
      "Step 75900, Loss: 3.0461, Loss_eval: 3.5026, Learning Rate: 5.000000e-05\n",
      "Step 76000, Loss: 2.8875, Loss_eval: 3.4954, Learning Rate: 5.000000e-05\n",
      "Step 76100, Loss: 3.1327, Loss_eval: 3.5000, Learning Rate: 5.000000e-05\n",
      "Step 76200, Loss: 3.4545, Loss_eval: 3.5267, Learning Rate: 5.000000e-05\n",
      "Step 76300, Loss: 3.4946, Loss_eval: 3.4939, Learning Rate: 5.000000e-05\n",
      "Step 76400, Loss: 3.2873, Loss_eval: 3.4796, Learning Rate: 5.000000e-05\n",
      "Step 76500, Loss: 3.4974, Loss_eval: 3.4734, Learning Rate: 5.000000e-05\n",
      "Step 76600, Loss: 3.3319, Loss_eval: 3.4850, Learning Rate: 5.000000e-05\n",
      "Step 76700, Loss: 3.4113, Loss_eval: 3.5059, Learning Rate: 5.000000e-05\n",
      "Step 76800, Loss: 3.0146, Loss_eval: 3.4852, Learning Rate: 5.000000e-05\n",
      "Step 76900, Loss: 3.6090, Loss_eval: 3.5131, Learning Rate: 5.000000e-05\n",
      "Step 77000, Loss: 3.4399, Loss_eval: 3.4819, Learning Rate: 5.000000e-05\n",
      "Step 77100, Loss: 3.2704, Loss_eval: 3.4883, Learning Rate: 5.000000e-05\n",
      "Step 77200, Loss: 2.5224, Loss_eval: 3.4636, Learning Rate: 5.000000e-05\n",
      "Step 77300, Loss: 3.0078, Loss_eval: 3.4660, Learning Rate: 5.000000e-05\n",
      "Step 77400, Loss: 3.0309, Loss_eval: 3.5080, Learning Rate: 5.000000e-05\n",
      "Step 77500, Loss: 3.6186, Loss_eval: 3.4882, Learning Rate: 5.000000e-05\n",
      "Step 77600, Loss: 3.0587, Loss_eval: 3.4737, Learning Rate: 5.000000e-05\n",
      "Step 77700, Loss: 3.1198, Loss_eval: 3.4888, Learning Rate: 5.000000e-05\n",
      "Step 77800, Loss: 3.3179, Loss_eval: 3.5005, Learning Rate: 5.000000e-05\n",
      "Step 77900, Loss: 2.9311, Loss_eval: 3.4626, Learning Rate: 5.000000e-05\n",
      "Step 78000, Loss: 3.1669, Loss_eval: 3.5305, Learning Rate: 5.000000e-05\n",
      "Step 78100, Loss: 3.0794, Loss_eval: 3.5129, Learning Rate: 5.000000e-05\n",
      "Step 78200, Loss: 3.2343, Loss_eval: 3.4902, Learning Rate: 5.000000e-05\n",
      "Step 78300, Loss: 3.2032, Loss_eval: 3.4687, Learning Rate: 5.000000e-05\n",
      "Step 78400, Loss: 3.4543, Loss_eval: 3.4817, Learning Rate: 5.000000e-05\n",
      "Step 78500, Loss: 3.3204, Loss_eval: 3.4854, Learning Rate: 5.000000e-05\n",
      "Step 78600, Loss: 3.2748, Loss_eval: 3.5248, Learning Rate: 5.000000e-05\n",
      "Step 78700, Loss: 3.1125, Loss_eval: 3.4932, Learning Rate: 5.000000e-05\n",
      "Step 78800, Loss: 3.0180, Loss_eval: 3.5222, Learning Rate: 5.000000e-05\n",
      "Step 78900, Loss: 3.2072, Loss_eval: 3.5015, Learning Rate: 5.000000e-05\n",
      "Step 79000, Loss: 2.9084, Loss_eval: 3.4985, Learning Rate: 5.000000e-05\n",
      "Step 79100, Loss: 3.0081, Loss_eval: 3.5265, Learning Rate: 5.000000e-05\n",
      "Step 79200, Loss: 3.1609, Loss_eval: 3.5068, Learning Rate: 5.000000e-05\n",
      "Step 79300, Loss: 3.3155, Loss_eval: 3.4858, Learning Rate: 5.000000e-05\n",
      "Step 79400, Loss: 3.3767, Loss_eval: 3.4835, Learning Rate: 5.000000e-05\n",
      "Step 79500, Loss: 3.7575, Loss_eval: 3.5023, Learning Rate: 5.000000e-05\n",
      "Step 79600, Loss: 3.3325, Loss_eval: 3.4608, Learning Rate: 5.000000e-05\n",
      "Step 79700, Loss: 2.9765, Loss_eval: 3.5259, Learning Rate: 5.000000e-05\n",
      "Step 79800, Loss: 3.2370, Loss_eval: 3.4814, Learning Rate: 5.000000e-05\n",
      "Step 79900, Loss: 3.3238, Loss_eval: 3.5195, Learning Rate: 5.000000e-05\n",
      "Step 80000, Loss: 3.4725, Loss_eval: 3.4647, Learning Rate: 5.000000e-05\n",
      "Step 80100, Loss: 3.1110, Loss_eval: 3.4814, Learning Rate: 5.000000e-05\n",
      "Step 80200, Loss: 3.2587, Loss_eval: 3.4760, Learning Rate: 5.000000e-05\n",
      "Step 80300, Loss: 3.1166, Loss_eval: 3.4620, Learning Rate: 5.000000e-05\n",
      "Step 80400, Loss: 3.0623, Loss_eval: 3.5063, Learning Rate: 5.000000e-05\n",
      "Step 80500, Loss: 3.1195, Loss_eval: 3.5260, Learning Rate: 5.000000e-05\n",
      "Step 80600, Loss: 3.0240, Loss_eval: 3.5073, Learning Rate: 5.000000e-05\n",
      "Step 80700, Loss: 3.0970, Loss_eval: 3.4873, Learning Rate: 5.000000e-05\n",
      "Step 80800, Loss: 3.4731, Loss_eval: 3.4815, Learning Rate: 5.000000e-05\n",
      "Step 80900, Loss: 2.6947, Loss_eval: 3.4958, Learning Rate: 5.000000e-05\n",
      "Step 81000, Loss: 3.3597, Loss_eval: 3.5141, Learning Rate: 5.000000e-05\n",
      "Step 81100, Loss: 3.1163, Loss_eval: 3.4939, Learning Rate: 5.000000e-05\n",
      "Step 81200, Loss: 3.4026, Loss_eval: 3.5188, Learning Rate: 5.000000e-05\n",
      "Step 81300, Loss: 3.0585, Loss_eval: 3.4696, Learning Rate: 5.000000e-05\n",
      "Step 81400, Loss: 3.3055, Loss_eval: 3.4786, Learning Rate: 5.000000e-05\n",
      "Step 81500, Loss: 3.6481, Loss_eval: 3.5063, Learning Rate: 5.000000e-05\n",
      "Step 81600, Loss: 3.2312, Loss_eval: 3.5116, Learning Rate: 5.000000e-05\n",
      "Step 81700, Loss: 3.3224, Loss_eval: 3.5045, Learning Rate: 5.000000e-05\n",
      "Step 81800, Loss: 3.6563, Loss_eval: 3.4828, Learning Rate: 5.000000e-05\n",
      "Step 81900, Loss: 3.1780, Loss_eval: 3.4926, Learning Rate: 5.000000e-05\n",
      "Step 82000, Loss: 3.1405, Loss_eval: 3.5137, Learning Rate: 5.000000e-05\n",
      "Step 82100, Loss: 3.0746, Loss_eval: 3.5158, Learning Rate: 5.000000e-05\n",
      "Step 82200, Loss: 3.3141, Loss_eval: 3.4908, Learning Rate: 5.000000e-05\n",
      "Step 82300, Loss: 3.0384, Loss_eval: 3.5101, Learning Rate: 5.000000e-05\n",
      "Step 82400, Loss: 3.4263, Loss_eval: 3.4854, Learning Rate: 5.000000e-05\n",
      "Step 82500, Loss: 3.1841, Loss_eval: 3.4879, Learning Rate: 5.000000e-05\n",
      "Step 82600, Loss: 3.3595, Loss_eval: 3.5087, Learning Rate: 5.000000e-05\n",
      "Step 82700, Loss: 3.2593, Loss_eval: 3.4782, Learning Rate: 5.000000e-05\n",
      "Step 82800, Loss: 3.2395, Loss_eval: 3.5419, Learning Rate: 5.000000e-05\n",
      "Step 82900, Loss: 3.2973, Loss_eval: 3.4766, Learning Rate: 5.000000e-05\n",
      "Step 83000, Loss: 3.4855, Loss_eval: 3.4588, Learning Rate: 5.000000e-05\n",
      "Step 83100, Loss: 3.5084, Loss_eval: 3.4797, Learning Rate: 5.000000e-05\n",
      "Step 83200, Loss: 2.9749, Loss_eval: 3.4863, Learning Rate: 5.000000e-05\n",
      "Step 83300, Loss: 2.9383, Loss_eval: 3.4640, Learning Rate: 5.000000e-05\n",
      "Step 83400, Loss: 3.2549, Loss_eval: 3.5320, Learning Rate: 5.000000e-05\n",
      "Step 83500, Loss: 3.0772, Loss_eval: 3.4929, Learning Rate: 5.000000e-05\n",
      "Step 83600, Loss: 3.0841, Loss_eval: 3.4996, Learning Rate: 5.000000e-05\n",
      "Step 83700, Loss: 2.7958, Loss_eval: 3.5138, Learning Rate: 5.000000e-05\n",
      "Step 83800, Loss: 2.7650, Loss_eval: 3.4629, Learning Rate: 5.000000e-05\n",
      "Step 83900, Loss: 3.4675, Loss_eval: 3.5114, Learning Rate: 5.000000e-05\n",
      "Step 84000, Loss: 2.9594, Loss_eval: 3.4708, Learning Rate: 5.000000e-05\n",
      "Step 84100, Loss: 3.1627, Loss_eval: 3.4823, Learning Rate: 5.000000e-05\n",
      "Step 84200, Loss: 2.9526, Loss_eval: 3.4942, Learning Rate: 5.000000e-05\n",
      "Step 84300, Loss: 3.3069, Loss_eval: 3.4874, Learning Rate: 5.000000e-05\n",
      "Step 84400, Loss: 3.1711, Loss_eval: 3.4824, Learning Rate: 5.000000e-05\n",
      "Step 84500, Loss: 3.4870, Loss_eval: 3.5002, Learning Rate: 5.000000e-05\n",
      "Step 84600, Loss: 2.7893, Loss_eval: 3.5136, Learning Rate: 5.000000e-05\n",
      "Step 84700, Loss: 3.3676, Loss_eval: 3.5017, Learning Rate: 5.000000e-05\n",
      "Step 84800, Loss: 3.2298, Loss_eval: 3.5018, Learning Rate: 5.000000e-05\n",
      "Step 84900, Loss: 3.1387, Loss_eval: 3.4773, Learning Rate: 5.000000e-05\n",
      "Step 85000, Loss: 3.3220, Loss_eval: 3.5043, Learning Rate: 5.000000e-05\n",
      "Step 85100, Loss: 3.0950, Loss_eval: 3.5030, Learning Rate: 5.000000e-05\n",
      "Step 85200, Loss: 3.0930, Loss_eval: 3.4811, Learning Rate: 5.000000e-05\n",
      "Step 85300, Loss: 3.3097, Loss_eval: 3.4878, Learning Rate: 5.000000e-05\n",
      "Step 85400, Loss: 2.9512, Loss_eval: 3.5106, Learning Rate: 5.000000e-05\n",
      "Step 85500, Loss: 3.2550, Loss_eval: 3.4933, Learning Rate: 5.000000e-05\n",
      "Step 85600, Loss: 3.4214, Loss_eval: 3.5068, Learning Rate: 5.000000e-05\n",
      "Step 85700, Loss: 3.4402, Loss_eval: 3.5111, Learning Rate: 5.000000e-05\n",
      "Step 85800, Loss: 3.3179, Loss_eval: 3.4894, Learning Rate: 5.000000e-05\n",
      "Step 85900, Loss: 2.9420, Loss_eval: 3.4931, Learning Rate: 5.000000e-05\n",
      "Step 86000, Loss: 3.1506, Loss_eval: 3.4830, Learning Rate: 5.000000e-05\n",
      "Step 86100, Loss: 3.2824, Loss_eval: 3.4937, Learning Rate: 5.000000e-05\n",
      "Step 86200, Loss: 2.9852, Loss_eval: 3.5160, Learning Rate: 5.000000e-05\n",
      "Step 86300, Loss: 3.3425, Loss_eval: 3.5169, Learning Rate: 5.000000e-05\n",
      "Step 86400, Loss: 3.2117, Loss_eval: 3.4716, Learning Rate: 5.000000e-05\n",
      "Step 86500, Loss: 3.3227, Loss_eval: 3.4843, Learning Rate: 5.000000e-05\n",
      "Step 86600, Loss: 3.0469, Loss_eval: 3.4623, Learning Rate: 5.000000e-05\n",
      "Step 86700, Loss: 3.1925, Loss_eval: 3.4909, Learning Rate: 5.000000e-05\n",
      "Step 86800, Loss: 3.0727, Loss_eval: 3.4762, Learning Rate: 5.000000e-05\n",
      "Step 86900, Loss: 3.1239, Loss_eval: 3.4896, Learning Rate: 5.000000e-05\n",
      "Step 87000, Loss: 3.0745, Loss_eval: 3.4989, Learning Rate: 5.000000e-05\n",
      "Step 87100, Loss: 3.2365, Loss_eval: 3.5125, Learning Rate: 5.000000e-05\n",
      "Step 87200, Loss: 3.1481, Loss_eval: 3.4776, Learning Rate: 5.000000e-05\n",
      "Step 87300, Loss: 3.1312, Loss_eval: 3.5002, Learning Rate: 5.000000e-05\n",
      "Step 87400, Loss: 3.0398, Loss_eval: 3.4761, Learning Rate: 5.000000e-05\n",
      "Step 87500, Loss: 2.9631, Loss_eval: 3.4888, Learning Rate: 5.000000e-05\n",
      "Step 87600, Loss: 2.9855, Loss_eval: 3.4924, Learning Rate: 5.000000e-05\n",
      "Step 87700, Loss: 3.1050, Loss_eval: 3.4727, Learning Rate: 5.000000e-05\n",
      "Step 87800, Loss: 3.3483, Loss_eval: 3.4884, Learning Rate: 5.000000e-05\n",
      "Step 87900, Loss: 3.3125, Loss_eval: 3.5018, Learning Rate: 5.000000e-05\n",
      "Step 88000, Loss: 2.9788, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 88100, Loss: 3.4135, Loss_eval: 3.4711, Learning Rate: 5.000000e-05\n",
      "Step 88200, Loss: 2.8122, Loss_eval: 3.4903, Learning Rate: 5.000000e-05\n",
      "Step 88300, Loss: 3.1343, Loss_eval: 3.4826, Learning Rate: 5.000000e-05\n",
      "Step 88400, Loss: 3.7061, Loss_eval: 3.5027, Learning Rate: 5.000000e-05\n",
      "Step 88500, Loss: 3.2263, Loss_eval: 3.4932, Learning Rate: 5.000000e-05\n",
      "Step 88600, Loss: 3.2465, Loss_eval: 3.5127, Learning Rate: 5.000000e-05\n",
      "Step 88700, Loss: 2.9388, Loss_eval: 3.4580, Learning Rate: 5.000000e-05\n",
      "Step 88800, Loss: 3.1419, Loss_eval: 3.4963, Learning Rate: 5.000000e-05\n",
      "Step 88900, Loss: 3.1338, Loss_eval: 3.5112, Learning Rate: 5.000000e-05\n",
      "Step 89000, Loss: 3.0822, Loss_eval: 3.4619, Learning Rate: 5.000000e-05\n",
      "Step 89100, Loss: 3.1190, Loss_eval: 3.4947, Learning Rate: 5.000000e-05\n",
      "Step 89200, Loss: 3.1281, Loss_eval: 3.4542, Learning Rate: 5.000000e-05\n",
      "Step 89300, Loss: 3.1129, Loss_eval: 3.5008, Learning Rate: 5.000000e-05\n",
      "Step 89400, Loss: 2.9001, Loss_eval: 3.4845, Learning Rate: 5.000000e-05\n",
      "Step 89500, Loss: 3.1775, Loss_eval: 3.4935, Learning Rate: 5.000000e-05\n",
      "Step 89600, Loss: 3.2047, Loss_eval: 3.4800, Learning Rate: 5.000000e-05\n",
      "Step 89700, Loss: 2.9494, Loss_eval: 3.4884, Learning Rate: 5.000000e-05\n",
      "Step 89800, Loss: 3.4029, Loss_eval: 3.4786, Learning Rate: 5.000000e-05\n",
      "Step 89900, Loss: 3.4554, Loss_eval: 3.4910, Learning Rate: 5.000000e-05\n",
      "Step 90000, Loss: 2.9934, Loss_eval: 3.5143, Learning Rate: 5.000000e-05\n",
      "Step 90100, Loss: 3.0695, Loss_eval: 3.5060, Learning Rate: 5.000000e-05\n",
      "Step 90200, Loss: 3.0316, Loss_eval: 3.4629, Learning Rate: 5.000000e-05\n",
      "Step 90300, Loss: 3.2131, Loss_eval: 3.5313, Learning Rate: 5.000000e-05\n",
      "Step 90400, Loss: 3.3245, Loss_eval: 3.5271, Learning Rate: 5.000000e-05\n",
      "Step 90500, Loss: 3.2271, Loss_eval: 3.4729, Learning Rate: 5.000000e-05\n",
      "Step 90600, Loss: 3.2067, Loss_eval: 3.4752, Learning Rate: 5.000000e-05\n",
      "Step 90700, Loss: 3.0756, Loss_eval: 3.5093, Learning Rate: 5.000000e-05\n",
      "Step 90800, Loss: 3.0789, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 90900, Loss: 3.0785, Loss_eval: 3.5025, Learning Rate: 5.000000e-05\n",
      "Step 91000, Loss: 3.5253, Loss_eval: 3.4693, Learning Rate: 5.000000e-05\n",
      "Step 91100, Loss: 3.5566, Loss_eval: 3.4755, Learning Rate: 5.000000e-05\n",
      "Step 91200, Loss: 3.3435, Loss_eval: 3.4581, Learning Rate: 5.000000e-05\n",
      "Step 91300, Loss: 3.4030, Loss_eval: 3.5043, Learning Rate: 5.000000e-05\n",
      "Step 91400, Loss: 3.3762, Loss_eval: 3.4987, Learning Rate: 5.000000e-05\n",
      "Step 91500, Loss: 3.2873, Loss_eval: 3.4794, Learning Rate: 5.000000e-05\n",
      "Step 91600, Loss: 3.0546, Loss_eval: 3.4871, Learning Rate: 5.000000e-05\n",
      "Step 91700, Loss: 3.1616, Loss_eval: 3.4890, Learning Rate: 5.000000e-05\n",
      "Step 91800, Loss: 3.0566, Loss_eval: 3.4648, Learning Rate: 5.000000e-05\n",
      "Step 91900, Loss: 3.3343, Loss_eval: 3.5166, Learning Rate: 5.000000e-05\n",
      "Step 92000, Loss: 3.1594, Loss_eval: 3.4872, Learning Rate: 5.000000e-05\n",
      "Step 92100, Loss: 3.1707, Loss_eval: 3.4644, Learning Rate: 5.000000e-05\n",
      "Step 92200, Loss: 2.9048, Loss_eval: 3.4997, Learning Rate: 5.000000e-05\n",
      "Step 92300, Loss: 3.0615, Loss_eval: 3.4597, Learning Rate: 5.000000e-05\n",
      "Step 92400, Loss: 3.1940, Loss_eval: 3.4747, Learning Rate: 5.000000e-05\n",
      "Step 92500, Loss: 3.4356, Loss_eval: 3.4636, Learning Rate: 5.000000e-05\n",
      "Step 92600, Loss: 3.1879, Loss_eval: 3.5053, Learning Rate: 5.000000e-05\n",
      "Step 92700, Loss: 3.1774, Loss_eval: 3.4910, Learning Rate: 5.000000e-05\n",
      "Step 92800, Loss: 3.0184, Loss_eval: 3.4720, Learning Rate: 5.000000e-05\n",
      "Step 92900, Loss: 3.1143, Loss_eval: 3.4521, Learning Rate: 5.000000e-05\n",
      "Step 93000, Loss: 3.3456, Loss_eval: 3.4581, Learning Rate: 5.000000e-05\n",
      "Step 93100, Loss: 3.3196, Loss_eval: 3.4616, Learning Rate: 5.000000e-05\n",
      "Step 93200, Loss: 3.2127, Loss_eval: 3.4820, Learning Rate: 5.000000e-05\n",
      "Step 93300, Loss: 2.9506, Loss_eval: 3.4801, Learning Rate: 5.000000e-05\n",
      "Step 93400, Loss: 3.2210, Loss_eval: 3.5302, Learning Rate: 5.000000e-05\n",
      "Step 93500, Loss: 3.1937, Loss_eval: 3.4737, Learning Rate: 5.000000e-05\n",
      "Step 93600, Loss: 3.3315, Loss_eval: 3.4571, Learning Rate: 5.000000e-05\n",
      "Step 93700, Loss: 3.3995, Loss_eval: 3.4849, Learning Rate: 5.000000e-05\n",
      "Step 93800, Loss: 3.3199, Loss_eval: 3.5151, Learning Rate: 5.000000e-05\n",
      "Step 93900, Loss: 3.1941, Loss_eval: 3.5012, Learning Rate: 5.000000e-05\n",
      "Step 94000, Loss: 3.1473, Loss_eval: 3.4805, Learning Rate: 5.000000e-05\n",
      "Step 94100, Loss: 2.8839, Loss_eval: 3.4893, Learning Rate: 5.000000e-05\n",
      "Step 94200, Loss: 3.0683, Loss_eval: 3.4668, Learning Rate: 5.000000e-05\n",
      "Step 94300, Loss: 3.2451, Loss_eval: 3.4927, Learning Rate: 5.000000e-05\n",
      "Step 94400, Loss: 3.6239, Loss_eval: 3.4805, Learning Rate: 5.000000e-05\n",
      "Step 94500, Loss: 3.1934, Loss_eval: 3.4957, Learning Rate: 5.000000e-05\n",
      "Step 94600, Loss: 3.2592, Loss_eval: 3.4588, Learning Rate: 5.000000e-05\n",
      "Step 94700, Loss: 3.2608, Loss_eval: 3.5114, Learning Rate: 5.000000e-05\n",
      "Step 94800, Loss: 3.2293, Loss_eval: 3.4853, Learning Rate: 5.000000e-05\n",
      "Step 94900, Loss: 3.0926, Loss_eval: 3.4884, Learning Rate: 5.000000e-05\n",
      "Step 95000, Loss: 3.0749, Loss_eval: 3.5048, Learning Rate: 5.000000e-05\n",
      "Step 95100, Loss: 3.1871, Loss_eval: 3.4725, Learning Rate: 5.000000e-05\n",
      "Step 95200, Loss: 3.3065, Loss_eval: 3.4868, Learning Rate: 5.000000e-05\n",
      "Step 95300, Loss: 3.1872, Loss_eval: 3.4784, Learning Rate: 5.000000e-05\n",
      "Step 95400, Loss: 3.3855, Loss_eval: 3.4813, Learning Rate: 5.000000e-05\n",
      "Step 95500, Loss: 3.0462, Loss_eval: 3.4733, Learning Rate: 5.000000e-05\n",
      "Step 95600, Loss: 3.2517, Loss_eval: 3.4862, Learning Rate: 5.000000e-05\n",
      "Step 95700, Loss: 3.0086, Loss_eval: 3.4949, Learning Rate: 5.000000e-05\n",
      "Step 95800, Loss: 2.6953, Loss_eval: 3.4697, Learning Rate: 5.000000e-05\n",
      "Step 95900, Loss: 3.3392, Loss_eval: 3.4807, Learning Rate: 5.000000e-05\n",
      "Step 96000, Loss: 3.0273, Loss_eval: 3.4875, Learning Rate: 5.000000e-05\n",
      "Step 96100, Loss: 3.2165, Loss_eval: 3.4822, Learning Rate: 5.000000e-05\n",
      "Step 96200, Loss: 3.1388, Loss_eval: 3.4798, Learning Rate: 5.000000e-05\n",
      "Step 96300, Loss: 3.2843, Loss_eval: 3.4619, Learning Rate: 5.000000e-05\n",
      "Step 96400, Loss: 3.2850, Loss_eval: 3.4823, Learning Rate: 5.000000e-05\n",
      "Step 96500, Loss: 3.0169, Loss_eval: 3.4648, Learning Rate: 5.000000e-05\n",
      "Step 96600, Loss: 3.1649, Loss_eval: 3.4905, Learning Rate: 5.000000e-05\n",
      "Step 96700, Loss: 3.0079, Loss_eval: 3.5111, Learning Rate: 5.000000e-05\n",
      "Step 96800, Loss: 3.0521, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 96900, Loss: 3.1568, Loss_eval: 3.4727, Learning Rate: 5.000000e-05\n",
      "Step 97000, Loss: 2.9409, Loss_eval: 3.4886, Learning Rate: 5.000000e-05\n",
      "Step 97100, Loss: 3.0842, Loss_eval: 3.4776, Learning Rate: 5.000000e-05\n",
      "Step 97200, Loss: 3.2455, Loss_eval: 3.4889, Learning Rate: 5.000000e-05\n",
      "Step 97300, Loss: 3.5038, Loss_eval: 3.4836, Learning Rate: 5.000000e-05\n",
      "Step 97400, Loss: 3.0423, Loss_eval: 3.4424, Learning Rate: 5.000000e-05\n",
      "Step 97500, Loss: 3.1713, Loss_eval: 3.4760, Learning Rate: 5.000000e-05\n",
      "Step 97600, Loss: 3.1291, Loss_eval: 3.4741, Learning Rate: 5.000000e-05\n",
      "Step 97700, Loss: 2.9594, Loss_eval: 3.4720, Learning Rate: 5.000000e-05\n",
      "Step 97800, Loss: 2.9867, Loss_eval: 3.4844, Learning Rate: 5.000000e-05\n",
      "Step 97900, Loss: 3.0299, Loss_eval: 3.4942, Learning Rate: 5.000000e-05\n",
      "Step 98000, Loss: 2.7660, Loss_eval: 3.4699, Learning Rate: 5.000000e-05\n",
      "Step 98100, Loss: 3.0883, Loss_eval: 3.4738, Learning Rate: 5.000000e-05\n",
      "Step 98200, Loss: 3.0329, Loss_eval: 3.5167, Learning Rate: 5.000000e-05\n",
      "Step 98300, Loss: 3.3612, Loss_eval: 3.4794, Learning Rate: 5.000000e-05\n",
      "Step 98400, Loss: 2.8426, Loss_eval: 3.4915, Learning Rate: 5.000000e-05\n",
      "Step 98500, Loss: 3.2519, Loss_eval: 3.4676, Learning Rate: 5.000000e-05\n",
      "Step 98600, Loss: 3.1240, Loss_eval: 3.4727, Learning Rate: 5.000000e-05\n",
      "Step 98700, Loss: 3.3033, Loss_eval: 3.4850, Learning Rate: 5.000000e-05\n",
      "Step 98800, Loss: 3.1614, Loss_eval: 3.4966, Learning Rate: 5.000000e-05\n",
      "Step 98900, Loss: 2.9972, Loss_eval: 3.4920, Learning Rate: 5.000000e-05\n",
      "Step 99000, Loss: 3.0306, Loss_eval: 3.4723, Learning Rate: 5.000000e-05\n",
      "Step 99100, Loss: 3.2678, Loss_eval: 3.4447, Learning Rate: 5.000000e-05\n",
      "Step 99200, Loss: 3.5155, Loss_eval: 3.4944, Learning Rate: 5.000000e-05\n",
      "Step 99300, Loss: 3.1607, Loss_eval: 3.4799, Learning Rate: 5.000000e-05\n",
      "Step 99400, Loss: 3.4424, Loss_eval: 3.4770, Learning Rate: 5.000000e-05\n",
      "Step 99500, Loss: 3.0966, Loss_eval: 3.4426, Learning Rate: 5.000000e-05\n",
      "Step 99600, Loss: 3.1760, Loss_eval: 3.4690, Learning Rate: 5.000000e-05\n",
      "Step 99700, Loss: 2.9862, Loss_eval: 3.4550, Learning Rate: 5.000000e-05\n",
      "Step 99800, Loss: 3.1325, Loss_eval: 3.5122, Learning Rate: 5.000000e-05\n",
      "Step 99900, Loss: 2.8855, Loss_eval: 3.4642, Learning Rate: 5.000000e-05\n",
      "Step 100000, Loss: 2.8484, Loss_eval: 3.4316, Learning Rate: 5.000000e-05\n",
      "Step 100100, Loss: 3.3602, Loss_eval: 3.5128, Learning Rate: 5.000000e-05\n",
      "Step 100200, Loss: 2.9088, Loss_eval: 3.4635, Learning Rate: 5.000000e-05\n",
      "Step 100300, Loss: 3.1187, Loss_eval: 3.5130, Learning Rate: 5.000000e-05\n",
      "Step 100400, Loss: 3.0450, Loss_eval: 3.4895, Learning Rate: 5.000000e-05\n",
      "Step 100500, Loss: 3.1593, Loss_eval: 3.4626, Learning Rate: 5.000000e-05\n",
      "Step 100600, Loss: 2.9195, Loss_eval: 3.4692, Learning Rate: 5.000000e-05\n",
      "Step 100700, Loss: 3.2617, Loss_eval: 3.4509, Learning Rate: 5.000000e-05\n",
      "Step 100800, Loss: 3.1015, Loss_eval: 3.4866, Learning Rate: 5.000000e-05\n",
      "Step 100900, Loss: 3.2648, Loss_eval: 3.4870, Learning Rate: 5.000000e-05\n",
      "Step 101000, Loss: 3.4134, Loss_eval: 3.4777, Learning Rate: 5.000000e-05\n",
      "Step 101100, Loss: 3.1974, Loss_eval: 3.5246, Learning Rate: 5.000000e-05\n",
      "Step 101200, Loss: 2.9254, Loss_eval: 3.4734, Learning Rate: 5.000000e-05\n",
      "Step 101300, Loss: 3.5462, Loss_eval: 3.4699, Learning Rate: 5.000000e-05\n",
      "Step 101400, Loss: 3.1304, Loss_eval: 3.4514, Learning Rate: 5.000000e-05\n",
      "Step 101500, Loss: 3.2553, Loss_eval: 3.5108, Learning Rate: 5.000000e-05\n",
      "Step 101600, Loss: 3.1686, Loss_eval: 3.4749, Learning Rate: 5.000000e-05\n",
      "Step 101700, Loss: 3.0561, Loss_eval: 3.4783, Learning Rate: 5.000000e-05\n",
      "Step 101800, Loss: 3.5835, Loss_eval: 3.4811, Learning Rate: 5.000000e-05\n",
      "Step 101900, Loss: 3.4085, Loss_eval: 3.4878, Learning Rate: 5.000000e-05\n",
      "Step 102000, Loss: 3.0068, Loss_eval: 3.4534, Learning Rate: 5.000000e-05\n",
      "Step 102100, Loss: 3.5199, Loss_eval: 3.4920, Learning Rate: 5.000000e-05\n",
      "Step 102200, Loss: 3.1866, Loss_eval: 3.5024, Learning Rate: 5.000000e-05\n",
      "Step 102300, Loss: 3.4888, Loss_eval: 3.4961, Learning Rate: 5.000000e-05\n",
      "Step 102400, Loss: 3.1896, Loss_eval: 3.4838, Learning Rate: 5.000000e-05\n",
      "Step 102500, Loss: 3.3063, Loss_eval: 3.4911, Learning Rate: 5.000000e-05\n",
      "Step 102600, Loss: 3.1784, Loss_eval: 3.4644, Learning Rate: 5.000000e-05\n",
      "Step 102700, Loss: 3.0818, Loss_eval: 3.4586, Learning Rate: 5.000000e-05\n",
      "Step 102800, Loss: 3.1162, Loss_eval: 3.5212, Learning Rate: 5.000000e-05\n",
      "Step 102900, Loss: 3.6163, Loss_eval: 3.4537, Learning Rate: 5.000000e-05\n",
      "Step 103000, Loss: 3.0109, Loss_eval: 3.4894, Learning Rate: 5.000000e-05\n",
      "Step 103100, Loss: 3.0951, Loss_eval: 3.4891, Learning Rate: 5.000000e-05\n",
      "Step 103200, Loss: 3.1693, Loss_eval: 3.4916, Learning Rate: 5.000000e-05\n",
      "Step 103300, Loss: 3.0522, Loss_eval: 3.4733, Learning Rate: 5.000000e-05\n",
      "Step 103400, Loss: 3.1735, Loss_eval: 3.5000, Learning Rate: 5.000000e-05\n",
      "Step 103500, Loss: 3.0167, Loss_eval: 3.4802, Learning Rate: 5.000000e-05\n",
      "Step 103600, Loss: 2.8991, Loss_eval: 3.4714, Learning Rate: 5.000000e-05\n",
      "Step 103700, Loss: 3.3634, Loss_eval: 3.5033, Learning Rate: 5.000000e-05\n",
      "Step 103800, Loss: 3.2605, Loss_eval: 3.4912, Learning Rate: 5.000000e-05\n",
      "Step 103900, Loss: 3.0283, Loss_eval: 3.4859, Learning Rate: 5.000000e-05\n",
      "Step 104000, Loss: 3.2647, Loss_eval: 3.4771, Learning Rate: 5.000000e-05\n",
      "Step 104100, Loss: 3.1219, Loss_eval: 3.4951, Learning Rate: 5.000000e-05\n",
      "Step 104200, Loss: 2.9613, Loss_eval: 3.4608, Learning Rate: 5.000000e-05\n",
      "Step 104300, Loss: 3.3796, Loss_eval: 3.4835, Learning Rate: 5.000000e-05\n",
      "Step 104400, Loss: 2.8668, Loss_eval: 3.4700, Learning Rate: 5.000000e-05\n",
      "Step 104500, Loss: 3.2405, Loss_eval: 3.4985, Learning Rate: 5.000000e-05\n",
      "Step 104600, Loss: 3.0156, Loss_eval: 3.5064, Learning Rate: 5.000000e-05\n",
      "Step 104700, Loss: 3.2886, Loss_eval: 3.4717, Learning Rate: 5.000000e-05\n",
      "Step 104800, Loss: 3.2524, Loss_eval: 3.4862, Learning Rate: 5.000000e-05\n",
      "Step 104900, Loss: 3.3289, Loss_eval: 3.4969, Learning Rate: 5.000000e-05\n",
      "Step 105000, Loss: 2.9342, Loss_eval: 3.5127, Learning Rate: 5.000000e-05\n",
      "Step 105100, Loss: 2.6649, Loss_eval: 3.4956, Learning Rate: 5.000000e-05\n",
      "Step 105200, Loss: 2.8770, Loss_eval: 3.4712, Learning Rate: 5.000000e-05\n",
      "Step 105300, Loss: 3.0841, Loss_eval: 3.4802, Learning Rate: 5.000000e-05\n",
      "Step 105400, Loss: 2.8620, Loss_eval: 3.4821, Learning Rate: 5.000000e-05\n",
      "Step 105500, Loss: 2.9819, Loss_eval: 3.4831, Learning Rate: 5.000000e-05\n",
      "Step 105600, Loss: 3.0381, Loss_eval: 3.4602, Learning Rate: 5.000000e-05\n",
      "Step 105700, Loss: 3.1092, Loss_eval: 3.5084, Learning Rate: 5.000000e-05\n",
      "Step 105800, Loss: 3.1505, Loss_eval: 3.4728, Learning Rate: 5.000000e-05\n",
      "Step 105900, Loss: 3.0717, Loss_eval: 3.4841, Learning Rate: 5.000000e-05\n",
      "Step 106000, Loss: 3.2699, Loss_eval: 3.4702, Learning Rate: 5.000000e-05\n",
      "Step 106100, Loss: 2.9843, Loss_eval: 3.5121, Learning Rate: 5.000000e-05\n",
      "Step 106200, Loss: 3.3021, Loss_eval: 3.4938, Learning Rate: 5.000000e-05\n",
      "Step 106300, Loss: 3.3667, Loss_eval: 3.4779, Learning Rate: 5.000000e-05\n",
      "Step 106400, Loss: 3.1344, Loss_eval: 3.4827, Learning Rate: 5.000000e-05\n",
      "Step 106500, Loss: 3.0860, Loss_eval: 3.4728, Learning Rate: 5.000000e-05\n",
      "Step 106600, Loss: 3.1675, Loss_eval: 3.4829, Learning Rate: 5.000000e-05\n",
      "Step 106700, Loss: 3.3163, Loss_eval: 3.4759, Learning Rate: 5.000000e-05\n",
      "Step 106800, Loss: 2.9410, Loss_eval: 3.4901, Learning Rate: 5.000000e-05\n",
      "Step 106900, Loss: 2.9298, Loss_eval: 3.4544, Learning Rate: 5.000000e-05\n",
      "Step 107000, Loss: 3.0923, Loss_eval: 3.4547, Learning Rate: 5.000000e-05\n",
      "Step 107100, Loss: 2.9481, Loss_eval: 3.4566, Learning Rate: 5.000000e-05\n",
      "Step 107200, Loss: 3.0624, Loss_eval: 3.4637, Learning Rate: 5.000000e-05\n",
      "Step 107300, Loss: 3.0232, Loss_eval: 3.4969, Learning Rate: 5.000000e-05\n",
      "Step 107400, Loss: 2.8550, Loss_eval: 3.4986, Learning Rate: 5.000000e-05\n",
      "Step 107500, Loss: 2.9162, Loss_eval: 3.4823, Learning Rate: 5.000000e-05\n",
      "Step 107600, Loss: 3.2963, Loss_eval: 3.4657, Learning Rate: 5.000000e-05\n",
      "Step 107700, Loss: 3.3950, Loss_eval: 3.4603, Learning Rate: 5.000000e-05\n",
      "Step 107800, Loss: 3.1902, Loss_eval: 3.4743, Learning Rate: 5.000000e-05\n",
      "Step 107900, Loss: 3.4215, Loss_eval: 3.5147, Learning Rate: 5.000000e-05\n",
      "Step 108000, Loss: 3.1416, Loss_eval: 3.4862, Learning Rate: 5.000000e-05\n",
      "Step 108100, Loss: 3.0779, Loss_eval: 3.4356, Learning Rate: 5.000000e-05\n",
      "Step 108200, Loss: 2.8908, Loss_eval: 3.4606, Learning Rate: 5.000000e-05\n",
      "Step 108300, Loss: 3.1908, Loss_eval: 3.5022, Learning Rate: 5.000000e-05\n",
      "Step 108400, Loss: 3.0464, Loss_eval: 3.4827, Learning Rate: 5.000000e-05\n",
      "Step 108500, Loss: 3.1883, Loss_eval: 3.4794, Learning Rate: 5.000000e-05\n",
      "Step 108600, Loss: 3.4110, Loss_eval: 3.4744, Learning Rate: 5.000000e-05\n",
      "Step 108700, Loss: 3.4071, Loss_eval: 3.4888, Learning Rate: 5.000000e-05\n",
      "Step 108800, Loss: 3.3457, Loss_eval: 3.4966, Learning Rate: 5.000000e-05\n",
      "Step 108900, Loss: 3.1007, Loss_eval: 3.5057, Learning Rate: 5.000000e-05\n",
      "Step 109000, Loss: 2.9462, Loss_eval: 3.4732, Learning Rate: 5.000000e-05\n",
      "Step 109100, Loss: 2.8875, Loss_eval: 3.4981, Learning Rate: 5.000000e-05\n",
      "Step 109200, Loss: 3.0024, Loss_eval: 3.4651, Learning Rate: 5.000000e-05\n",
      "Step 109300, Loss: 3.1383, Loss_eval: 3.4627, Learning Rate: 5.000000e-05\n",
      "Step 109400, Loss: 3.2258, Loss_eval: 3.4902, Learning Rate: 5.000000e-05\n",
      "Step 109500, Loss: 3.4061, Loss_eval: 3.4991, Learning Rate: 5.000000e-05\n",
      "Step 109600, Loss: 3.0137, Loss_eval: 3.5013, Learning Rate: 5.000000e-05\n",
      "Step 109700, Loss: 3.3239, Loss_eval: 3.4805, Learning Rate: 5.000000e-05\n",
      "Step 109800, Loss: 2.9454, Loss_eval: 3.4884, Learning Rate: 5.000000e-05\n",
      "Step 109900, Loss: 3.3769, Loss_eval: 3.4679, Learning Rate: 5.000000e-05\n",
      "Step 110000, Loss: 3.0545, Loss_eval: 3.4748, Learning Rate: 5.000000e-05\n",
      "Step 110100, Loss: 3.5120, Loss_eval: 3.5018, Learning Rate: 5.000000e-05\n",
      "Step 110200, Loss: 3.1372, Loss_eval: 3.4942, Learning Rate: 5.000000e-05\n",
      "Step 110300, Loss: 3.2189, Loss_eval: 3.5203, Learning Rate: 5.000000e-05\n",
      "Step 110400, Loss: 2.6824, Loss_eval: 3.5118, Learning Rate: 5.000000e-05\n",
      "Step 110500, Loss: 3.3953, Loss_eval: 3.4540, Learning Rate: 5.000000e-05\n",
      "Step 110600, Loss: 3.0823, Loss_eval: 3.4984, Learning Rate: 5.000000e-05\n",
      "Step 110700, Loss: 3.3756, Loss_eval: 3.4560, Learning Rate: 5.000000e-05\n",
      "Step 110800, Loss: 3.3607, Loss_eval: 3.4648, Learning Rate: 5.000000e-05\n",
      "Step 110900, Loss: 3.0266, Loss_eval: 3.4648, Learning Rate: 5.000000e-05\n",
      "Step 111000, Loss: 3.3734, Loss_eval: 3.4973, Learning Rate: 5.000000e-05\n",
      "Step 111100, Loss: 3.3567, Loss_eval: 3.4877, Learning Rate: 5.000000e-05\n",
      "Step 111200, Loss: 3.4130, Loss_eval: 3.4961, Learning Rate: 5.000000e-05\n",
      "Step 111300, Loss: 2.9473, Loss_eval: 3.4731, Learning Rate: 5.000000e-05\n",
      "Step 111400, Loss: 3.1340, Loss_eval: 3.4982, Learning Rate: 5.000000e-05\n",
      "Step 111500, Loss: 3.4351, Loss_eval: 3.4692, Learning Rate: 5.000000e-05\n",
      "Step 111600, Loss: 3.0480, Loss_eval: 3.4865, Learning Rate: 5.000000e-05\n",
      "Step 111700, Loss: 2.9625, Loss_eval: 3.4711, Learning Rate: 5.000000e-05\n",
      "Step 111800, Loss: 3.2227, Loss_eval: 3.5043, Learning Rate: 5.000000e-05\n",
      "Step 111900, Loss: 3.2003, Loss_eval: 3.4971, Learning Rate: 5.000000e-05\n",
      "Step 112000, Loss: 3.2003, Loss_eval: 3.4704, Learning Rate: 5.000000e-05\n",
      "Step 112100, Loss: 3.1506, Loss_eval: 3.4893, Learning Rate: 5.000000e-05\n",
      "Step 112200, Loss: 2.8088, Loss_eval: 3.4537, Learning Rate: 5.000000e-05\n",
      "Step 112300, Loss: 3.0305, Loss_eval: 3.4722, Learning Rate: 5.000000e-05\n",
      "Step 112400, Loss: 3.1941, Loss_eval: 3.4608, Learning Rate: 5.000000e-05\n",
      "Step 112500, Loss: 3.1262, Loss_eval: 3.4545, Learning Rate: 5.000000e-05\n",
      "Step 112600, Loss: 2.9515, Loss_eval: 3.4616, Learning Rate: 5.000000e-05\n",
      "Step 112700, Loss: 3.1672, Loss_eval: 3.4681, Learning Rate: 5.000000e-05\n",
      "Step 112800, Loss: 3.6531, Loss_eval: 3.4511, Learning Rate: 5.000000e-05\n",
      "Step 112900, Loss: 2.9792, Loss_eval: 3.4756, Learning Rate: 5.000000e-05\n",
      "Step 113000, Loss: 3.1587, Loss_eval: 3.4640, Learning Rate: 5.000000e-05\n",
      "Step 113100, Loss: 3.2916, Loss_eval: 3.4806, Learning Rate: 5.000000e-05\n",
      "Step 113200, Loss: 3.0283, Loss_eval: 3.4837, Learning Rate: 5.000000e-05\n",
      "Step 113300, Loss: 3.2633, Loss_eval: 3.4532, Learning Rate: 5.000000e-05\n",
      "Step 113400, Loss: 3.0332, Loss_eval: 3.4486, Learning Rate: 5.000000e-05\n",
      "Step 113500, Loss: 2.8633, Loss_eval: 3.4843, Learning Rate: 5.000000e-05\n",
      "Step 113600, Loss: 3.1994, Loss_eval: 3.4894, Learning Rate: 5.000000e-05\n",
      "Step 113700, Loss: 3.2409, Loss_eval: 3.4869, Learning Rate: 5.000000e-05\n",
      "Step 113800, Loss: 2.9855, Loss_eval: 3.4595, Learning Rate: 5.000000e-05\n",
      "Step 113900, Loss: 3.2976, Loss_eval: 3.4862, Learning Rate: 5.000000e-05\n",
      "Step 114000, Loss: 3.1951, Loss_eval: 3.4754, Learning Rate: 5.000000e-05\n",
      "Step 114100, Loss: 3.2377, Loss_eval: 3.4663, Learning Rate: 5.000000e-05\n",
      "Step 114200, Loss: 3.2217, Loss_eval: 3.4642, Learning Rate: 5.000000e-05\n",
      "Step 114300, Loss: 3.3113, Loss_eval: 3.4672, Learning Rate: 5.000000e-05\n",
      "Step 114400, Loss: 3.3084, Loss_eval: 3.5147, Learning Rate: 5.000000e-05\n",
      "Step 114500, Loss: 2.8036, Loss_eval: 3.4780, Learning Rate: 5.000000e-05\n",
      "Step 114600, Loss: 3.4349, Loss_eval: 3.5045, Learning Rate: 5.000000e-05\n",
      "Step 114700, Loss: 3.1054, Loss_eval: 3.4696, Learning Rate: 5.000000e-05\n",
      "Step 114800, Loss: 3.1598, Loss_eval: 3.5061, Learning Rate: 5.000000e-05\n",
      "Step 114900, Loss: 3.2388, Loss_eval: 3.5136, Learning Rate: 5.000000e-05\n",
      "Step 115000, Loss: 3.1923, Loss_eval: 3.4740, Learning Rate: 5.000000e-05\n",
      "Step 115100, Loss: 2.6682, Loss_eval: 3.4669, Learning Rate: 5.000000e-05\n",
      "Step 115200, Loss: 3.1115, Loss_eval: 3.4688, Learning Rate: 5.000000e-05\n",
      "Step 115300, Loss: 2.9959, Loss_eval: 3.4851, Learning Rate: 5.000000e-05\n",
      "Step 115400, Loss: 3.2177, Loss_eval: 3.5087, Learning Rate: 5.000000e-05\n",
      "Step 115500, Loss: 3.4171, Loss_eval: 3.4892, Learning Rate: 5.000000e-05\n",
      "Step 115600, Loss: 3.3215, Loss_eval: 3.4668, Learning Rate: 5.000000e-05\n",
      "Step 115700, Loss: 3.2467, Loss_eval: 3.4619, Learning Rate: 5.000000e-05\n",
      "Step 115800, Loss: 3.2024, Loss_eval: 3.4885, Learning Rate: 5.000000e-05\n",
      "Step 115900, Loss: 2.8361, Loss_eval: 3.4663, Learning Rate: 5.000000e-05\n",
      "Step 116000, Loss: 3.2044, Loss_eval: 3.4898, Learning Rate: 5.000000e-05\n",
      "Step 116100, Loss: 3.1786, Loss_eval: 3.4611, Learning Rate: 5.000000e-05\n",
      "Step 116200, Loss: 2.9743, Loss_eval: 3.4906, Learning Rate: 5.000000e-05\n",
      "Step 116300, Loss: 2.8176, Loss_eval: 3.4737, Learning Rate: 5.000000e-05\n",
      "Step 116400, Loss: 3.3892, Loss_eval: 3.4837, Learning Rate: 5.000000e-05\n",
      "Step 116500, Loss: 3.1446, Loss_eval: 3.4859, Learning Rate: 5.000000e-05\n",
      "Step 116600, Loss: 2.9762, Loss_eval: 3.4977, Learning Rate: 5.000000e-05\n",
      "Step 116700, Loss: 3.1579, Loss_eval: 3.4966, Learning Rate: 5.000000e-05\n",
      "Step 116800, Loss: 3.0658, Loss_eval: 3.4766, Learning Rate: 5.000000e-05\n",
      "Step 116900, Loss: 3.3594, Loss_eval: 3.4743, Learning Rate: 5.000000e-05\n",
      "Step 117000, Loss: 3.3596, Loss_eval: 3.4588, Learning Rate: 5.000000e-05\n",
      "Step 117100, Loss: 3.1692, Loss_eval: 3.4827, Learning Rate: 5.000000e-05\n",
      "Step 117200, Loss: 3.1924, Loss_eval: 3.4763, Learning Rate: 5.000000e-05\n",
      "Step 117300, Loss: 3.5981, Loss_eval: 3.5216, Learning Rate: 5.000000e-05\n",
      "Step 117400, Loss: 3.2238, Loss_eval: 3.4641, Learning Rate: 5.000000e-05\n",
      "Step 117500, Loss: 3.0682, Loss_eval: 3.4917, Learning Rate: 5.000000e-05\n",
      "Step 117600, Loss: 3.0997, Loss_eval: 3.4747, Learning Rate: 5.000000e-05\n",
      "Step 117700, Loss: 3.2218, Loss_eval: 3.4714, Learning Rate: 5.000000e-05\n",
      "Step 117800, Loss: 3.3265, Loss_eval: 3.4797, Learning Rate: 5.000000e-05\n",
      "Step 117900, Loss: 3.0706, Loss_eval: 3.4519, Learning Rate: 5.000000e-05\n",
      "Step 118000, Loss: 3.0388, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 118100, Loss: 3.2647, Loss_eval: 3.4770, Learning Rate: 5.000000e-05\n",
      "Step 118200, Loss: 3.1725, Loss_eval: 3.4420, Learning Rate: 5.000000e-05\n",
      "Step 118300, Loss: 3.3090, Loss_eval: 3.4942, Learning Rate: 5.000000e-05\n",
      "Step 118400, Loss: 3.2888, Loss_eval: 3.4646, Learning Rate: 5.000000e-05\n",
      "Step 118500, Loss: 3.3892, Loss_eval: 3.4774, Learning Rate: 5.000000e-05\n",
      "Step 118600, Loss: 3.2933, Loss_eval: 3.4732, Learning Rate: 5.000000e-05\n",
      "Step 118700, Loss: 3.1677, Loss_eval: 3.4577, Learning Rate: 5.000000e-05\n",
      "Step 118800, Loss: 3.0271, Loss_eval: 3.4875, Learning Rate: 5.000000e-05\n",
      "Step 118900, Loss: 3.2782, Loss_eval: 3.4846, Learning Rate: 5.000000e-05\n",
      "Step 119000, Loss: 3.0693, Loss_eval: 3.4659, Learning Rate: 5.000000e-05\n",
      "Step 119100, Loss: 3.3557, Loss_eval: 3.4447, Learning Rate: 5.000000e-05\n",
      "Step 119200, Loss: 3.1182, Loss_eval: 3.4773, Learning Rate: 5.000000e-05\n",
      "Step 119300, Loss: 2.9079, Loss_eval: 3.5020, Learning Rate: 5.000000e-05\n",
      "Step 119400, Loss: 3.3056, Loss_eval: 3.4545, Learning Rate: 5.000000e-05\n",
      "Step 119500, Loss: 3.1534, Loss_eval: 3.4764, Learning Rate: 5.000000e-05\n",
      "Step 119600, Loss: 3.2816, Loss_eval: 3.4643, Learning Rate: 5.000000e-05\n",
      "Step 119700, Loss: 3.2924, Loss_eval: 3.4745, Learning Rate: 5.000000e-05\n",
      "Step 119800, Loss: 3.1096, Loss_eval: 3.4807, Learning Rate: 5.000000e-05\n",
      "Step 119900, Loss: 3.0203, Loss_eval: 3.4907, Learning Rate: 5.000000e-05\n",
      "Step 120000, Loss: 2.9542, Loss_eval: 3.4535, Learning Rate: 5.000000e-05\n",
      "Step 120100, Loss: 2.8181, Loss_eval: 3.4228, Learning Rate: 5.000000e-05\n",
      "Step 120200, Loss: 3.3472, Loss_eval: 3.4590, Learning Rate: 5.000000e-05\n",
      "Step 120300, Loss: 3.2564, Loss_eval: 3.4527, Learning Rate: 5.000000e-05\n",
      "Step 120400, Loss: 3.5600, Loss_eval: 3.4626, Learning Rate: 5.000000e-05\n",
      "Step 120500, Loss: 2.9310, Loss_eval: 3.4566, Learning Rate: 5.000000e-05\n",
      "Step 120600, Loss: 3.1801, Loss_eval: 3.5004, Learning Rate: 5.000000e-05\n",
      "Step 120700, Loss: 3.2524, Loss_eval: 3.4483, Learning Rate: 5.000000e-05\n",
      "Step 120800, Loss: 3.3181, Loss_eval: 3.4762, Learning Rate: 5.000000e-05\n",
      "Step 120900, Loss: 3.0850, Loss_eval: 3.4585, Learning Rate: 5.000000e-05\n",
      "Step 121000, Loss: 3.3357, Loss_eval: 3.5006, Learning Rate: 5.000000e-05\n",
      "Step 121100, Loss: 3.3676, Loss_eval: 3.4589, Learning Rate: 5.000000e-05\n",
      "Step 121200, Loss: 3.0145, Loss_eval: 3.4930, Learning Rate: 5.000000e-05\n",
      "Step 121300, Loss: 2.8941, Loss_eval: 3.4825, Learning Rate: 5.000000e-05\n",
      "Step 121400, Loss: 3.1919, Loss_eval: 3.4897, Learning Rate: 5.000000e-05\n",
      "Step 121500, Loss: 3.2498, Loss_eval: 3.4816, Learning Rate: 5.000000e-05\n",
      "Step 121600, Loss: 3.1532, Loss_eval: 3.4747, Learning Rate: 5.000000e-05\n",
      "Step 121700, Loss: 2.9241, Loss_eval: 3.4521, Learning Rate: 5.000000e-05\n",
      "Step 121800, Loss: 3.0782, Loss_eval: 3.4693, Learning Rate: 5.000000e-05\n",
      "Step 121900, Loss: 3.2397, Loss_eval: 3.4710, Learning Rate: 5.000000e-05\n",
      "Step 122000, Loss: 3.0498, Loss_eval: 3.4787, Learning Rate: 5.000000e-05\n",
      "Step 122100, Loss: 3.2340, Loss_eval: 3.4769, Learning Rate: 5.000000e-05\n",
      "Step 122200, Loss: 3.2910, Loss_eval: 3.5069, Learning Rate: 5.000000e-05\n",
      "Step 122300, Loss: 3.1874, Loss_eval: 3.4744, Learning Rate: 5.000000e-05\n",
      "Step 122400, Loss: 3.4323, Loss_eval: 3.4673, Learning Rate: 5.000000e-05\n",
      "Step 122500, Loss: 3.4209, Loss_eval: 3.5127, Learning Rate: 5.000000e-05\n",
      "Step 122600, Loss: 3.1284, Loss_eval: 3.4515, Learning Rate: 5.000000e-05\n",
      "Step 122700, Loss: 3.2496, Loss_eval: 3.4913, Learning Rate: 5.000000e-05\n",
      "Step 122800, Loss: 3.5662, Loss_eval: 3.4864, Learning Rate: 5.000000e-05\n",
      "Step 122900, Loss: 3.3544, Loss_eval: 3.4503, Learning Rate: 5.000000e-05\n",
      "Step 123000, Loss: 3.2130, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 123100, Loss: 3.4778, Loss_eval: 3.4795, Learning Rate: 5.000000e-05\n",
      "Step 123200, Loss: 3.3712, Loss_eval: 3.4745, Learning Rate: 5.000000e-05\n",
      "Step 123300, Loss: 2.9209, Loss_eval: 3.5014, Learning Rate: 5.000000e-05\n",
      "Step 123400, Loss: 3.0362, Loss_eval: 3.5085, Learning Rate: 5.000000e-05\n",
      "Step 123500, Loss: 3.0969, Loss_eval: 3.4633, Learning Rate: 5.000000e-05\n",
      "Step 123600, Loss: 3.3041, Loss_eval: 3.4481, Learning Rate: 5.000000e-05\n",
      "Step 123700, Loss: 3.2164, Loss_eval: 3.4497, Learning Rate: 5.000000e-05\n",
      "Step 123800, Loss: 3.0690, Loss_eval: 3.4488, Learning Rate: 5.000000e-05\n",
      "Step 123900, Loss: 3.2806, Loss_eval: 3.4686, Learning Rate: 5.000000e-05\n",
      "Step 124000, Loss: 2.9994, Loss_eval: 3.4894, Learning Rate: 5.000000e-05\n",
      "Step 124100, Loss: 3.3052, Loss_eval: 3.4740, Learning Rate: 5.000000e-05\n",
      "Step 124200, Loss: 3.1474, Loss_eval: 3.4802, Learning Rate: 5.000000e-05\n",
      "Step 124300, Loss: 3.4229, Loss_eval: 3.4478, Learning Rate: 5.000000e-05\n",
      "Step 124400, Loss: 3.1864, Loss_eval: 3.4348, Learning Rate: 5.000000e-05\n",
      "Step 124500, Loss: 3.2385, Loss_eval: 3.4596, Learning Rate: 5.000000e-05\n",
      "Step 124600, Loss: 3.0650, Loss_eval: 3.4831, Learning Rate: 5.000000e-05\n",
      "Step 124700, Loss: 3.1509, Loss_eval: 3.4701, Learning Rate: 5.000000e-05\n",
      "Step 124800, Loss: 3.2590, Loss_eval: 3.4749, Learning Rate: 5.000000e-05\n",
      "Step 124900, Loss: 3.0724, Loss_eval: 3.4767, Learning Rate: 5.000000e-05\n",
      "Step 125000, Loss: 3.3206, Loss_eval: 3.5107, Learning Rate: 5.000000e-05\n",
      "Step 125100, Loss: 2.9824, Loss_eval: 3.5051, Learning Rate: 5.000000e-05\n",
      "Step 125200, Loss: 3.1591, Loss_eval: 3.4811, Learning Rate: 5.000000e-05\n",
      "Step 125300, Loss: 2.9183, Loss_eval: 3.4833, Learning Rate: 5.000000e-05\n",
      "Step 125400, Loss: 3.1309, Loss_eval: 3.4777, Learning Rate: 5.000000e-05\n",
      "Step 125500, Loss: 3.0401, Loss_eval: 3.4812, Learning Rate: 5.000000e-05\n",
      "Step 125600, Loss: 3.0603, Loss_eval: 3.4929, Learning Rate: 5.000000e-05\n",
      "Step 125700, Loss: 2.9543, Loss_eval: 3.5018, Learning Rate: 5.000000e-05\n",
      "Step 125800, Loss: 3.0083, Loss_eval: 3.4886, Learning Rate: 5.000000e-05\n",
      "Step 125900, Loss: 3.3179, Loss_eval: 3.4645, Learning Rate: 5.000000e-05\n",
      "Step 126000, Loss: 3.3435, Loss_eval: 3.4674, Learning Rate: 5.000000e-05\n",
      "Step 126100, Loss: 3.1503, Loss_eval: 3.4548, Learning Rate: 5.000000e-05\n",
      "Step 126200, Loss: 3.1258, Loss_eval: 3.5023, Learning Rate: 5.000000e-05\n",
      "Step 126300, Loss: 3.1915, Loss_eval: 3.4726, Learning Rate: 5.000000e-05\n",
      "Step 126400, Loss: 3.0700, Loss_eval: 3.4757, Learning Rate: 5.000000e-05\n",
      "Step 126500, Loss: 3.2761, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 126600, Loss: 3.3801, Loss_eval: 3.5036, Learning Rate: 5.000000e-05\n",
      "Step 126700, Loss: 3.0492, Loss_eval: 3.4565, Learning Rate: 5.000000e-05\n",
      "Step 126800, Loss: 3.3155, Loss_eval: 3.4645, Learning Rate: 5.000000e-05\n",
      "Step 126900, Loss: 3.1016, Loss_eval: 3.4852, Learning Rate: 5.000000e-05\n",
      "Step 127000, Loss: 2.9824, Loss_eval: 3.4784, Learning Rate: 5.000000e-05\n",
      "Step 127100, Loss: 3.2151, Loss_eval: 3.5162, Learning Rate: 5.000000e-05\n",
      "Step 127200, Loss: 3.3677, Loss_eval: 3.4807, Learning Rate: 5.000000e-05\n",
      "Step 127300, Loss: 3.6370, Loss_eval: 3.4641, Learning Rate: 5.000000e-05\n",
      "Step 127400, Loss: 3.3876, Loss_eval: 3.4892, Learning Rate: 5.000000e-05\n",
      "Step 127500, Loss: 3.0353, Loss_eval: 3.4547, Learning Rate: 5.000000e-05\n",
      "Step 127600, Loss: 3.1189, Loss_eval: 3.4875, Learning Rate: 5.000000e-05\n",
      "Step 127700, Loss: 3.0479, Loss_eval: 3.4627, Learning Rate: 5.000000e-05\n",
      "Step 127800, Loss: 3.0600, Loss_eval: 3.4822, Learning Rate: 5.000000e-05\n",
      "Step 127900, Loss: 3.2636, Loss_eval: 3.4906, Learning Rate: 5.000000e-05\n",
      "Step 128000, Loss: 2.9617, Loss_eval: 3.4725, Learning Rate: 5.000000e-05\n",
      "Step 128100, Loss: 3.4605, Loss_eval: 3.4841, Learning Rate: 5.000000e-05\n",
      "Step 128200, Loss: 3.0327, Loss_eval: 3.4922, Learning Rate: 5.000000e-05\n",
      "Step 128300, Loss: 3.0940, Loss_eval: 3.4772, Learning Rate: 5.000000e-05\n",
      "Step 128400, Loss: 3.5037, Loss_eval: 3.4859, Learning Rate: 5.000000e-05\n",
      "Step 128500, Loss: 2.9343, Loss_eval: 3.4502, Learning Rate: 5.000000e-05\n",
      "Step 128600, Loss: 3.1787, Loss_eval: 3.4837, Learning Rate: 5.000000e-05\n",
      "Step 128700, Loss: 3.0766, Loss_eval: 3.4531, Learning Rate: 5.000000e-05\n",
      "Step 128800, Loss: 3.0662, Loss_eval: 3.4720, Learning Rate: 5.000000e-05\n",
      "Step 128900, Loss: 3.2655, Loss_eval: 3.4444, Learning Rate: 5.000000e-05\n",
      "Step 129000, Loss: 3.2268, Loss_eval: 3.4584, Learning Rate: 5.000000e-05\n",
      "Step 129100, Loss: 3.2042, Loss_eval: 3.4913, Learning Rate: 5.000000e-05\n",
      "Step 129200, Loss: 3.1167, Loss_eval: 3.4902, Learning Rate: 5.000000e-05\n",
      "Step 129300, Loss: 3.2039, Loss_eval: 3.4686, Learning Rate: 5.000000e-05\n",
      "Step 129400, Loss: 3.1424, Loss_eval: 3.4804, Learning Rate: 5.000000e-05\n",
      "Step 129500, Loss: 3.2034, Loss_eval: 3.4865, Learning Rate: 5.000000e-05\n",
      "Step 129600, Loss: 3.1651, Loss_eval: 3.4766, Learning Rate: 5.000000e-05\n",
      "Step 129700, Loss: 3.0968, Loss_eval: 3.5030, Learning Rate: 5.000000e-05\n",
      "Step 129800, Loss: 3.0951, Loss_eval: 3.4756, Learning Rate: 5.000000e-05\n",
      "Step 129900, Loss: 3.0332, Loss_eval: 3.4745, Learning Rate: 5.000000e-05\n",
      "Step 130000, Loss: 3.0723, Loss_eval: 3.4506, Learning Rate: 5.000000e-05\n",
      "Step 130100, Loss: 3.3319, Loss_eval: 3.5158, Learning Rate: 5.000000e-05\n",
      "Step 130200, Loss: 3.1073, Loss_eval: 3.4854, Learning Rate: 5.000000e-05\n",
      "Step 130300, Loss: 2.9527, Loss_eval: 3.4701, Learning Rate: 5.000000e-05\n",
      "Step 130400, Loss: 3.5332, Loss_eval: 3.4531, Learning Rate: 5.000000e-05\n",
      "Step 130500, Loss: 3.2279, Loss_eval: 3.4855, Learning Rate: 5.000000e-05\n",
      "Step 130600, Loss: 3.2107, Loss_eval: 3.4791, Learning Rate: 5.000000e-05\n",
      "Step 130700, Loss: 3.3706, Loss_eval: 3.4441, Learning Rate: 5.000000e-05\n",
      "Step 130800, Loss: 2.9830, Loss_eval: 3.4643, Learning Rate: 5.000000e-05\n",
      "Step 130900, Loss: 3.4183, Loss_eval: 3.4770, Learning Rate: 5.000000e-05\n",
      "Step 131000, Loss: 2.7917, Loss_eval: 3.4837, Learning Rate: 5.000000e-05\n",
      "Step 131100, Loss: 3.2755, Loss_eval: 3.4846, Learning Rate: 5.000000e-05\n",
      "Step 131200, Loss: 3.1030, Loss_eval: 3.4687, Learning Rate: 5.000000e-05\n",
      "Step 131300, Loss: 3.3394, Loss_eval: 3.4856, Learning Rate: 5.000000e-05\n",
      "Step 131400, Loss: 3.2396, Loss_eval: 3.4789, Learning Rate: 5.000000e-05\n",
      "Step 131500, Loss: 3.2085, Loss_eval: 3.4779, Learning Rate: 5.000000e-05\n",
      "Step 131600, Loss: 2.8664, Loss_eval: 3.4983, Learning Rate: 5.000000e-05\n",
      "Step 131700, Loss: 3.2163, Loss_eval: 3.4781, Learning Rate: 5.000000e-05\n",
      "Step 131800, Loss: 3.0317, Loss_eval: 3.4782, Learning Rate: 5.000000e-05\n",
      "Step 131900, Loss: 3.1737, Loss_eval: 3.4425, Learning Rate: 5.000000e-05\n",
      "Step 132000, Loss: 3.0903, Loss_eval: 3.4693, Learning Rate: 5.000000e-05\n",
      "Step 132100, Loss: 3.1106, Loss_eval: 3.4570, Learning Rate: 5.000000e-05\n",
      "Step 132200, Loss: 3.1102, Loss_eval: 3.4695, Learning Rate: 5.000000e-05\n",
      "Step 132300, Loss: 3.2853, Loss_eval: 3.4350, Learning Rate: 5.000000e-05\n",
      "Step 132400, Loss: 3.2364, Loss_eval: 3.4972, Learning Rate: 5.000000e-05\n",
      "Step 132500, Loss: 3.1544, Loss_eval: 3.4554, Learning Rate: 5.000000e-05\n",
      "Step 132600, Loss: 3.2428, Loss_eval: 3.4714, Learning Rate: 5.000000e-05\n",
      "Step 132700, Loss: 2.8210, Loss_eval: 3.4538, Learning Rate: 5.000000e-05\n",
      "Step 132800, Loss: 3.1395, Loss_eval: 3.4568, Learning Rate: 5.000000e-05\n",
      "Step 132900, Loss: 3.3915, Loss_eval: 3.4913, Learning Rate: 5.000000e-05\n",
      "Step 133000, Loss: 3.1242, Loss_eval: 3.4723, Learning Rate: 5.000000e-05\n",
      "Step 133100, Loss: 3.2179, Loss_eval: 3.4437, Learning Rate: 5.000000e-05\n",
      "Step 133200, Loss: 3.1842, Loss_eval: 3.4732, Learning Rate: 5.000000e-05\n",
      "Step 133300, Loss: 3.1452, Loss_eval: 3.4368, Learning Rate: 5.000000e-05\n",
      "Step 133400, Loss: 2.9167, Loss_eval: 3.4718, Learning Rate: 5.000000e-05\n",
      "Step 133500, Loss: 3.3196, Loss_eval: 3.4627, Learning Rate: 5.000000e-05\n",
      "Step 133600, Loss: 3.5641, Loss_eval: 3.4922, Learning Rate: 5.000000e-05\n",
      "Step 133700, Loss: 3.0697, Loss_eval: 3.4640, Learning Rate: 5.000000e-05\n",
      "Step 133800, Loss: 2.9036, Loss_eval: 3.5007, Learning Rate: 5.000000e-05\n",
      "Step 133900, Loss: 2.9796, Loss_eval: 3.4311, Learning Rate: 5.000000e-05\n",
      "Step 134000, Loss: 3.1244, Loss_eval: 3.4706, Learning Rate: 5.000000e-05\n",
      "Step 134100, Loss: 3.3730, Loss_eval: 3.4930, Learning Rate: 5.000000e-05\n",
      "Step 134200, Loss: 3.2973, Loss_eval: 3.4882, Learning Rate: 5.000000e-05\n",
      "Step 134300, Loss: 3.0295, Loss_eval: 3.4897, Learning Rate: 5.000000e-05\n",
      "Step 134400, Loss: 3.2945, Loss_eval: 3.4654, Learning Rate: 5.000000e-05\n",
      "Step 134500, Loss: 3.2795, Loss_eval: 3.4897, Learning Rate: 5.000000e-05\n",
      "Step 134600, Loss: 3.0325, Loss_eval: 3.4758, Learning Rate: 5.000000e-05\n",
      "Step 134700, Loss: 3.0593, Loss_eval: 3.4534, Learning Rate: 5.000000e-05\n",
      "Step 134800, Loss: 3.1731, Loss_eval: 3.4551, Learning Rate: 5.000000e-05\n",
      "Step 134900, Loss: 3.0295, Loss_eval: 3.4691, Learning Rate: 5.000000e-05\n",
      "Step 135000, Loss: 3.0064, Loss_eval: 3.4829, Learning Rate: 5.000000e-05\n",
      "Step 135100, Loss: 3.3138, Loss_eval: 3.4251, Learning Rate: 5.000000e-05\n",
      "Step 135200, Loss: 3.0518, Loss_eval: 3.4948, Learning Rate: 5.000000e-05\n",
      "Step 135300, Loss: 3.3461, Loss_eval: 3.4545, Learning Rate: 5.000000e-05\n",
      "Step 135400, Loss: 2.9119, Loss_eval: 3.4534, Learning Rate: 5.000000e-05\n",
      "Step 135500, Loss: 3.2262, Loss_eval: 3.4700, Learning Rate: 5.000000e-05\n",
      "Step 135600, Loss: 3.1630, Loss_eval: 3.4588, Learning Rate: 5.000000e-05\n",
      "Step 135700, Loss: 2.9734, Loss_eval: 3.4960, Learning Rate: 5.000000e-05\n",
      "Step 135800, Loss: 3.3401, Loss_eval: 3.4706, Learning Rate: 5.000000e-05\n",
      "Step 135900, Loss: 3.2867, Loss_eval: 3.4664, Learning Rate: 5.000000e-05\n",
      "Step 136000, Loss: 3.4030, Loss_eval: 3.4788, Learning Rate: 5.000000e-05\n",
      "Step 136100, Loss: 3.2791, Loss_eval: 3.4717, Learning Rate: 5.000000e-05\n",
      "Step 136200, Loss: 3.2667, Loss_eval: 3.4957, Learning Rate: 5.000000e-05\n",
      "Step 136300, Loss: 3.1106, Loss_eval: 3.4476, Learning Rate: 5.000000e-05\n",
      "Step 136400, Loss: 3.0369, Loss_eval: 3.4416, Learning Rate: 5.000000e-05\n",
      "Step 136500, Loss: 3.3541, Loss_eval: 3.4810, Learning Rate: 5.000000e-05\n",
      "Step 136600, Loss: 3.1774, Loss_eval: 3.4421, Learning Rate: 5.000000e-05\n",
      "Step 136700, Loss: 3.3007, Loss_eval: 3.4820, Learning Rate: 5.000000e-05\n",
      "Step 136800, Loss: 3.0853, Loss_eval: 3.4854, Learning Rate: 5.000000e-05\n",
      "Step 136900, Loss: 3.3929, Loss_eval: 3.4942, Learning Rate: 5.000000e-05\n",
      "Step 137000, Loss: 3.1884, Loss_eval: 3.4444, Learning Rate: 5.000000e-05\n",
      "Step 137100, Loss: 3.0567, Loss_eval: 3.4769, Learning Rate: 5.000000e-05\n",
      "Step 137200, Loss: 2.9315, Loss_eval: 3.4885, Learning Rate: 5.000000e-05\n",
      "Step 137300, Loss: 3.1793, Loss_eval: 3.4752, Learning Rate: 5.000000e-05\n",
      "Step 137400, Loss: 3.4761, Loss_eval: 3.4798, Learning Rate: 5.000000e-05\n",
      "Step 137500, Loss: 3.2685, Loss_eval: 3.4532, Learning Rate: 5.000000e-05\n",
      "Step 137600, Loss: 3.0642, Loss_eval: 3.4805, Learning Rate: 5.000000e-05\n",
      "Step 137700, Loss: 3.0793, Loss_eval: 3.4744, Learning Rate: 5.000000e-05\n",
      "Step 137800, Loss: 3.0820, Loss_eval: 3.4792, Learning Rate: 5.000000e-05\n",
      "Step 137900, Loss: 3.0213, Loss_eval: 3.4715, Learning Rate: 5.000000e-05\n",
      "Step 138000, Loss: 3.3060, Loss_eval: 3.4543, Learning Rate: 5.000000e-05\n",
      "Step 138100, Loss: 2.9038, Loss_eval: 3.4523, Learning Rate: 5.000000e-05\n",
      "Step 138200, Loss: 3.3396, Loss_eval: 3.4826, Learning Rate: 5.000000e-05\n",
      "Step 138300, Loss: 3.3825, Loss_eval: 3.4739, Learning Rate: 5.000000e-05\n",
      "Step 138400, Loss: 3.4606, Loss_eval: 3.4645, Learning Rate: 5.000000e-05\n",
      "Step 138500, Loss: 3.2754, Loss_eval: 3.4854, Learning Rate: 5.000000e-05\n",
      "Step 138600, Loss: 3.2541, Loss_eval: 3.4306, Learning Rate: 5.000000e-05\n",
      "Step 138700, Loss: 3.0774, Loss_eval: 3.4748, Learning Rate: 5.000000e-05\n",
      "Step 138800, Loss: 3.2259, Loss_eval: 3.4474, Learning Rate: 5.000000e-05\n",
      "Step 138900, Loss: 3.1852, Loss_eval: 3.4710, Learning Rate: 5.000000e-05\n",
      "Step 139000, Loss: 3.0088, Loss_eval: 3.4604, Learning Rate: 5.000000e-05\n",
      "Step 139100, Loss: 3.2559, Loss_eval: 3.4734, Learning Rate: 5.000000e-05\n",
      "Step 139200, Loss: 3.0894, Loss_eval: 3.4547, Learning Rate: 5.000000e-05\n",
      "Step 139300, Loss: 2.8766, Loss_eval: 3.4644, Learning Rate: 5.000000e-05\n",
      "Step 139400, Loss: 2.9359, Loss_eval: 3.4660, Learning Rate: 5.000000e-05\n",
      "Step 139500, Loss: 2.9527, Loss_eval: 3.4764, Learning Rate: 5.000000e-05\n",
      "Step 139600, Loss: 3.2670, Loss_eval: 3.4510, Learning Rate: 5.000000e-05\n",
      "Step 139700, Loss: 3.2241, Loss_eval: 3.4573, Learning Rate: 5.000000e-05\n",
      "Step 139800, Loss: 3.2975, Loss_eval: 3.4786, Learning Rate: 5.000000e-05\n",
      "Step 139900, Loss: 3.3454, Loss_eval: 3.5072, Learning Rate: 5.000000e-05\n",
      "Step 140000, Loss: 2.7804, Loss_eval: 3.4515, Learning Rate: 5.000000e-05\n",
      "Step 140100, Loss: 3.2938, Loss_eval: 3.4459, Learning Rate: 5.000000e-05\n",
      "Step 140200, Loss: 3.0279, Loss_eval: 3.4643, Learning Rate: 5.000000e-05\n",
      "Step 140300, Loss: 2.8033, Loss_eval: 3.4607, Learning Rate: 5.000000e-05\n",
      "Step 140400, Loss: 2.9240, Loss_eval: 3.5006, Learning Rate: 5.000000e-05\n",
      "Step 140500, Loss: 3.1200, Loss_eval: 3.4559, Learning Rate: 5.000000e-05\n",
      "Step 140600, Loss: 2.9476, Loss_eval: 3.4900, Learning Rate: 5.000000e-05\n",
      "Step 140700, Loss: 3.2554, Loss_eval: 3.4374, Learning Rate: 5.000000e-05\n",
      "Step 140800, Loss: 3.4690, Loss_eval: 3.5067, Learning Rate: 5.000000e-05\n",
      "Step 140900, Loss: 2.8795, Loss_eval: 3.4693, Learning Rate: 5.000000e-05\n",
      "Step 141000, Loss: 3.0509, Loss_eval: 3.4839, Learning Rate: 5.000000e-05\n",
      "Step 141100, Loss: 3.0588, Loss_eval: 3.4602, Learning Rate: 5.000000e-05\n",
      "Step 141200, Loss: 2.9347, Loss_eval: 3.4901, Learning Rate: 5.000000e-05\n",
      "Step 141300, Loss: 2.8283, Loss_eval: 3.4782, Learning Rate: 5.000000e-05\n",
      "Step 141400, Loss: 3.2060, Loss_eval: 3.4851, Learning Rate: 5.000000e-05\n",
      "Step 141500, Loss: 3.0479, Loss_eval: 3.4710, Learning Rate: 5.000000e-05\n",
      "Step 141600, Loss: 2.6337, Loss_eval: 3.5014, Learning Rate: 5.000000e-05\n",
      "Step 141700, Loss: 3.0523, Loss_eval: 3.4744, Learning Rate: 5.000000e-05\n",
      "Step 141800, Loss: 3.1892, Loss_eval: 3.4675, Learning Rate: 5.000000e-05\n",
      "Step 141900, Loss: 3.3608, Loss_eval: 3.4563, Learning Rate: 5.000000e-05\n",
      "Step 142000, Loss: 3.0871, Loss_eval: 3.4706, Learning Rate: 5.000000e-05\n",
      "Step 142100, Loss: 3.4014, Loss_eval: 3.4387, Learning Rate: 5.000000e-05\n",
      "Step 142200, Loss: 3.0126, Loss_eval: 3.4860, Learning Rate: 5.000000e-05\n",
      "Step 142300, Loss: 3.0392, Loss_eval: 3.4924, Learning Rate: 5.000000e-05\n",
      "Step 142400, Loss: 3.0772, Loss_eval: 3.4744, Learning Rate: 5.000000e-05\n",
      "Step 142500, Loss: 2.9002, Loss_eval: 3.4617, Learning Rate: 5.000000e-05\n",
      "Step 142600, Loss: 3.1810, Loss_eval: 3.4724, Learning Rate: 5.000000e-05\n",
      "Step 142700, Loss: 3.1016, Loss_eval: 3.4717, Learning Rate: 5.000000e-05\n",
      "Step 142800, Loss: 3.1657, Loss_eval: 3.4725, Learning Rate: 5.000000e-05\n",
      "Step 142900, Loss: 3.2569, Loss_eval: 3.4549, Learning Rate: 5.000000e-05\n",
      "Step 143000, Loss: 3.3600, Loss_eval: 3.4241, Learning Rate: 5.000000e-05\n",
      "Step 143100, Loss: 3.2826, Loss_eval: 3.4534, Learning Rate: 5.000000e-05\n",
      "Step 143200, Loss: 3.2706, Loss_eval: 3.4532, Learning Rate: 5.000000e-05\n",
      "Step 143300, Loss: 3.2525, Loss_eval: 3.4590, Learning Rate: 5.000000e-05\n",
      "Step 143400, Loss: 3.3065, Loss_eval: 3.4630, Learning Rate: 5.000000e-05\n",
      "Step 143500, Loss: 3.0048, Loss_eval: 3.4980, Learning Rate: 5.000000e-05\n",
      "Step 143600, Loss: 3.3981, Loss_eval: 3.4785, Learning Rate: 5.000000e-05\n",
      "Step 143700, Loss: 3.2577, Loss_eval: 3.4884, Learning Rate: 5.000000e-05\n",
      "Step 143800, Loss: 3.0780, Loss_eval: 3.4911, Learning Rate: 5.000000e-05\n",
      "Step 143900, Loss: 3.2454, Loss_eval: 3.4623, Learning Rate: 5.000000e-05\n",
      "Step 144000, Loss: 2.9251, Loss_eval: 3.4381, Learning Rate: 5.000000e-05\n",
      "Step 144100, Loss: 3.1036, Loss_eval: 3.4752, Learning Rate: 5.000000e-05\n",
      "Step 144200, Loss: 3.2586, Loss_eval: 3.5135, Learning Rate: 5.000000e-05\n",
      "Step 144300, Loss: 3.1124, Loss_eval: 3.4704, Learning Rate: 5.000000e-05\n",
      "Step 144400, Loss: 3.0406, Loss_eval: 3.4967, Learning Rate: 5.000000e-05\n",
      "Step 144500, Loss: 2.9364, Loss_eval: 3.4592, Learning Rate: 5.000000e-05\n",
      "Step 144600, Loss: 3.2006, Loss_eval: 3.4834, Learning Rate: 5.000000e-05\n",
      "Step 144700, Loss: 2.8275, Loss_eval: 3.4913, Learning Rate: 5.000000e-05\n",
      "Step 144800, Loss: 3.3598, Loss_eval: 3.4721, Learning Rate: 5.000000e-05\n",
      "Step 144900, Loss: 3.4053, Loss_eval: 3.4902, Learning Rate: 5.000000e-05\n",
      "Step 145000, Loss: 3.0014, Loss_eval: 3.4514, Learning Rate: 5.000000e-05\n",
      "Step 145100, Loss: 3.1197, Loss_eval: 3.4713, Learning Rate: 5.000000e-05\n",
      "Step 145200, Loss: 3.0450, Loss_eval: 3.4784, Learning Rate: 5.000000e-05\n",
      "Step 145300, Loss: 3.3224, Loss_eval: 3.4816, Learning Rate: 5.000000e-05\n",
      "Step 145400, Loss: 3.4870, Loss_eval: 3.4670, Learning Rate: 5.000000e-05\n",
      "Step 145500, Loss: 2.9959, Loss_eval: 3.4193, Learning Rate: 5.000000e-05\n",
      "Step 145600, Loss: 3.4192, Loss_eval: 3.4794, Learning Rate: 5.000000e-05\n",
      "Step 145700, Loss: 3.1399, Loss_eval: 3.4590, Learning Rate: 5.000000e-05\n",
      "Step 145800, Loss: 2.8308, Loss_eval: 3.4740, Learning Rate: 5.000000e-05\n",
      "Step 145900, Loss: 3.1068, Loss_eval: 3.4634, Learning Rate: 5.000000e-05\n",
      "Step 146000, Loss: 3.2747, Loss_eval: 3.4609, Learning Rate: 5.000000e-05\n",
      "Step 146100, Loss: 3.4441, Loss_eval: 3.4833, Learning Rate: 5.000000e-05\n",
      "Step 146200, Loss: 3.1290, Loss_eval: 3.4615, Learning Rate: 5.000000e-05\n",
      "Step 146300, Loss: 3.2396, Loss_eval: 3.4665, Learning Rate: 5.000000e-05\n",
      "Step 146400, Loss: 3.0918, Loss_eval: 3.4605, Learning Rate: 5.000000e-05\n",
      "Step 146500, Loss: 3.7561, Loss_eval: 3.4676, Learning Rate: 5.000000e-05\n",
      "Step 146600, Loss: 3.1340, Loss_eval: 3.4700, Learning Rate: 5.000000e-05\n",
      "Step 146700, Loss: 3.3016, Loss_eval: 3.4824, Learning Rate: 5.000000e-05\n",
      "Step 146800, Loss: 3.1830, Loss_eval: 3.4453, Learning Rate: 5.000000e-05\n",
      "Step 146900, Loss: 3.1091, Loss_eval: 3.4704, Learning Rate: 5.000000e-05\n",
      "Step 147000, Loss: 2.9498, Loss_eval: 3.4542, Learning Rate: 5.000000e-05\n",
      "Step 147100, Loss: 3.1496, Loss_eval: 3.4869, Learning Rate: 5.000000e-05\n",
      "Step 147200, Loss: 3.0973, Loss_eval: 3.4703, Learning Rate: 5.000000e-05\n",
      "Step 147300, Loss: 3.5530, Loss_eval: 3.5016, Learning Rate: 5.000000e-05\n",
      "Step 147400, Loss: 3.2020, Loss_eval: 3.4859, Learning Rate: 5.000000e-05\n",
      "Step 147500, Loss: 2.8913, Loss_eval: 3.4632, Learning Rate: 5.000000e-05\n",
      "Step 147600, Loss: 2.9649, Loss_eval: 3.4608, Learning Rate: 5.000000e-05\n",
      "Step 147700, Loss: 3.0026, Loss_eval: 3.4720, Learning Rate: 5.000000e-05\n",
      "Step 147800, Loss: 3.4096, Loss_eval: 3.4665, Learning Rate: 5.000000e-05\n",
      "Step 147900, Loss: 3.2883, Loss_eval: 3.4826, Learning Rate: 5.000000e-05\n",
      "Step 148000, Loss: 3.0746, Loss_eval: 3.4778, Learning Rate: 5.000000e-05\n",
      "Step 148100, Loss: 3.1920, Loss_eval: 3.5031, Learning Rate: 5.000000e-05\n",
      "Step 148200, Loss: 3.1423, Loss_eval: 3.4620, Learning Rate: 5.000000e-05\n",
      "Step 148300, Loss: 3.0683, Loss_eval: 3.4569, Learning Rate: 5.000000e-05\n",
      "Step 148400, Loss: 2.8751, Loss_eval: 3.4635, Learning Rate: 5.000000e-05\n",
      "Step 148500, Loss: 3.1054, Loss_eval: 3.4800, Learning Rate: 5.000000e-05\n",
      "Step 148600, Loss: 3.5615, Loss_eval: 3.4435, Learning Rate: 5.000000e-05\n",
      "Step 148700, Loss: 2.9921, Loss_eval: 3.4902, Learning Rate: 5.000000e-05\n",
      "Step 148800, Loss: 2.9126, Loss_eval: 3.4544, Learning Rate: 5.000000e-05\n",
      "Step 148900, Loss: 3.1013, Loss_eval: 3.4492, Learning Rate: 5.000000e-05\n",
      "Step 149000, Loss: 3.2607, Loss_eval: 3.4681, Learning Rate: 5.000000e-05\n",
      "Step 149100, Loss: 3.2015, Loss_eval: 3.4932, Learning Rate: 5.000000e-05\n",
      "Step 149200, Loss: 3.4567, Loss_eval: 3.4827, Learning Rate: 5.000000e-05\n",
      "Step 149300, Loss: 3.1294, Loss_eval: 3.4408, Learning Rate: 5.000000e-05\n",
      "Step 149400, Loss: 3.0180, Loss_eval: 3.4981, Learning Rate: 5.000000e-05\n",
      "Step 149500, Loss: 3.1251, Loss_eval: 3.5048, Learning Rate: 5.000000e-05\n",
      "Step 149600, Loss: 3.0336, Loss_eval: 3.4612, Learning Rate: 5.000000e-05\n",
      "Step 149700, Loss: 3.3072, Loss_eval: 3.4120, Learning Rate: 5.000000e-05\n",
      "Step 149800, Loss: 2.8603, Loss_eval: 3.4362, Learning Rate: 5.000000e-05\n",
      "Step 149900, Loss: 3.4124, Loss_eval: 3.4868, Learning Rate: 5.000000e-05\n",
      "Step 150000, Loss: 2.8642, Loss_eval: 3.4433, Learning Rate: 5.000000e-05\n",
      "Step 150100, Loss: 3.2416, Loss_eval: 3.4691, Learning Rate: 5.000000e-05\n",
      "Step 150200, Loss: 3.2254, Loss_eval: 3.4930, Learning Rate: 5.000000e-05\n",
      "Step 150300, Loss: 2.9175, Loss_eval: 3.4558, Learning Rate: 5.000000e-05\n",
      "Step 150400, Loss: 2.8715, Loss_eval: 3.4397, Learning Rate: 5.000000e-05\n",
      "Step 150500, Loss: 3.0944, Loss_eval: 3.4568, Learning Rate: 5.000000e-05\n",
      "Step 150600, Loss: 3.0909, Loss_eval: 3.4725, Learning Rate: 5.000000e-05\n",
      "Step 150700, Loss: 3.1423, Loss_eval: 3.4620, Learning Rate: 5.000000e-05\n",
      "Step 150800, Loss: 3.0569, Loss_eval: 3.4812, Learning Rate: 5.000000e-05\n",
      "Step 150900, Loss: 2.8299, Loss_eval: 3.4527, Learning Rate: 5.000000e-05\n",
      "Step 151000, Loss: 2.9855, Loss_eval: 3.4598, Learning Rate: 5.000000e-05\n",
      "Step 151100, Loss: 3.0487, Loss_eval: 3.4677, Learning Rate: 5.000000e-05\n",
      "Step 151200, Loss: 2.8913, Loss_eval: 3.4797, Learning Rate: 5.000000e-05\n",
      "Step 151300, Loss: 3.0268, Loss_eval: 3.4326, Learning Rate: 5.000000e-05\n",
      "Step 151400, Loss: 3.0210, Loss_eval: 3.4414, Learning Rate: 5.000000e-05\n",
      "Step 151500, Loss: 3.4118, Loss_eval: 3.4367, Learning Rate: 5.000000e-05\n",
      "Step 151600, Loss: 3.4806, Loss_eval: 3.4312, Learning Rate: 5.000000e-05\n",
      "Step 151700, Loss: 2.9803, Loss_eval: 3.4849, Learning Rate: 5.000000e-05\n",
      "Step 151800, Loss: 3.3143, Loss_eval: 3.4472, Learning Rate: 5.000000e-05\n",
      "Step 151900, Loss: 2.9210, Loss_eval: 3.4784, Learning Rate: 5.000000e-05\n",
      "Step 152000, Loss: 3.0916, Loss_eval: 3.4617, Learning Rate: 5.000000e-05\n",
      "Step 152100, Loss: 3.6109, Loss_eval: 3.4612, Learning Rate: 5.000000e-05\n",
      "Step 152200, Loss: 3.0171, Loss_eval: 3.4527, Learning Rate: 5.000000e-05\n",
      "Step 152300, Loss: 3.1822, Loss_eval: 3.4737, Learning Rate: 5.000000e-05\n",
      "Step 152400, Loss: 3.1637, Loss_eval: 3.4366, Learning Rate: 5.000000e-05\n",
      "Step 152500, Loss: 3.2923, Loss_eval: 3.4451, Learning Rate: 5.000000e-05\n",
      "Step 152600, Loss: 2.7394, Loss_eval: 3.4361, Learning Rate: 5.000000e-05\n",
      "Step 152700, Loss: 3.0686, Loss_eval: 3.4911, Learning Rate: 5.000000e-05\n",
      "Step 152800, Loss: 3.4971, Loss_eval: 3.4526, Learning Rate: 5.000000e-05\n",
      "Step 152900, Loss: 3.0828, Loss_eval: 3.4804, Learning Rate: 5.000000e-05\n",
      "Step 153000, Loss: 2.8575, Loss_eval: 3.4497, Learning Rate: 5.000000e-05\n",
      "Step 153100, Loss: 3.4324, Loss_eval: 3.4729, Learning Rate: 5.000000e-05\n",
      "Step 153200, Loss: 3.1665, Loss_eval: 3.4621, Learning Rate: 5.000000e-05\n",
      "Step 153300, Loss: 3.3793, Loss_eval: 3.4590, Learning Rate: 5.000000e-05\n",
      "Step 153400, Loss: 3.1300, Loss_eval: 3.4588, Learning Rate: 5.000000e-05\n",
      "Step 153500, Loss: 2.9240, Loss_eval: 3.4835, Learning Rate: 5.000000e-05\n",
      "Step 153600, Loss: 3.1015, Loss_eval: 3.4993, Learning Rate: 5.000000e-05\n",
      "Step 153700, Loss: 3.1051, Loss_eval: 3.4591, Learning Rate: 5.000000e-05\n",
      "Step 153800, Loss: 3.2079, Loss_eval: 3.4911, Learning Rate: 5.000000e-05\n",
      "Step 153900, Loss: 3.0655, Loss_eval: 3.4757, Learning Rate: 5.000000e-05\n",
      "Step 154000, Loss: 2.9775, Loss_eval: 3.4846, Learning Rate: 5.000000e-05\n",
      "Step 154100, Loss: 3.2240, Loss_eval: 3.4291, Learning Rate: 5.000000e-05\n",
      "Step 154200, Loss: 3.0559, Loss_eval: 3.4644, Learning Rate: 5.000000e-05\n",
      "Step 154300, Loss: 3.2194, Loss_eval: 3.4721, Learning Rate: 5.000000e-05\n",
      "Step 154400, Loss: 2.9475, Loss_eval: 3.4545, Learning Rate: 5.000000e-05\n",
      "Step 154500, Loss: 3.2528, Loss_eval: 3.4312, Learning Rate: 5.000000e-05\n",
      "Step 154600, Loss: 3.1212, Loss_eval: 3.4696, Learning Rate: 5.000000e-05\n",
      "Step 154700, Loss: 2.9639, Loss_eval: 3.4661, Learning Rate: 5.000000e-05\n",
      "Step 154800, Loss: 3.0631, Loss_eval: 3.4468, Learning Rate: 5.000000e-05\n",
      "Step 154900, Loss: 3.0476, Loss_eval: 3.4679, Learning Rate: 5.000000e-05\n",
      "Step 155000, Loss: 2.9428, Loss_eval: 3.4644, Learning Rate: 5.000000e-05\n",
      "Step 155100, Loss: 3.3540, Loss_eval: 3.4504, Learning Rate: 5.000000e-05\n",
      "Step 155200, Loss: 3.2334, Loss_eval: 3.4540, Learning Rate: 5.000000e-05\n",
      "Step 155300, Loss: 3.0697, Loss_eval: 3.4631, Learning Rate: 5.000000e-05\n",
      "Step 155400, Loss: 3.2773, Loss_eval: 3.4748, Learning Rate: 5.000000e-05\n",
      "Step 155500, Loss: 3.0579, Loss_eval: 3.4954, Learning Rate: 5.000000e-05\n",
      "Step 155600, Loss: 3.2906, Loss_eval: 3.4784, Learning Rate: 5.000000e-05\n",
      "Step 155700, Loss: 2.9474, Loss_eval: 3.4858, Learning Rate: 5.000000e-05\n",
      "Step 155800, Loss: 3.1188, Loss_eval: 3.4756, Learning Rate: 5.000000e-05\n",
      "Step 155900, Loss: 3.2637, Loss_eval: 3.4443, Learning Rate: 5.000000e-05\n",
      "Step 156000, Loss: 3.0965, Loss_eval: 3.4687, Learning Rate: 5.000000e-05\n",
      "Step 156100, Loss: 3.1005, Loss_eval: 3.4637, Learning Rate: 5.000000e-05\n",
      "Step 156200, Loss: 3.1938, Loss_eval: 3.4361, Learning Rate: 5.000000e-05\n",
      "Step 156300, Loss: 3.0840, Loss_eval: 3.4533, Learning Rate: 5.000000e-05\n",
      "Step 156400, Loss: 3.2164, Loss_eval: 3.4452, Learning Rate: 5.000000e-05\n",
      "Step 156500, Loss: 3.1838, Loss_eval: 3.4412, Learning Rate: 5.000000e-05\n",
      "Step 156600, Loss: 3.1911, Loss_eval: 3.4485, Learning Rate: 5.000000e-05\n",
      "Step 156700, Loss: 3.0418, Loss_eval: 3.4715, Learning Rate: 5.000000e-05\n",
      "Step 156800, Loss: 3.0798, Loss_eval: 3.4395, Learning Rate: 5.000000e-05\n",
      "Step 156900, Loss: 2.9370, Loss_eval: 3.4586, Learning Rate: 5.000000e-05\n",
      "Step 157000, Loss: 3.3349, Loss_eval: 3.4739, Learning Rate: 5.000000e-05\n",
      "Step 157100, Loss: 3.0888, Loss_eval: 3.4308, Learning Rate: 5.000000e-05\n",
      "Step 157200, Loss: 2.9504, Loss_eval: 3.4384, Learning Rate: 5.000000e-05\n",
      "Step 157300, Loss: 3.0994, Loss_eval: 3.4817, Learning Rate: 5.000000e-05\n",
      "Step 157400, Loss: 3.1832, Loss_eval: 3.4474, Learning Rate: 5.000000e-05\n",
      "Step 157500, Loss: 3.3027, Loss_eval: 3.4827, Learning Rate: 5.000000e-05\n",
      "Step 157600, Loss: 2.8815, Loss_eval: 3.4512, Learning Rate: 5.000000e-05\n",
      "Step 157700, Loss: 3.2333, Loss_eval: 3.4425, Learning Rate: 5.000000e-05\n",
      "Step 157800, Loss: 3.0965, Loss_eval: 3.4661, Learning Rate: 5.000000e-05\n",
      "Step 157900, Loss: 3.0769, Loss_eval: 3.4436, Learning Rate: 5.000000e-05\n",
      "Step 158000, Loss: 3.1804, Loss_eval: 3.4577, Learning Rate: 5.000000e-05\n",
      "Step 158100, Loss: 3.3675, Loss_eval: 3.4409, Learning Rate: 5.000000e-05\n",
      "Step 158200, Loss: 2.8144, Loss_eval: 3.4564, Learning Rate: 5.000000e-05\n",
      "Step 158300, Loss: 3.0176, Loss_eval: 3.4537, Learning Rate: 5.000000e-05\n",
      "Step 158400, Loss: 3.0634, Loss_eval: 3.4324, Learning Rate: 5.000000e-05\n",
      "Step 158500, Loss: 3.2127, Loss_eval: 3.5074, Learning Rate: 5.000000e-05\n",
      "Step 158600, Loss: 3.0874, Loss_eval: 3.4498, Learning Rate: 5.000000e-05\n",
      "Step 158700, Loss: 3.0654, Loss_eval: 3.4292, Learning Rate: 5.000000e-05\n",
      "Step 158800, Loss: 3.0292, Loss_eval: 3.4235, Learning Rate: 5.000000e-05\n",
      "Step 158900, Loss: 3.1628, Loss_eval: 3.4588, Learning Rate: 5.000000e-05\n",
      "Step 159000, Loss: 3.2859, Loss_eval: 3.4492, Learning Rate: 5.000000e-05\n",
      "Step 159100, Loss: 2.8833, Loss_eval: 3.4689, Learning Rate: 5.000000e-05\n",
      "Step 159200, Loss: 3.1806, Loss_eval: 3.4689, Learning Rate: 5.000000e-05\n",
      "Step 159300, Loss: 2.9329, Loss_eval: 3.4536, Learning Rate: 5.000000e-05\n",
      "Step 159400, Loss: 3.2461, Loss_eval: 3.4677, Learning Rate: 5.000000e-05\n",
      "Step 159500, Loss: 2.9476, Loss_eval: 3.4633, Learning Rate: 5.000000e-05\n",
      "Step 159600, Loss: 3.0128, Loss_eval: 3.4590, Learning Rate: 5.000000e-05\n",
      "Step 159700, Loss: 3.4916, Loss_eval: 3.4497, Learning Rate: 5.000000e-05\n",
      "Step 159800, Loss: 2.6888, Loss_eval: 3.4352, Learning Rate: 5.000000e-05\n",
      "Step 159900, Loss: 3.4205, Loss_eval: 3.4450, Learning Rate: 5.000000e-05\n",
      "Step 160000, Loss: 2.9568, Loss_eval: 3.4823, Learning Rate: 5.000000e-05\n",
      "Step 160100, Loss: 3.0025, Loss_eval: 3.4668, Learning Rate: 5.000000e-05\n",
      "Step 160200, Loss: 3.2407, Loss_eval: 3.4654, Learning Rate: 5.000000e-05\n",
      "Step 160300, Loss: 2.9327, Loss_eval: 3.4548, Learning Rate: 5.000000e-05\n",
      "Step 160400, Loss: 3.2047, Loss_eval: 3.4562, Learning Rate: 5.000000e-05\n",
      "Step 160500, Loss: 3.2245, Loss_eval: 3.4508, Learning Rate: 5.000000e-05\n",
      "Step 160600, Loss: 3.3524, Loss_eval: 3.4533, Learning Rate: 5.000000e-05\n",
      "Step 160700, Loss: 3.2066, Loss_eval: 3.4794, Learning Rate: 5.000000e-05\n",
      "Step 160800, Loss: 3.0451, Loss_eval: 3.4822, Learning Rate: 5.000000e-05\n",
      "Step 160900, Loss: 3.0488, Loss_eval: 3.4630, Learning Rate: 5.000000e-05\n",
      "Step 161000, Loss: 3.0715, Loss_eval: 3.4793, Learning Rate: 5.000000e-05\n",
      "Step 161100, Loss: 3.1133, Loss_eval: 3.4531, Learning Rate: 5.000000e-05\n",
      "Step 161200, Loss: 3.2750, Loss_eval: 3.4762, Learning Rate: 5.000000e-05\n",
      "Step 161300, Loss: 3.1402, Loss_eval: 3.5042, Learning Rate: 5.000000e-05\n",
      "Step 161400, Loss: 3.0793, Loss_eval: 3.4437, Learning Rate: 5.000000e-05\n",
      "Step 161500, Loss: 3.1839, Loss_eval: 3.4498, Learning Rate: 5.000000e-05\n",
      "Step 161600, Loss: 3.3088, Loss_eval: 3.4537, Learning Rate: 5.000000e-05\n",
      "Step 161700, Loss: 3.3357, Loss_eval: 3.4962, Learning Rate: 5.000000e-05\n",
      "Step 161800, Loss: 2.9704, Loss_eval: 3.4398, Learning Rate: 5.000000e-05\n",
      "Step 161900, Loss: 3.1655, Loss_eval: 3.4707, Learning Rate: 5.000000e-05\n",
      "Step 162000, Loss: 3.3544, Loss_eval: 3.4542, Learning Rate: 5.000000e-05\n",
      "Step 162100, Loss: 3.0184, Loss_eval: 3.4588, Learning Rate: 5.000000e-05\n",
      "Step 162200, Loss: 3.4176, Loss_eval: 3.4405, Learning Rate: 5.000000e-05\n",
      "Step 162300, Loss: 3.1767, Loss_eval: 3.4764, Learning Rate: 5.000000e-05\n",
      "Step 162400, Loss: 3.1904, Loss_eval: 3.4613, Learning Rate: 5.000000e-05\n",
      "Step 162500, Loss: 2.8638, Loss_eval: 3.4591, Learning Rate: 5.000000e-05\n",
      "Step 162600, Loss: 3.0388, Loss_eval: 3.4874, Learning Rate: 5.000000e-05\n",
      "Step 162700, Loss: 3.4603, Loss_eval: 3.4973, Learning Rate: 5.000000e-05\n",
      "Step 162800, Loss: 3.1085, Loss_eval: 3.4595, Learning Rate: 5.000000e-05\n",
      "Step 162900, Loss: 3.2336, Loss_eval: 3.4617, Learning Rate: 5.000000e-05\n",
      "Step 163000, Loss: 3.3333, Loss_eval: 3.4477, Learning Rate: 5.000000e-05\n",
      "Step 163100, Loss: 3.2490, Loss_eval: 3.4310, Learning Rate: 5.000000e-05\n",
      "Step 163200, Loss: 2.9522, Loss_eval: 3.4671, Learning Rate: 5.000000e-05\n",
      "Step 163300, Loss: 3.0630, Loss_eval: 3.4852, Learning Rate: 5.000000e-05\n",
      "Step 163400, Loss: 3.2940, Loss_eval: 3.4791, Learning Rate: 5.000000e-05\n",
      "Step 163500, Loss: 3.2350, Loss_eval: 3.4492, Learning Rate: 5.000000e-05\n",
      "Step 163600, Loss: 3.3098, Loss_eval: 3.4836, Learning Rate: 5.000000e-05\n",
      "Step 163700, Loss: 3.3188, Loss_eval: 3.4937, Learning Rate: 5.000000e-05\n",
      "Step 163800, Loss: 3.0336, Loss_eval: 3.4632, Learning Rate: 5.000000e-05\n",
      "Step 163900, Loss: 3.2523, Loss_eval: 3.4665, Learning Rate: 5.000000e-05\n",
      "Step 164000, Loss: 3.1641, Loss_eval: 3.4486, Learning Rate: 5.000000e-05\n",
      "Step 164100, Loss: 3.1216, Loss_eval: 3.4529, Learning Rate: 5.000000e-05\n",
      "Step 164200, Loss: 2.9434, Loss_eval: 3.4602, Learning Rate: 5.000000e-05\n",
      "Step 164300, Loss: 3.1839, Loss_eval: 3.4828, Learning Rate: 5.000000e-05\n",
      "Step 164400, Loss: 3.4197, Loss_eval: 3.4782, Learning Rate: 5.000000e-05\n",
      "Step 164500, Loss: 3.3071, Loss_eval: 3.4312, Learning Rate: 5.000000e-05\n",
      "Step 164600, Loss: 3.3792, Loss_eval: 3.4268, Learning Rate: 5.000000e-05\n",
      "Step 164700, Loss: 3.5220, Loss_eval: 3.4431, Learning Rate: 5.000000e-05\n",
      "Step 164800, Loss: 3.1872, Loss_eval: 3.4373, Learning Rate: 5.000000e-05\n",
      "Step 164900, Loss: 3.1517, Loss_eval: 3.4519, Learning Rate: 5.000000e-05\n",
      "Step 165000, Loss: 2.9596, Loss_eval: 3.4362, Learning Rate: 5.000000e-05\n",
      "Step 165100, Loss: 2.8356, Loss_eval: 3.4815, Learning Rate: 5.000000e-05\n",
      "Step 165200, Loss: 3.0414, Loss_eval: 3.4497, Learning Rate: 5.000000e-05\n",
      "Step 165300, Loss: 2.9458, Loss_eval: 3.4759, Learning Rate: 5.000000e-05\n",
      "Step 165400, Loss: 3.1638, Loss_eval: 3.4438, Learning Rate: 5.000000e-05\n",
      "Step 165500, Loss: 3.3136, Loss_eval: 3.4537, Learning Rate: 5.000000e-05\n",
      "Step 165600, Loss: 3.1649, Loss_eval: 3.4593, Learning Rate: 5.000000e-05\n",
      "Step 165700, Loss: 3.2610, Loss_eval: 3.4885, Learning Rate: 5.000000e-05\n",
      "Step 165800, Loss: 3.1524, Loss_eval: 3.4385, Learning Rate: 5.000000e-05\n",
      "Step 165900, Loss: 3.0389, Loss_eval: 3.4521, Learning Rate: 5.000000e-05\n",
      "Step 166000, Loss: 3.0827, Loss_eval: 3.4519, Learning Rate: 5.000000e-05\n",
      "Step 166100, Loss: 3.2217, Loss_eval: 3.4407, Learning Rate: 5.000000e-05\n",
      "Step 166200, Loss: 3.0442, Loss_eval: 3.4334, Learning Rate: 5.000000e-05\n",
      "Step 166300, Loss: 2.7955, Loss_eval: 3.4752, Learning Rate: 5.000000e-05\n",
      "Step 166400, Loss: 3.1873, Loss_eval: 3.4491, Learning Rate: 5.000000e-05\n",
      "Step 166500, Loss: 2.9414, Loss_eval: 3.4763, Learning Rate: 5.000000e-05\n",
      "Step 166600, Loss: 3.1984, Loss_eval: 3.4498, Learning Rate: 5.000000e-05\n",
      "Step 166700, Loss: 3.3109, Loss_eval: 3.4399, Learning Rate: 5.000000e-05\n",
      "Step 166800, Loss: 3.2596, Loss_eval: 3.4577, Learning Rate: 5.000000e-05\n",
      "Step 166900, Loss: 3.4433, Loss_eval: 3.4349, Learning Rate: 5.000000e-05\n",
      "Step 167000, Loss: 2.8163, Loss_eval: 3.4736, Learning Rate: 5.000000e-05\n",
      "Step 167100, Loss: 3.4062, Loss_eval: 3.4535, Learning Rate: 5.000000e-05\n",
      "Step 167200, Loss: 2.9262, Loss_eval: 3.4637, Learning Rate: 5.000000e-05\n",
      "Step 167300, Loss: 3.2504, Loss_eval: 3.4633, Learning Rate: 5.000000e-05\n",
      "Step 167400, Loss: 3.1562, Loss_eval: 3.4466, Learning Rate: 5.000000e-05\n",
      "Step 167500, Loss: 3.2694, Loss_eval: 3.4596, Learning Rate: 5.000000e-05\n",
      "Step 167600, Loss: 3.5809, Loss_eval: 3.4808, Learning Rate: 5.000000e-05\n",
      "Step 167700, Loss: 3.2796, Loss_eval: 3.4343, Learning Rate: 5.000000e-05\n",
      "Step 167800, Loss: 3.0966, Loss_eval: 3.4733, Learning Rate: 5.000000e-05\n",
      "Step 167900, Loss: 3.1031, Loss_eval: 3.4368, Learning Rate: 5.000000e-05\n",
      "Step 168000, Loss: 3.3371, Loss_eval: 3.4487, Learning Rate: 5.000000e-05\n",
      "Step 168100, Loss: 3.3031, Loss_eval: 3.4638, Learning Rate: 5.000000e-05\n",
      "Step 168200, Loss: 3.1037, Loss_eval: 3.4344, Learning Rate: 5.000000e-05\n",
      "Step 168300, Loss: 3.0476, Loss_eval: 3.4551, Learning Rate: 5.000000e-05\n",
      "Step 168400, Loss: 3.1471, Loss_eval: 3.4326, Learning Rate: 5.000000e-05\n",
      "Step 168500, Loss: 2.9462, Loss_eval: 3.4419, Learning Rate: 5.000000e-05\n",
      "Step 168600, Loss: 3.1040, Loss_eval: 3.4650, Learning Rate: 5.000000e-05\n",
      "Step 168700, Loss: 3.0146, Loss_eval: 3.4538, Learning Rate: 5.000000e-05\n",
      "Step 168800, Loss: 3.0332, Loss_eval: 3.4871, Learning Rate: 5.000000e-05\n",
      "Step 168900, Loss: 3.0083, Loss_eval: 3.4164, Learning Rate: 5.000000e-05\n",
      "Step 169000, Loss: 2.9474, Loss_eval: 3.4550, Learning Rate: 5.000000e-05\n",
      "Step 169100, Loss: 2.9001, Loss_eval: 3.4479, Learning Rate: 5.000000e-05\n",
      "Step 169200, Loss: 2.9610, Loss_eval: 3.4727, Learning Rate: 5.000000e-05\n",
      "Step 169300, Loss: 2.9197, Loss_eval: 3.4721, Learning Rate: 5.000000e-05\n",
      "Step 169400, Loss: 2.8171, Loss_eval: 3.4717, Learning Rate: 5.000000e-05\n",
      "Step 169500, Loss: 3.1712, Loss_eval: 3.4560, Learning Rate: 5.000000e-05\n",
      "Step 169600, Loss: 3.2008, Loss_eval: 3.4900, Learning Rate: 5.000000e-05\n",
      "Step 169700, Loss: 3.1610, Loss_eval: 3.4731, Learning Rate: 5.000000e-05\n",
      "Step 169800, Loss: 3.0832, Loss_eval: 3.4643, Learning Rate: 5.000000e-05\n",
      "Step 169900, Loss: 3.3728, Loss_eval: 3.4429, Learning Rate: 5.000000e-05\n",
      "Step 170000, Loss: 3.1636, Loss_eval: 3.4280, Learning Rate: 5.000000e-05\n",
      "Step 170100, Loss: 3.3614, Loss_eval: 3.4699, Learning Rate: 5.000000e-05\n",
      "Step 170200, Loss: 3.0000, Loss_eval: 3.4547, Learning Rate: 5.000000e-05\n",
      "Step 170300, Loss: 3.1930, Loss_eval: 3.4534, Learning Rate: 5.000000e-05\n",
      "Step 170400, Loss: 3.4806, Loss_eval: 3.5064, Learning Rate: 5.000000e-05\n",
      "Step 170500, Loss: 3.1011, Loss_eval: 3.4802, Learning Rate: 5.000000e-05\n",
      "Step 170600, Loss: 3.3516, Loss_eval: 3.4772, Learning Rate: 5.000000e-05\n",
      "Step 170700, Loss: 3.1676, Loss_eval: 3.4885, Learning Rate: 5.000000e-05\n",
      "Step 170800, Loss: 3.0241, Loss_eval: 3.4658, Learning Rate: 5.000000e-05\n",
      "Step 170900, Loss: 3.4132, Loss_eval: 3.4659, Learning Rate: 5.000000e-05\n",
      "Step 171000, Loss: 2.8800, Loss_eval: 3.4773, Learning Rate: 5.000000e-05\n",
      "Step 171100, Loss: 3.0448, Loss_eval: 3.4331, Learning Rate: 5.000000e-05\n",
      "Step 171200, Loss: 3.1660, Loss_eval: 3.4824, Learning Rate: 5.000000e-05\n",
      "Step 171300, Loss: 3.3689, Loss_eval: 3.4587, Learning Rate: 5.000000e-05\n",
      "Step 171400, Loss: 2.8934, Loss_eval: 3.4705, Learning Rate: 5.000000e-05\n",
      "Step 171500, Loss: 3.2949, Loss_eval: 3.4453, Learning Rate: 5.000000e-05\n",
      "Step 171600, Loss: 2.8519, Loss_eval: 3.4749, Learning Rate: 5.000000e-05\n",
      "Step 171700, Loss: 3.5220, Loss_eval: 3.4887, Learning Rate: 5.000000e-05\n",
      "Step 171800, Loss: 3.2513, Loss_eval: 3.4543, Learning Rate: 5.000000e-05\n",
      "Step 171900, Loss: 3.2106, Loss_eval: 3.4437, Learning Rate: 5.000000e-05\n",
      "Step 172000, Loss: 2.7844, Loss_eval: 3.4742, Learning Rate: 5.000000e-05\n",
      "Step 172100, Loss: 2.9505, Loss_eval: 3.4939, Learning Rate: 5.000000e-05\n",
      "Step 172200, Loss: 3.1156, Loss_eval: 3.4524, Learning Rate: 5.000000e-05\n",
      "Step 172300, Loss: 2.9293, Loss_eval: 3.4651, Learning Rate: 5.000000e-05\n",
      "Step 172400, Loss: 2.9032, Loss_eval: 3.4604, Learning Rate: 5.000000e-05\n",
      "Step 172500, Loss: 2.8769, Loss_eval: 3.4592, Learning Rate: 5.000000e-05\n",
      "Step 172600, Loss: 3.0734, Loss_eval: 3.4750, Learning Rate: 5.000000e-05\n",
      "Step 172700, Loss: 2.9844, Loss_eval: 3.4474, Learning Rate: 5.000000e-05\n",
      "Step 172800, Loss: 3.2725, Loss_eval: 3.4460, Learning Rate: 5.000000e-05\n",
      "Step 172900, Loss: 3.2451, Loss_eval: 3.4453, Learning Rate: 5.000000e-05\n",
      "Step 173000, Loss: 3.1951, Loss_eval: 3.4299, Learning Rate: 5.000000e-05\n",
      "Step 173100, Loss: 3.2227, Loss_eval: 3.4272, Learning Rate: 5.000000e-05\n",
      "Step 173200, Loss: 3.0364, Loss_eval: 3.4664, Learning Rate: 5.000000e-05\n",
      "Step 173300, Loss: 3.2887, Loss_eval: 3.4325, Learning Rate: 5.000000e-05\n",
      "Step 173400, Loss: 3.1888, Loss_eval: 3.4720, Learning Rate: 5.000000e-05\n",
      "Step 173500, Loss: 3.2052, Loss_eval: 3.4686, Learning Rate: 5.000000e-05\n",
      "Step 173600, Loss: 3.6169, Loss_eval: 3.4381, Learning Rate: 5.000000e-05\n",
      "Step 173700, Loss: 3.3251, Loss_eval: 3.4394, Learning Rate: 5.000000e-05\n",
      "Step 173800, Loss: 3.1721, Loss_eval: 3.4336, Learning Rate: 5.000000e-05\n",
      "Step 173900, Loss: 2.8331, Loss_eval: 3.4616, Learning Rate: 5.000000e-05\n",
      "Step 174000, Loss: 3.2016, Loss_eval: 3.4412, Learning Rate: 5.000000e-05\n",
      "Step 174100, Loss: 3.0307, Loss_eval: 3.4664, Learning Rate: 5.000000e-05\n",
      "Step 174200, Loss: 2.9675, Loss_eval: 3.4671, Learning Rate: 5.000000e-05\n",
      "Step 174300, Loss: 3.2250, Loss_eval: 3.4564, Learning Rate: 5.000000e-05\n",
      "Step 174400, Loss: 2.8290, Loss_eval: 3.4623, Learning Rate: 5.000000e-05\n",
      "Step 174500, Loss: 3.2383, Loss_eval: 3.4933, Learning Rate: 5.000000e-05\n",
      "Step 174600, Loss: 3.1017, Loss_eval: 3.4440, Learning Rate: 5.000000e-05\n",
      "Step 174700, Loss: 3.1283, Loss_eval: 3.4566, Learning Rate: 5.000000e-05\n",
      "Step 174800, Loss: 2.9819, Loss_eval: 3.4534, Learning Rate: 5.000000e-05\n",
      "Step 174900, Loss: 2.9223, Loss_eval: 3.4746, Learning Rate: 5.000000e-05\n",
      "Step 175000, Loss: 2.8179, Loss_eval: 3.4563, Learning Rate: 5.000000e-05\n",
      "Step 175100, Loss: 3.2622, Loss_eval: 3.4683, Learning Rate: 5.000000e-05\n",
      "Step 175200, Loss: 3.3631, Loss_eval: 3.4508, Learning Rate: 5.000000e-05\n",
      "Step 175300, Loss: 3.0888, Loss_eval: 3.4562, Learning Rate: 5.000000e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "model.train()\n",
    "device = next(model.parameters()).device\n",
    "accum_steps = 40\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(tqdm(loader_train, desc=\"Training\")):\n",
    "        batch = batch.to(device)\n",
    "        loss = train_step(model, \n",
    "                          batch, \n",
    "                          criterion, \n",
    "                          optimizer, \n",
    "                          scaler, \n",
    "                          scheduler, \n",
    "                          accum_steps,\n",
    "                          step)\n",
    "        if (step+1) % 100 == 0:\n",
    "            model.eval()\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            iter_test = iter(loader_test)\n",
    "            with torch.no_grad():\n",
    "                loss_eval = [forward_and_loss(model, next(iter_test).to(device), criterion).item() for _ in range(accum_steps)]\n",
    "                print(f\"Step {step+1}, Loss: {loss.item():<.4f}, Loss_eval: {np.mean(loss_eval):<.4f}, Learning Rate: {lr:4e}\")\n",
    "            model.train()\n",
    "\n",
    "        if (step+1) % 5000 == 0:\n",
    "            save_checkpoint(model, \n",
    "                            optimizer, \n",
    "                            scheduler, \n",
    "                            filename=\"checkpoint_transformer.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "962b0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"  <s><h>\"\n",
    "\n",
    "tokens = torch.tensor(tokenizer.encode(text.lower()), dtype=torch.long).reshape(1, -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f599efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa066bc415cd42e6b1b915f9c0323381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "# create a read-only text area\n",
    "ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    layout=widgets.Layout(width='80ch', height='20em'),\n",
    "    disabled=True\n",
    ")\n",
    "display(ta)\n",
    "\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "\n",
    "#torch.random.torch.manual_seed(42) \n",
    "\n",
    "for i in range(1024):\n",
    "    logits = model(tokens)[0, -1:]\n",
    "    topk_vals, _    = torch.topk(logits, k=k)\n",
    "    #print(topk_vals)\n",
    "    kth_value       = topk_vals[:,-1]\n",
    "\n",
    "    logits = torch.where(logits >= kth_value, logits, -torch.inf)\n",
    "    dist = Categorical(logits=logits/T)\n",
    "    idx = dist.sample()\n",
    "    tokens = torch.cat([tokens, idx.reshape(1,1)], dim=1)\n",
    "    #print(tokens.shape)\n",
    "    text = tokenizer.decode(tokens[0].tolist())\n",
    "    ta.value = wrapper.fill(text.replace(\"\\n\", \" \"))  # this updates in-place\n",
    "\n",
    "    if idx[0] == tokenizer.token_to_idx[\"</s>\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c0f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
